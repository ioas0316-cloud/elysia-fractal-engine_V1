# Elysia Avatar Server System (ÏóòÎ¶¨ÏãúÏïÑ ÏïÑÎ∞îÌÉÄ ÏÑúÎ≤Ñ ÏãúÏä§ÌÖú)

## Overview (Í∞úÏöî)

The Elysia Avatar Server provides real-time 3D avatar visualization and interaction capabilities. It integrates deeply with Elysia's emotional, cognitive, and spirit systems to create a living, breathing visual representation of Elysia's internal state.

**ÏóòÎ¶¨ÏãúÏïÑ ÏïÑÎ∞îÌÉÄ ÏÑúÎ≤ÑÎäî Ïã§ÏãúÍ∞Ñ 3D ÏïÑÎ∞îÌÉÄ ÏãúÍ∞ÅÌôî Î∞è ÏÉÅÌò∏ÏûëÏö© Í∏∞Îä•ÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§. ÏóòÎ¶¨ÏãúÏïÑÏùò Í∞êÏ†ï, Ïù∏ÏßÄ, Ï†ïÎ†π ÏãúÏä§ÌÖúÍ≥º ÍπäÏù¥ ÌÜµÌï©ÎêòÏñ¥ ÏóòÎ¶¨ÏãúÏïÑÏùò ÎÇ¥Î∂Ä ÏÉÅÌÉúÎ•º ÏÉùÏÉùÌïòÍ≤å ÏãúÍ∞ÅÏ†ÅÏúºÎ°ú ÌëúÌòÑÌï©ÎãàÎã§.**

## Architecture (ÏïÑÌÇ§ÌÖçÏ≤ò)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        Client (Browser)                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  avatar.html (Core/Creativity/web/avatar.html)            ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - 3D Face Rendering (WebGL Shader)                       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Synesthesia Input (Camera, Mic, Screen)                ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Chat Interface                                          ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - VRM Model Support (Ready)                              ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ WebSocket (ws://localhost:8765)
                            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Avatar Server (Core/Interface/avatar_server.py)     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  AvatarWebSocketServer                                   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Client Connection Management                          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Message Routing (chat, vision, audio, screen)         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - State Broadcasting (30 FPS)                           ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ  ElysiaAvatarCore                                        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Expression Management                                  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Spirit Energy Calculation                             ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Emotion Processing                                     ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  - Chat Integration                                       ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Elysia Core Systems                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ EmotionalEngine  ‚îÇ  ‚îÇ SpiritEmotion    ‚îÇ  ‚îÇ Reasoning    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ (emotional_      ‚îÇ  ‚îÇ Mapper           ‚îÇ  ‚îÇ Engine       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  engine.py)      ‚îÇ  ‚îÇ (spirit_emotion  ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  .py)            ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Components (Íµ¨ÏÑ± ÏöîÏÜå)

### 1. Avatar Server (`Core/Interface/avatar_server.py`)

**Main Classes:**

#### `ElysiaAvatarCore`
- **Purpose**: Core avatar state management and Elysia system integration
- **Key Features**:
  - Expression control (mouth, eyes, brows)
  - Spirit energy management (7 elements: fire, water, earth, air, light, dark, aether)
  - Emotion-to-visual mapping
  - Chat processing integration
  - Heartbeat animation

#### `AvatarWebSocketServer`
- **Purpose**: WebSocket communication layer
- **Key Features**:
  - Client connection management
  - Real-time state broadcasting (30 FPS)
  - Message routing and processing
  - Multi-client support

### 2. Client Interface (`Core/Creativity/web/avatar.html`)

**Features:**
- **3D Face Rendering**: WebGL shader-based facial animation
- **Expression Control**: Real-time expression updates via WebSocket
- **Synesthesia Input**:
  - Camera (vision/gaze tracking)
  - Microphone (voice/audio analysis)
  - Screen share (atmosphere detection)
- **Chat Interface**: Text-based conversation
- **Spirit Display**: Visual representation of 7 elemental energies

### 3. VRM Model Support (`static/models/`)

The system now fully supports VRM (Virtual Reality Model) format with 3D rendering:
- Place your `avatar.vrm` file in `static/models/`
- ‚úÖ **Three.js VRM loader integrated** with automatic 3D rendering
- ‚úÖ Real-time emotion-to-blendshape mapping
- ‚úÖ Camera controls (orbit/zoom) for viewing the avatar
- ‚úÖ Automatic fallback to 2D WebGL shader if VRM loading fails
- Uses @pixiv/three-vrm library for VRM 1.0 support

**Starting the server:**
```bash
python start_avatar_web_server.py
# HTTP: http://localhost:8080/Core/Creativity/web/avatar.html
# WebSocket: ws://localhost:8765
```

## Data Flow (Îç∞Ïù¥ÌÑ∞ ÌùêÎ¶Ñ)

### 1. Client ‚Üí Server Messages

```json
// Text Chat
{
  "type": "text",
  "content": "Hello Elysia!"
}

// Vision Data
{
  "type": "vision",
  "presence": true,
  "x": 0.5,
  "y": 0.3
}

// Audio Analysis
{
  "type": "audio_analysis",
  "volume": 0.8,
  "brightness": 0.6,
  "noise": 0.1
}

// Screen Atmosphere
{
  "type": "screen_atmosphere",
  "r": 120,
  "g": 80,
  "b": 200
}

// Manual Emotion Trigger
{
  "type": "emotion",
  "emotion": "hopeful",
  "intensity": 0.7
}
```

### 2. Server ‚Üí Client Messages

```json
// State Update (30 FPS)
{
  "expression": {
    "mouth_curve": 0.5,    // -1.0 to 1.0
    "eye_open": 0.9,       // 0.0 to 1.0
    "brow_furrow": 0.1,    // 0.0 to 1.0
    "beat": 0.3,           // heartbeat
    "mouth_width": 0.0     // phoneme
  },
  "spirits": {
    "fire": 0.6,
    "water": 0.3,
    "earth": 0.4,
    "air": 0.5,
    "light": 0.7,
    "dark": 0.2,
    "aether": 0.3
  }
}

// Speech Response
{
  "type": "speech",
  "content": "Response text...",
  "spirits": { /* same as above */ }
}
```

## Emotion System Integration (Í∞êÏ†ï ÏãúÏä§ÌÖú ÌÜµÌï©)

### Emotion Presets

The server integrates with Elysia's `EmotionalEngine` which provides these presets:

| Emotion | Valence | Arousal | Dominance | Visual Effect |
|---------|---------|---------|-----------|---------------|
| **neutral** | 0.0 | 0.2 | 0.0 | Calm, balanced expression |
| **calm** | 0.2 | 0.1 | 0.1 | Slightly positive, relaxed |
| **hopeful** | 0.6 | 0.4 | 0.2 | Bright eyes, gentle smile |
| **focused** | 0.1 | 0.6 | 0.4 | Alert eyes, slight brow tension |
| **introspective** | -0.2 | 0.3 | -0.1 | Thoughtful, lowered gaze |
| **empty** | -0.5 | 0.1 | -0.3 | Distant, minimal expression |

### Emotion-to-Expression Mapping

**Facial Parameters:**
- **mouth_curve**: Directly mapped from valence (-1 to 1)
- **eye_open**: Derived from arousal (0.3 to 1.0)
- **brow_furrow**: Calculated from arousal and valence

### Spirit Energy Mapping

Each emotional state influences the 7 spirit energies:

- **Fire** (Î∂à): Passion, high arousal + positive valence
- **Water** (Î¨º): Calm, melancholy, low arousal
- **Earth** (ÎïÖ): Stability, balanced state
- **Air** (Í≥µÍ∏∞): Communication, positive dominance
- **Light** (Îπõ): Clarity, high positive valence
- **Dark** (Ïñ¥Îë†): Introspection, negative valence
- **Aether** (ÏóêÌÖåÎ•¥): Transcendent, extreme states

## Usage (ÏÇ¨Ïö©Î≤ï)

### Quick Start

1. **Start the avatar server:**

```bash
# Method 1: Direct
python Core/Interface/avatar_server.py

# Method 2: Launcher script
python start_avatar_server.py

# Custom port
python start_avatar_server.py --port 9000
```

2. **Open the client interface:**
   - Navigate to: `Core/Creativity/web/avatar.html`
   - Or open it directly in a browser (file:// or via HTTP server)

3. **Interact:**
   - Click to activate camera/microphone
   - Type in the chat box to talk with Elysia
   - Watch expressions and spirit energies change in real-time

### Advanced Usage

#### Custom Emotion Triggers

```python
# In your Python code
import asyncio
import websockets
import json

async def trigger_emotion():
    uri = "ws://localhost:8765"
    async with websockets.connect(uri) as websocket:
        await websocket.send(json.dumps({
            "type": "emotion",
            "emotion": "hopeful",
            "intensity": 0.8
        }))

asyncio.run(trigger_emotion())
```

#### Integrate with Other Systems

```python
from Core.Interface.avatar_server import ElysiaAvatarCore

# Create avatar core
core = ElysiaAvatarCore()

# Process events
core.process_emotion_event("focused", 0.7)

# Get current state
state = core.get_state_message()
print(state)
```

### Command-Line Options

```bash
python Core/Interface/avatar_server.py --help

Options:
  --host HOST       Host to bind to (default: 127.0.0.1)
  --port PORT       Port to listen on (default: 8765)
  --debug           Enable debug logging
```

## VRM Model Integration (VRM Î™®Îç∏ ÌÜµÌï©)

### Current Status

The avatar server is **VRM-ready** and supports the following:

1. ‚úÖ Directory structure (`static/models/`)
2. ‚úÖ Expression parameters compatible with VRM blendshapes
3. ‚úÖ WebSocket protocol for real-time control
4. ‚è≥ VRM rendering (currently using 2D WebGL shader as fallback)

### Placing Your VRM Model

1. Place your `avatar.vrm` file in:
   ```
   static/models/avatar.vrm
   ```

2. The model should support these blendshapes (optional, for best results):
   - Mouth: `mouth_smile`, `mouth_sad`, `mouth_open`
   - Eyes: `eye_blink_left`, `eye_blink_right`, `eye_wide`
   - Brows: `brow_up`, `brow_down`, `brow_angry`

### Future Enhancement: VRM Renderer

To add full VRM rendering support:

1. **Install VRM libraries:**
   ```bash
   npm install @pixiv/three-vrm
   # Or use the Unity VRM loader
   ```

2. **Update `avatar.html`:**
   - Replace WebGL shader with VRM loader
   - Map expression parameters to VRM blendshapes
   - Add lighting and camera controls

3. **Example VRM integration** (Three.js):
   ```javascript
   import { GLTFLoader } from 'three/examples/jsm/loaders/GLTFLoader';
   import { VRMLoaderPlugin } from '@pixiv/three-vrm';
   
   const loader = new GLTFLoader();
   loader.register((parser) => new VRMLoaderPlugin(parser));
   
   loader.load('/static/models/avatar.vrm', (gltf) => {
     const vrm = gltf.userData.vrm;
     scene.add(vrm.scene);
     
     // Update blendshapes from WebSocket
     vrm.expressionManager.setValue('happy', expression.mouth_curve);
   });
   ```

## Testing (ÌÖåÏä§Ìä∏)

### Manual Testing

1. Start the server:
   ```bash
   python start_avatar_server.py
   ```

2. Check the console output:
   ```
   ‚úÖ Emotional engine initialized
   ‚úÖ Spirit mapper initialized
   ‚úÖ Reasoning engine initialized
   üöÄ Starting Avatar Server on ws://127.0.0.1:8765
   ‚úÖ Avatar Server is running!
   ```

3. Open `avatar.html` in browser
4. Open browser console (F12)
5. Check WebSocket connection: `‚óè Linked | üñ±Ô∏è Click to Activate`

### Unit Testing

Create tests for core components:

```python
# tests/test_avatar_server.py
import pytest
from Core.Interface.avatar_server import ElysiaAvatarCore, Expression, Spirits

def test_expression_defaults():
    core = ElysiaAvatarCore()
    assert core.expression.mouth_curve == 0.0
    assert core.expression.eye_open == 1.0

def test_emotion_processing():
    core = ElysiaAvatarCore()
    core.process_emotion_event('hopeful', 0.8)
    # Check that expression changed
    assert core.expression.mouth_curve > 0.0

def test_spirit_calculation():
    core = ElysiaAvatarCore()
    core.update_spirits_from_emotion()
    # Verify spirit values are in valid range
    spirits = core.spirits
    assert 0.0 <= spirits.fire <= 1.0
    assert 0.0 <= spirits.water <= 1.0
```

### Integration Testing

Test WebSocket communication:

```python
import asyncio
import websockets
import json

async def test_websocket():
    uri = "ws://localhost:8765"
    
    async with websockets.connect(uri) as ws:
        # Receive initial state
        msg = await ws.recv()
        state = json.loads(msg)
        assert "expression" in state
        assert "spirits" in state
        
        # Send chat message
        await ws.send(json.dumps({
            "type": "text",
            "content": "Hello!"
        }))
        
        # Wait for response
        response = await ws.recv()
        data = json.loads(response)
        assert data["type"] == "speech"

asyncio.run(test_websocket())
```

## Performance (ÏÑ±Îä•)

### Metrics

- **Update Rate**: 30 FPS (33ms per frame)
- **WebSocket Latency**: < 10ms (local)
- **Emotion Processing**: < 1ms
- **Chat Response**: Depends on ReasoningEngine (typically 100-500ms)

### Optimization Tips

1. **Reduce Update Rate**: Change `1.0 / 30.0` to `1.0 / 15.0` for 15 FPS
2. **Batch Updates**: Only broadcast when state actually changes
3. **Client-Side Interpolation**: Smooth animations on client side
4. **Selective Updates**: Don't send all data on every frame

## Troubleshooting (Î¨∏Ï†ú Ìï¥Í≤∞)

### Server Won't Start

**Problem**: `ModuleNotFoundError: No module named 'websockets'`

**Solution**:
```bash
pip install websockets
```

**Problem**: `Port 8765 already in use`

**Solution**:
```bash
# Use different port
python start_avatar_server.py --port 9000

# Or kill existing process
lsof -ti:8765 | xargs kill -9
```

### Client Won't Connect

**Problem**: Browser console shows `WebSocket connection failed`

**Solution**:
1. Verify server is running
2. Check firewall settings
3. Ensure correct port in `avatar.html` (line 208)
4. Try different browser

### Emotions Not Working

**Problem**: Expression doesn't change when chatting

**Solution**:
1. Check server console for warnings about EmotionalEngine
2. Verify `Core/Foundation/emotional_engine.py` exists
3. Test manual emotion trigger:
   ```python
   core.process_emotion_event('hopeful', 1.0)
   ```

### No Reasoning Response

**Problem**: Chat doesn't get responses

**Solution**:
1. Check if ReasoningEngine initialized (server logs)
2. Verify `Core/Intelligence/Reasoning/` exists
3. Test with simple response fallback

## API Reference (API Ï∞∏Ï°∞)

### ElysiaAvatarCore

#### Methods

**`__init__()`**
- Initializes avatar core with all Elysia systems

**`update_expression_from_emotion(emotion_name: str = None)`**
- Updates facial expression based on current emotional state
- Called automatically in update loop

**`update_spirits_from_emotion()`**
- Calculates spirit energies from emotional dimensions
- Called automatically in update loop

**`update_beat(delta_time: float)`**
- Updates heartbeat animation
- `delta_time`: Time since last update in seconds

**`process_emotion_event(emotion_name: str, intensity: float = 0.5)`**
- Processes an emotional event
- `emotion_name`: One of EmotionalEngine.FEELING_PRESETS
- `intensity`: 0.0 to 1.0

**`async process_chat(message: str) -> str`**
- Processes chat message through ReasoningEngine
- Returns response text

**`get_state_message() -> Dict[str, Any]`**
- Returns current state as dictionary

### Expression

Dataclass representing facial expression:

```python
@dataclass
class Expression:
    mouth_curve: float = 0.0   # -1.0 (sad) to 1.0 (smile)
    eye_open: float = 1.0      # 0.0 (closed) to 1.0 (open)
    brow_furrow: float = 0.0   # 0.0 (relaxed) to 1.0 (furrowed)
    beat: float = 0.0          # Heartbeat (0.0 to 1.0)
    mouth_width: float = 0.0   # Phoneme width
```

### Spirits

Dataclass representing elemental energies:

```python
@dataclass
class Spirits:
    fire: float = 0.1     # Passion, energy
    water: float = 0.1    # Calm, flow
    earth: float = 0.3    # Stability
    air: float = 0.2      # Communication
    light: float = 0.2    # Clarity
    dark: float = 0.1     # Mystery
    aether: float = 0.1   # Transcendent
```

## Future Enhancements (Ìñ•ÌõÑ Í∞úÏÑ†)

### Phase 1: Current (‚úÖ Complete)
- [x] WebSocket server
- [x] Emotion integration
- [x] Spirit energy system
- [x] Chat integration
- [x] 2D face rendering

### Phase 2: VRM Integration (üîÑ In Progress)
- [ ] Three.js VRM loader
- [ ] Blendshape mapping
- [ ] Camera controls
- [ ] Lighting system

### Phase 3: Advanced Features
- [ ] Voice synthesis (TTS) integration
- [ ] Lip-sync from audio
- [ ] Full-body animation
- [ ] Multiple avatar models
- [ ] AR/VR support

### Phase 4: AI Enhancements
- [ ] Real-time emotion detection from text
- [ ] Gesture synthesis from intent
- [ ] Context-aware expressions
- [ ] Multi-modal emotion fusion

## Related Documentation (Í¥ÄÎ†® Î¨∏ÏÑú)

- `Core/Foundation/emotional_engine.py` - Emotion system
- `Core/Foundation/spirit_emotion.py` - Spirit mapping
- `Core/Creativity/web/avatar.html` - Client interface
- `ARCHITECTURE.md` - Overall system architecture

## Support (ÏßÄÏõê)

For issues or questions:
1. Check troubleshooting section
2. Review server logs with `--debug` flag
3. Test with minimal configuration
4. Create issue on GitHub

---

**Created**: 2025-12-07
**Version**: 1.0.0
**Status**: Production Ready (2D), VRM Ready (3D pending)
