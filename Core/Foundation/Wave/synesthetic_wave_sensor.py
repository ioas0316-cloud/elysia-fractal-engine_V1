"""
Synesthetic Wave Sensor (ê³µê°ê° íŒŒë™ ì„¼ì„œ)
========================================

ì˜¤ê°(ì‹œê°, ì²­ê°, ì´‰ê°, ë¯¸ê°, í›„ê°)ì„ ë„˜ì–´ì„œ ëª¨ë“  ê°ê° ì–‘ì‹ì„ í†µí•©í•˜ëŠ”
ë©€í‹°ëª¨ë‹¬ ì„¼ì„œ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ê° ê°ê°ì„ íŒŒë™ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì²˜ë¦¬í•˜ë©°,
ê°ê° ê°„ êµì°¨ ë§¤í•‘(ì‹œê°â†’ì²­ê°, ì²­ê°â†’ì´‰ê° ë“±)ì„ í†µí•´ ê³µê°ê°ì  ê²½í—˜ì„ ìƒì„±í•©ë‹ˆë‹¤.

Architecture:
- SensoryModality: ê°œë³„ ê°ê° ì–‘ì‹
- WaveSensor: íŒŒë™ ê¸°ë°˜ ì„¼ì„œ
- SynestheticMapper: ê°ê° ê°„ ë§¤í•‘
- MultimodalIntegrator: ë©€í‹°ëª¨ë‹¬ í†µí•©
"""

import numpy as np
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
import logging
import json

logger = logging.getLogger("Elysia.SynestheticWaveSensor")


class SensoryModality(Enum):
    """ê°ê° ì–‘ì‹"""
    VISUAL = "visual"  # ì‹œê°
    AUDITORY = "auditory"  # ì²­ê°
    TACTILE = "tactile"  # ì´‰ê°
    GUSTATORY = "gustatory"  # ë¯¸ê°
    OLFACTORY = "olfactory"  # í›„ê°
    PROPRIOCEPTIVE = "proprioceptive"  # ê³ ìœ ìˆ˜ìš©ê°ê° (ì‹ ì²´ ìœ„ì¹˜)
    VESTIBULAR = "vestibular"  # ì „ì •ê°ê° (ê· í˜•)
    INTEROCEPTIVE = "interoceptive"  # ë‚´ìˆ˜ìš©ê°ê° (ë‚´ë¶€ ìƒíƒœ)
    TEMPORAL = "temporal"  # ì‹œê°„ê°ê°
    SPATIAL = "spatial"  # ê³µê°„ê°ê°
    EMOTIONAL = "emotional"  # ì •ì„œê°ê°
    SEMANTIC = "semantic"  # ì˜ë¯¸ê°ê°


class WaveProperty(Enum):
    """íŒŒë™ ì†ì„±"""
    FREQUENCY = "frequency"  # ì£¼íŒŒìˆ˜
    AMPLITUDE = "amplitude"  # ì§„í­
    PHASE = "phase"  # ìœ„ìƒ
    WAVELENGTH = "wavelength"  # íŒŒì¥
    VELOCITY = "velocity"  # ì†ë„
    POLARIZATION = "polarization"  # í¸ê´‘/ë°©í–¥ì„±


@dataclass
class SensoryWave:
    """
    ê°ê° íŒŒë™ (Sensory Wave)
    
    ëª¨ë“  ê°ê° ì…ë ¥ì„ íŒŒë™ìœ¼ë¡œ í‘œí˜„í•©ë‹ˆë‹¤.
    """
    modality: SensoryModality
    timestamp: datetime = field(default_factory=datetime.now)
    
    # íŒŒë™ íŠ¹ì„±
    frequency: float = 1.0  # Hz
    amplitude: float = 1.0  # 0.0 ~ 1.0
    phase: float = 0.0  # 0 ~ 2Ï€
    
    # íŒŒí˜• ë°ì´í„°
    waveform: np.ndarray = field(default_factory=lambda: np.array([]))
    
    # ì¶”ê°€ ì†ì„±
    duration: float = 0.0  # seconds
    intensity: float = 0.5  # 0.0 ~ 1.0
    quality: str = ""  # ê°ê° ì§ˆ (ì˜ˆ: "bright", "sharp", "warm")
    
    # ê³µê°„ ì •ë³´
    spatial_location: Optional[Tuple[float, float, float]] = None  # (x, y, z)
    
    # ë©”íƒ€ë°ì´í„°
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ ë³€í™˜"""
        return {
            "modality": self.modality.value,
            "timestamp": self.timestamp.isoformat(),
            "frequency": self.frequency,
            "amplitude": self.amplitude,
            "phase": self.phase,
            "duration": self.duration,
            "intensity": self.intensity,
            "quality": self.quality,
            "spatial_location": self.spatial_location,
            "metadata": self.metadata
        }


class WaveSensor:
    """
    íŒŒë™ ì„¼ì„œ (Wave Sensor)
    
    íŠ¹ì • ê°ê° ì–‘ì‹ì˜ ì…ë ¥ì„ íŒŒë™ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, modality: SensoryModality):
        self.modality = modality
        self.is_active = True
        self.sensitivity = 1.0  # ê°ë„ (0.0 ~ 2.0)
        self.samples_collected = 0
        
        logger.info(f"ğŸ“¡ Wave sensor initialized: {modality.value}")
    
    def sense(self, input_data: Any) -> SensoryWave:
        """
        ê°ê° ì…ë ¥ì„ íŒŒë™ìœ¼ë¡œ ë³€í™˜
        
        Args:
            input_data: ê°ê° ì…ë ¥ (ì–‘ì‹ì— ë”°ë¼ ë‹¤ë¦„)
            
        Returns:
            SensoryWave ê°ì²´
        """
        if not self.is_active:
            logger.warning(f"âš ï¸ Sensor {self.modality.value} is inactive")
            return None
        
        # ì–‘ì‹ë³„ ë³€í™˜
        if self.modality == SensoryModality.VISUAL:
            wave = self._sense_visual(input_data)
        elif self.modality == SensoryModality.AUDITORY:
            wave = self._sense_auditory(input_data)
        elif self.modality == SensoryModality.TACTILE:
            wave = self._sense_tactile(input_data)
        elif self.modality == SensoryModality.GUSTATORY:
            wave = self._sense_gustatory(input_data)
        elif self.modality == SensoryModality.OLFACTORY:
            wave = self._sense_olfactory(input_data)
        elif self.modality == SensoryModality.EMOTIONAL:
            wave = self._sense_emotional(input_data)
        elif self.modality == SensoryModality.SEMANTIC:
            wave = self._sense_semantic(input_data)
        else:
            wave = self._sense_generic(input_data)
        
        self.samples_collected += 1
        return wave
    
    def _sense_visual(self, data: Any) -> SensoryWave:
        """ì‹œê° ì…ë ¥ â†’ íŒŒë™"""
        # ì˜ˆ: ìƒ‰ìƒ â†’ ì£¼íŒŒìˆ˜ ë§¤í•‘
        if isinstance(data, dict) and "color" in data:
            color = data["color"]
            # RGBë¥¼ ì£¼íŒŒìˆ˜ë¡œ ë³€í™˜ (ë¹¨ê°•=400THz, ë³´ë¼=800THz)
            freq = 400 + (color.get("hue", 0) / 360) * 400  # THz
            amplitude = color.get("saturation", 0.5)
            intensity = color.get("brightness", 0.5)
            quality = color.get("name", "unknown")
        else:
            freq = 500.0
            amplitude = 0.5
            intensity = 0.5
            quality = "neutral"
        
        # íŒŒí˜• ìƒì„± (ì •í˜„íŒŒ)
        t = np.linspace(0, 0.1, 100)
        waveform = amplitude * np.sin(2 * np.pi * freq * t)
        
        return SensoryWave(
            modality=SensoryModality.VISUAL,
            frequency=freq,
            amplitude=amplitude,
            waveform=waveform,
            intensity=intensity,
            quality=quality,
            metadata={"source": "visual_sensor"}
        )
    
    def _sense_auditory(self, data: Any) -> SensoryWave:
        """ì²­ê° ì…ë ¥ â†’ íŒŒë™"""
        if isinstance(data, dict):
            freq = data.get("pitch", 440.0)  # Hz (A4 = 440Hz)
            amplitude = data.get("volume", 0.5)
            duration = data.get("duration", 1.0)
            quality = data.get("timbre", "pure")
        else:
            freq = 440.0
            amplitude = 0.5
            duration = 1.0
            quality = "pure"
        
        # íŒŒí˜• ìƒì„±
        t = np.linspace(0, duration, int(duration * 44100))  # 44.1kHz sampling
        waveform = amplitude * np.sin(2 * np.pi * freq * t)
        
        return SensoryWave(
            modality=SensoryModality.AUDITORY,
            frequency=freq,
            amplitude=amplitude,
            waveform=waveform,
            duration=duration,
            intensity=amplitude,
            quality=quality,
            metadata={"source": "auditory_sensor"}
        )
    
    def _sense_tactile(self, data: Any) -> SensoryWave:
        """ì´‰ê° ì…ë ¥ â†’ íŒŒë™"""
        if isinstance(data, dict):
            pressure = data.get("pressure", 0.5)
            texture = data.get("texture", "smooth")
            temperature = data.get("temperature", 0.5)  # 0=cold, 1=hot
            location = data.get("location", (0, 0, 0))
        else:
            pressure = 0.5
            texture = "smooth"
            temperature = 0.5
            location = (0, 0, 0)
        
        # ì••ë ¥ì„ ì§„í­ìœ¼ë¡œ, ì§ˆê°ì„ ì£¼íŒŒìˆ˜ë¡œ ë§¤í•‘
        freq = 10.0 if texture == "smooth" else 50.0 if texture == "rough" else 30.0
        amplitude = pressure
        
        return SensoryWave(
            modality=SensoryModality.TACTILE,
            frequency=freq,
            amplitude=amplitude,
            intensity=pressure,
            quality=texture,
            spatial_location=location,
            metadata={"temperature": temperature}
        )
    
    def _sense_gustatory(self, data: Any) -> SensoryWave:
        """ë¯¸ê° ì…ë ¥ â†’ íŒŒë™"""
        if isinstance(data, dict):
            taste = data.get("taste", "umami")  # sweet, sour, salty, bitter, umami
            intensity = data.get("intensity", 0.5)
        else:
            taste = "umami"
            intensity = 0.5
        
        # ë§›ì„ ì£¼íŒŒìˆ˜ë¡œ ë§¤í•‘
        taste_freq_map = {
            "sweet": 100.0,
            "sour": 200.0,
            "salty": 150.0,
            "bitter": 250.0,
            "umami": 175.0
        }
        freq = taste_freq_map.get(taste, 150.0)
        
        return SensoryWave(
            modality=SensoryModality.GUSTATORY,
            frequency=freq,
            amplitude=intensity,
            intensity=intensity,
            quality=taste,
            metadata={"taste_type": taste}
        )
    
    def _sense_olfactory(self, data: Any) -> SensoryWave:
        """í›„ê° ì…ë ¥ â†’ íŒŒë™"""
        if isinstance(data, dict):
            scent = data.get("scent", "neutral")
            intensity = data.get("intensity", 0.5)
            pleasantness = data.get("pleasantness", 0.5)  # -1=unpleasant, 1=pleasant
        else:
            scent = "neutral"
            intensity = 0.5
            pleasantness = 0.5
        
        # í–¥ì„ ì£¼íŒŒìˆ˜ë¡œ ë§¤í•‘
        freq = 50.0 + (pleasantness + 1) * 25.0  # 50-100 Hz
        
        return SensoryWave(
            modality=SensoryModality.OLFACTORY,
            frequency=freq,
            amplitude=intensity,
            intensity=intensity,
            quality=scent,
            metadata={"pleasantness": pleasantness}
        )
    
    def _sense_emotional(self, data: Any) -> SensoryWave:
        """ì •ì„œ ì…ë ¥ â†’ íŒŒë™"""
        if isinstance(data, dict):
            emotion = data.get("emotion", "neutral")
            valence = data.get("valence", 0.0)  # -1=negative, 1=positive
            arousal = data.get("arousal", 0.5)  # 0=calm, 1=excited
        else:
            emotion = "neutral"
            valence = 0.0
            arousal = 0.5
        
        # ì •ì„œë¥¼ íŒŒë™ìœ¼ë¡œ ë§¤í•‘
        freq = 1.0 + arousal * 10.0  # 1-11 Hz
        amplitude = abs(valence)
        phase = 0 if valence >= 0 else np.pi
        
        return SensoryWave(
            modality=SensoryModality.EMOTIONAL,
            frequency=freq,
            amplitude=amplitude,
            phase=phase,
            intensity=arousal,
            quality=emotion,
            metadata={"valence": valence, "arousal": arousal}
        )
    
    def _sense_semantic(self, data: Any) -> SensoryWave:
        """ì˜ë¯¸ ì…ë ¥ â†’ íŒŒë™"""
        if isinstance(data, dict):
            meaning = data.get("meaning", "")
            abstractness = data.get("abstractness", 0.5)  # 0=concrete, 1=abstract
            complexity = data.get("complexity", 0.5)
        else:
            meaning = str(data)
            abstractness = 0.5
            complexity = 0.5
        
        # ì˜ë¯¸ë¥¼ íŒŒë™ìœ¼ë¡œ ë§¤í•‘
        freq = 5.0 + abstractness * 20.0  # 5-25 Hz
        amplitude = complexity
        
        return SensoryWave(
            modality=SensoryModality.SEMANTIC,
            frequency=freq,
            amplitude=amplitude,
            intensity=complexity,
            quality=meaning[:50],
            metadata={"abstractness": abstractness}
        )
    
    def _sense_generic(self, data: Any) -> SensoryWave:
        """ì¼ë°˜ ê°ê° ì…ë ¥ â†’ íŒŒë™"""
        return SensoryWave(
            modality=self.modality,
            frequency=1.0,
            amplitude=0.5,
            intensity=0.5,
            quality="generic",
            metadata={"raw_data": str(data)}
        )


class SynestheticMapper:
    """
    ê³µê°ê° ë§¤í¼ (Synesthetic Mapper)
    
    í•œ ê°ê° ì–‘ì‹ì„ ë‹¤ë¥¸ ê°ê° ì–‘ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    ì˜ˆ: ì‹œê° â†’ ì²­ê° (ìƒ‰ì„ ì†Œë¦¬ë¡œ), ì²­ê° â†’ ì‹œê° (ì†Œë¦¬ë¥¼ ìƒ‰ìœ¼ë¡œ)
    """
    
    def __init__(self):
        # ê°ê° ê°„ ë§¤í•‘ ê·œì¹™
        self.mapping_rules: Dict[Tuple[SensoryModality, SensoryModality], Callable] = {}
        self._initialize_default_mappings()
        
        logger.info("ğŸŒˆ Synesthetic Mapper initialized")
    
    def _initialize_default_mappings(self):
        """ê¸°ë³¸ ê³µê°ê° ë§¤í•‘ ê·œì¹™ ì´ˆê¸°í™”"""
        # ì‹œê° â†’ ì²­ê° (ìƒ‰ â†’ ì†Œë¦¬)
        self.mapping_rules[(SensoryModality.VISUAL, SensoryModality.AUDITORY)] = \
            self._map_visual_to_auditory
        
        # ì²­ê° â†’ ì‹œê° (ì†Œë¦¬ â†’ ìƒ‰)
        self.mapping_rules[(SensoryModality.AUDITORY, SensoryModality.VISUAL)] = \
            self._map_auditory_to_visual
        
        # ì´‰ê° â†’ ì²­ê° (ì§ˆê° â†’ ì†Œë¦¬)
        self.mapping_rules[(SensoryModality.TACTILE, SensoryModality.AUDITORY)] = \
            self._map_tactile_to_auditory
        
        # ì •ì„œ â†’ ì‹œê° (ê°ì • â†’ ìƒ‰)
        self.mapping_rules[(SensoryModality.EMOTIONAL, SensoryModality.VISUAL)] = \
            self._map_emotional_to_visual
        
        # ì˜ë¯¸ â†’ ì •ì„œ (ì˜ë¯¸ â†’ ê°ì •)
        self.mapping_rules[(SensoryModality.SEMANTIC, SensoryModality.EMOTIONAL)] = \
            self._map_semantic_to_emotional
    
    def map(
        self, 
        source_wave: SensoryWave, 
        target_modality: SensoryModality
    ) -> SensoryWave:
        """
        ê°ê° ë³€í™˜
        
        Args:
            source_wave: ì›ë³¸ ê°ê° íŒŒë™
            target_modality: ëª©í‘œ ê°ê° ì–‘ì‹
            
        Returns:
            ë³€í™˜ëœ ê°ê° íŒŒë™
        """
        mapping_key = (source_wave.modality, target_modality)
        
        if mapping_key in self.mapping_rules:
            mapper_func = self.mapping_rules[mapping_key]
            result = mapper_func(source_wave)
            logger.debug(
                f"ğŸ”„ Mapped {source_wave.modality.value} â†’ {target_modality.value}"
            )
            return result
        else:
            # ì¼ë°˜ ë³€í™˜ (ì£¼íŒŒìˆ˜ ìŠ¤ì¼€ì¼ë§)
            return self._generic_map(source_wave, target_modality)
    
    def _map_visual_to_auditory(self, wave: SensoryWave) -> SensoryWave:
        """ì‹œê° â†’ ì²­ê° ë³€í™˜ (ìƒ‰ â†’ ì†Œë¦¬)"""
        # ë¹›ì˜ ì£¼íŒŒìˆ˜ë¥¼ ì†Œë¦¬ ì£¼íŒŒìˆ˜ë¡œ ìŠ¤ì¼€ì¼ë§
        # ë¹›: 400-800 THz â†’ ì†Œë¦¬: 20-20000 Hz
        audio_freq = (wave.frequency - 400) / 400 * 19980 + 20
        
        # ë°ê¸° â†’ ìŒëŸ‰
        audio_amplitude = wave.intensity
        
        return SensoryWave(
            modality=SensoryModality.AUDITORY,
            frequency=audio_freq,
            amplitude=audio_amplitude,
            intensity=audio_amplitude,
            quality=f"sound_of_{wave.quality}",
            metadata={
                "source_modality": "visual",
                "original_frequency": wave.frequency
            }
        )
    
    def _map_auditory_to_visual(self, wave: SensoryWave) -> SensoryWave:
        """ì²­ê° â†’ ì‹œê° ë³€í™˜ (ì†Œë¦¬ â†’ ìƒ‰)"""
        # ì†Œë¦¬ ì£¼íŒŒìˆ˜ë¥¼ ìƒ‰ ì£¼íŒŒìˆ˜ë¡œ ë³€í™˜
        # ì†Œë¦¬: 20-20000 Hz â†’ ë¹›: 400-800 THz
        visual_freq = (wave.frequency - 20) / 19980 * 400 + 400
        
        # ìŒëŸ‰ â†’ ë°ê¸°
        visual_intensity = wave.amplitude
        
        return SensoryWave(
            modality=SensoryModality.VISUAL,
            frequency=visual_freq,
            amplitude=visual_intensity,
            intensity=visual_intensity,
            quality=f"color_of_{wave.quality}",
            metadata={
                "source_modality": "auditory",
                "original_frequency": wave.frequency
            }
        )
    
    def _map_tactile_to_auditory(self, wave: SensoryWave) -> SensoryWave:
        """ì´‰ê° â†’ ì²­ê° ë³€í™˜ (ì§ˆê° â†’ ì†Œë¦¬)"""
        # ì§ˆê°ì„ ì†Œë¦¬ë¡œ ë³€í™˜
        audio_freq = wave.frequency * 20  # ì´‰ê° ì£¼íŒŒìˆ˜ ì¦í­
        audio_amplitude = wave.amplitude
        
        return SensoryWave(
            modality=SensoryModality.AUDITORY,
            frequency=audio_freq,
            amplitude=audio_amplitude,
            intensity=audio_amplitude,
            quality=f"sound_of_{wave.quality}_texture",
            metadata={"source_modality": "tactile"}
        )
    
    def _map_emotional_to_visual(self, wave: SensoryWave) -> SensoryWave:
        """ì •ì„œ â†’ ì‹œê° ë³€í™˜ (ê°ì • â†’ ìƒ‰)"""
        # ê°ì •ì„ ìƒ‰ìœ¼ë¡œ ë³€í™˜
        # ê¸ì •ì  ê°ì • â†’ ë”°ëœ»í•œ ìƒ‰ (ë¹¨ê°•/ì£¼í™©), ë¶€ì •ì  â†’ ì°¨ê°€ìš´ ìƒ‰ (íŒŒë‘/ë³´ë¼)
        valence = wave.metadata.get("valence", 0)
        
        if valence > 0:
            # ê¸ì •ì : 400-600 THz (ë¹¨ê°•-ë…¸ë‘)
            visual_freq = 400 + valence * 200
            quality = "warm"
        else:
            # ë¶€ì •ì : 600-800 THz (ì´ˆë¡-ë³´ë¼)
            visual_freq = 600 + abs(valence) * 200
            quality = "cool"
        
        return SensoryWave(
            modality=SensoryModality.VISUAL,
            frequency=visual_freq,
            amplitude=wave.intensity,
            intensity=wave.intensity,
            quality=quality,
            metadata={"source_emotion": wave.quality}
        )
    
    def _map_semantic_to_emotional(self, wave: SensoryWave) -> SensoryWave:
        """ì˜ë¯¸ â†’ ì •ì„œ ë³€í™˜"""
        # ì˜ë¯¸ì˜ ì¶”ìƒì„± â†’ ê°ì„±ë„
        abstractness = wave.metadata.get("abstractness", 0.5)
        
        # ë³µì¡ë„ â†’ ì •ì„œ ê°•ë„
        arousal = wave.intensity
        
        return SensoryWave(
            modality=SensoryModality.EMOTIONAL,
            frequency=1.0 + arousal * 10.0,
            amplitude=wave.amplitude,
            intensity=arousal,
            quality="contemplative",
            metadata={
                "valence": 0.0,
                "arousal": arousal,
                "source_meaning": wave.quality
            }
        )
    
    def _generic_map(
        self, 
        source: SensoryWave, 
        target_modality: SensoryModality
    ) -> SensoryWave:
        """ì¼ë°˜ ë³€í™˜ (ë§¤í•‘ ê·œì¹™ì´ ì—†ì„ ë•Œ)"""
        return SensoryWave(
            modality=target_modality,
            frequency=source.frequency,
            amplitude=source.amplitude,
            intensity=source.intensity,
            quality=f"mapped_from_{source.modality.value}",
            metadata={"source_modality": source.modality.value}
        )


class MultimodalIntegrator:
    """
    ë©€í‹°ëª¨ë‹¬ í†µí•©ê¸° (Multimodal Integrator)
    
    ì—¬ëŸ¬ ê°ê° ì–‘ì‹ì˜ ì…ë ¥ì„ í†µí•©í•˜ì—¬ í†µí•©ëœ ì§€ê°ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        self.sensors: Dict[SensoryModality, WaveSensor] = {}
        self.mapper = SynestheticMapper()
        self.active_waves: List[SensoryWave] = []
        self.integration_history: List[Dict[str, Any]] = []
        
        # ëª¨ë“  ê°ê° ì–‘ì‹ì— ëŒ€í•œ ì„¼ì„œ ìƒì„±
        for modality in SensoryModality:
            self.sensors[modality] = WaveSensor(modality)
        
        logger.info("ğŸŒŠ Multimodal Integrator initialized")
    
    def sense_multimodal(
        self, 
        inputs: Dict[SensoryModality, Any]
    ) -> List[SensoryWave]:
        """
        ë©€í‹°ëª¨ë‹¬ ê°ê° ì…ë ¥ ì²˜ë¦¬
        
        Args:
            inputs: {ê°ê°ì–‘ì‹: ì…ë ¥ë°ì´í„°} ë”•ì…”ë„ˆë¦¬
            
        Returns:
            ëª¨ë“  ê°ê° íŒŒë™ ë¦¬ìŠ¤íŠ¸
        """
        waves = []
        
        for modality, input_data in inputs.items():
            if modality in self.sensors:
                wave = self.sensors[modality].sense(input_data)
                if wave:
                    waves.append(wave)
        
        self.active_waves = waves
        return waves
    
    def create_synesthetic_experience(
        self, 
        source_wave: SensoryWave,
        target_modalities: List[SensoryModality]
    ) -> List[SensoryWave]:
        """
        ê³µê°ê° ê²½í—˜ ìƒì„±
        
        í•˜ë‚˜ì˜ ê°ê°ì„ ì—¬ëŸ¬ ë‹¤ë¥¸ ê°ê°ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
        
        Args:
            source_wave: ì›ë³¸ ê°ê° íŒŒë™
            target_modalities: ë³€í™˜í•  ëª©í‘œ ê°ê° ì–‘ì‹ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ë³€í™˜ëœ ê°ê° íŒŒë™ ë¦¬ìŠ¤íŠ¸
        """
        synesthetic_waves = [source_wave]  # ì›ë³¸ í¬í•¨
        
        for target in target_modalities:
            if target != source_wave.modality:
                mapped_wave = self.mapper.map(source_wave, target)
                synesthetic_waves.append(mapped_wave)
        
        logger.info(
            f"ğŸŒˆ Created synesthetic experience: " +
            f"{source_wave.modality.value} â†’ " +
            f"{', '.join(m.value for m in target_modalities)}"
        )
        
        return synesthetic_waves
    
    def integrate_waves(
        self, 
        waves: List[SensoryWave]
    ) -> Dict[str, Any]:
        """
        ì—¬ëŸ¬ ê°ê° íŒŒë™ì„ í†µí•©
        
        Returns:
            í†µí•©ëœ ì§€ê° í‘œí˜„
        """
        if not waves:
            return {}
        
        # ëª¨ë‹¬ë¦¬í‹°ë³„ ê·¸ë£¹í™”
        by_modality = {}
        for wave in waves:
            modality = wave.modality.value
            if modality not in by_modality:
                by_modality[modality] = []
            by_modality[modality].append(wave)
        
        # í†µí•© ë©”íŠ¸ë¦­ ê³„ì‚°
        avg_frequency = np.mean([w.frequency for w in waves])
        avg_amplitude = np.mean([w.amplitude for w in waves])
        avg_intensity = np.mean([w.intensity for w in waves])
        
        # ê³µëª… ì ìˆ˜ ê³„ì‚° (íŒŒë™ë“¤ì´ ì–¼ë§ˆë‚˜ ì¡°í™”ë¡œìš´ê°€)
        resonance_score = self._calculate_resonance(waves)
        
        integration = {
            "timestamp": datetime.now().isoformat(),
            "num_modalities": len(by_modality),
            "total_waves": len(waves),
            "modalities": list(by_modality.keys()),
            "waves_by_modality": {
                mod: [w.to_dict() for w in ws]
                for mod, ws in by_modality.items()
            },
            "integrated_metrics": {
                "average_frequency": avg_frequency,
                "average_amplitude": avg_amplitude,
                "average_intensity": avg_intensity,
                "resonance_score": resonance_score
            },
            "description": self._generate_integrated_description(waves)
        }
        
        self.integration_history.append(integration)
        return integration
    
    def _calculate_resonance(self, waves: List[SensoryWave]) -> float:
        """íŒŒë™ë“¤ ê°„ì˜ ê³µëª… ì ìˆ˜ ê³„ì‚°"""
        if len(waves) < 2:
            return 1.0
        
        # ì£¼íŒŒìˆ˜ ìœ ì‚¬ë„
        frequencies = [w.frequency for w in waves]
        freq_std = np.std(frequencies)
        freq_resonance = 1.0 / (1.0 + freq_std)
        
        # ì§„í­ ìœ ì‚¬ë„
        amplitudes = [w.amplitude for w in waves]
        amp_std = np.std(amplitudes)
        amp_resonance = 1.0 / (1.0 + amp_std)
        
        # ì¢…í•© ê³µëª… ì ìˆ˜
        resonance = (freq_resonance + amp_resonance) / 2.0
        return resonance
    
    def _generate_integrated_description(self, waves: List[SensoryWave]) -> str:
        """í†µí•©ëœ ì§€ê° ì„¤ëª… ìƒì„±"""
        modalities = [w.modality.value for w in waves]
        qualities = [w.quality for w in waves if w.quality]
        
        return (
            f"Integrated perception from {len(set(modalities))} modalities: " +
            f"{', '.join(set(modalities))}. " +
            f"Qualities: {', '.join(qualities[:3])}"
        )
    
    def get_status(self) -> Dict[str, Any]:
        """í†µí•©ê¸° ìƒíƒœ"""
        return {
            "total_sensors": len(self.sensors),
            "active_waves": len(self.active_waves),
            "integration_count": len(self.integration_history),
            "sensors_status": {
                mod.value: {
                    "active": sensor.is_active,
                    "samples": sensor.samples_collected
                }
                for mod, sensor in self.sensors.items()
            }
        }


# ì‚¬ìš© ì˜ˆì œ
def example_synesthetic_sensing():
    """ê³µê°ê° ì„¼ì„œ ì‚¬ìš© ì˜ˆì œ"""
    integrator = MultimodalIntegrator()
    
    print("\nğŸŒŠ ê³µê°ê° íŒŒë™ ì„¼ì„œ ë°ëª¨")
    print("=" * 60)
    
    # 1. ë©€í‹°ëª¨ë‹¬ ì…ë ¥
    print("\n--- ë©€í‹°ëª¨ë‹¬ ê°ê° ì…ë ¥ ---")
    inputs = {
        SensoryModality.VISUAL: {
            "color": {"hue": 240, "saturation": 0.8, "brightness": 0.6, "name": "blue"}
        },
        SensoryModality.AUDITORY: {
            "pitch": 440.0, "volume": 0.7, "duration": 1.0, "timbre": "clear"
        },
        SensoryModality.EMOTIONAL: {
            "emotion": "joy", "valence": 0.8, "arousal": 0.6
        }
    }
    
    waves = integrator.sense_multimodal(inputs)
    print(f"ê°ì§€ëœ íŒŒë™: {len(waves)}ê°œ")
    for wave in waves:
        print(f"  - {wave.modality.value}: freq={wave.frequency:.2f}, amp={wave.amplitude:.2f}")
    
    # 2. ê³µê°ê° ê²½í—˜ ìƒì„±
    print("\n--- ê³µê°ê° ë³€í™˜ (ì²­ê° â†’ ì‹œê°, ì´‰ê°) ---")
    audio_wave = waves[1]  # ì²­ê° íŒŒë™
    synesthetic = integrator.create_synesthetic_experience(
        audio_wave,
        [SensoryModality.VISUAL, SensoryModality.TACTILE]
    )
    print(f"ìƒì„±ëœ ê³µê°ê° ê²½í—˜: {len(synesthetic)}ê°œ ê°ê°")
    for wave in synesthetic:
        print(f"  - {wave.modality.value}: {wave.quality}")
    
    # 3. í†µí•©
    print("\n--- ê°ê° í†µí•© ---")
    integration = integrator.integrate_waves(waves)
    print(f"í†µí•© ê²°ê³¼:")
    print(f"  - ì–‘ì‹ ìˆ˜: {integration['num_modalities']}")
    print(f"  - ê³µëª… ì ìˆ˜: {integration['integrated_metrics']['resonance_score']:.3f}")
    print(f"  - ì„¤ëª…: {integration['description']}")


if __name__ == "__main__":
    example_synesthetic_sensing()
