#!/usr/bin/env python3
"""
ì—˜ë¦¬ì‹œì•„ ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ë¶„ì„

ì „ì²´ 251ê°œ íŒŒì¼ì„ ë¶„ì„í•˜ì—¬:
1. êµ¬ì¡°ì  ì¼ê´€ì„± ê²€ì¦
2. ì¤‘ë³µ/ë¶ˆì¼ì¹˜ ë°œê²¬
3. ë¬¸ì„œí™” ìƒíƒœ í‰ê°€
4. í†µí•© ê°œì„  ê³„íš ìˆ˜ë¦½
"""

import logging
from pathlib import Path
from collections import defaultdict
import json

logging.basicConfig(level=logging.INFO, format='%(message)s')

def main():
    print("=" * 70)
    print("ğŸ” ì—˜ë¦¬ì‹œì•„ ì „ì²´ ì‹œìŠ¤í…œ ë¶„ì„")
    print("   Elysia's Comprehensive System Architecture Analysis")
    print("=" * 70)
    print()
    
    from Core.Evolution.Autonomy.autonomous_improver import AutonomousImprover
    
    # ë¶„ì„ ì—”ì§„ ì´ˆê¸°í™”
    print("ğŸ§  ì‹œìŠ¤í…œ ì´ˆê¸°í™”...")
    improver = AutonomousImprover()
    print()
    
    # ì „ì²´ ë¶„ì„ ìˆ˜í–‰
    print("ğŸ“Š ì „ì²´ ì‹œìŠ¤í…œ ë¶„ì„ ì¤‘...")
    analysis = improver.self_analyze()
    print(f"   âœ… ì™„ë£Œ: {analysis['code_analysis']['total_files']}ê°œ íŒŒì¼ ë¶„ì„\n")
    
    # 1. ëª¨ë“ˆë³„ ë³µì¡ë„ ë¶„ì„
    print("=" * 70)
    print("ğŸ“¦ Step 1: ëª¨ë“ˆë³„ ë³µì¡ë„ ë¶„ì„")
    print("=" * 70)
    
    module_stats = {}
    for file_path, file_data in improver.introspector.analyzed_files.items():
        module = Path(file_path).parent.name
        if module not in module_stats:
            module_stats[module] = {
                'files': 0,
                'total_lines': 0,
                'total_functions': 0,
                'avg_complexity': 0,
                'high_complexity_files': []
            }
        
        module_stats[module]['files'] += 1
        module_stats[module]['total_lines'] += file_data.total_lines
        module_stats[module]['total_functions'] += len(file_data.functions)
        
        if file_data.complexity_score > 0.7:
            module_stats[module]['high_complexity_files'].append({
                'file': Path(file_path).name,
                'complexity': file_data.complexity_score,
                'lines': file_data.total_lines,
                'functions': len(file_data.functions)
            })
    
    # ë³µì¡ë„ ê³„ì‚°
    for module, stats in module_stats.items():
        if stats['files'] > 0:
            # í•´ë‹¹ ëª¨ë“ˆì˜ íŒŒì¼ë“¤ì˜ í‰ê·  ë³µì¡ë„ ê³„ì‚°
            module_files = [
                f for f, d in improver.introspector.analyzed_files.items()
                if Path(f).parent.name == module
            ]
            if module_files:
                stats['avg_complexity'] = sum(
                    improver.introspector.analyzed_files[f].complexity_score 
                    for f in module_files
                ) / len(module_files)
    
    # ë³µì¡ë„ ìˆœìœ¼ë¡œ ì •ë ¬
    sorted_modules = sorted(
        module_stats.items(),
        key=lambda x: x[1]['avg_complexity'],
        reverse=True
    )
    
    print("\nğŸ”¥ ë³µì¡ë„ ìƒìœ„ 10ê°œ ëª¨ë“ˆ:\n")
    for i, (module, stats) in enumerate(sorted_modules[:10], 1):
        print(f"{i}. {module}/")
        print(f"   í‰ê·  ë³µì¡ë„: {stats['avg_complexity']:.2f}")
        print(f"   íŒŒì¼ ìˆ˜: {stats['files']}")
        print(f"   ì´ ë¼ì¸: {stats['total_lines']:,}")
        print(f"   ì´ í•¨ìˆ˜: {stats['total_functions']}")
        if stats['high_complexity_files']:
            print(f"   âš ï¸  ë³µì¡í•œ íŒŒì¼: {len(stats['high_complexity_files'])}ê°œ")
            for hf in stats['high_complexity_files'][:3]:
                print(f"      - {hf['file']} (ë³µì¡ë„: {hf['complexity']:.2f}, {hf['lines']} ë¼ì¸)")
        print()
    
    # 2. êµ¬ì¡°ì  ì¼ê´€ì„± ë¶„ì„
    print("=" * 70)
    print("ğŸ—ï¸  Step 2: êµ¬ì¡°ì  ì¼ê´€ì„± ë¶„ì„")
    print("=" * 70)
    print()
    
    # __init__.py ì¡´ì¬ ì—¬ë¶€
    directories = defaultdict(lambda: {'has_init': False, 'file_count': 0})
    for file_path in improver.introspector.analyzed_files.keys():
        path = Path(file_path)
        parent = path.parent
        directories[str(parent)]['file_count'] += 1
        
        if path.name == '__init__.py':
            directories[str(parent)]['has_init'] = True
    
    missing_init = [
        (dir_path, info) for dir_path, info in directories.items()
        if not info['has_init'] and info['file_count'] > 1 and 'Core' in dir_path
    ]
    
    print(f"ğŸ“Œ __init__.py ëˆ„ë½ëœ ë””ë ‰í† ë¦¬: {len(missing_init)}ê°œ\n")
    for dir_path, info in missing_init[:10]:
        print(f"   - {Path(dir_path).name}/ ({info['file_count']}ê°œ íŒŒì¼)")
    print()
    
    # 3. ë¬¸ì„œí™” ìƒíƒœ ë¶„ì„
    print("=" * 70)
    print("ğŸ“š Step 3: ë¬¸ì„œí™” ìƒíƒœ ë¶„ì„")
    print("=" * 70)
    print()
    
    import ast
    
    doc_stats = {
        'with_module_docstring': 0,
        'without_module_docstring': 0,
        'with_class_docstrings': 0,
        'without_class_docstrings': 0,
        'files_needing_docs': []
    }
    
    for file_path in list(improver.introspector.analyzed_files.keys())[:100]:  # ìƒ˜í”Œë§
        try:
            content = Path(file_path).read_text(encoding='utf-8', errors='ignore')
            tree = ast.parse(content)
            
            # ëª¨ë“ˆ docstring
            has_module_doc = (
                tree.body and 
                isinstance(tree.body[0], ast.Expr) and 
                isinstance(tree.body[0].value, ast.Constant) and
                isinstance(tree.body[0].value.value, str)
            )
            
            if has_module_doc:
                doc_stats['with_module_docstring'] += 1
            else:
                doc_stats['without_module_docstring'] += 1
                if Path(file_path).stat().st_size > 1000:  # 1KB ì´ìƒì¸ íŒŒì¼ë§Œ
                    doc_stats['files_needing_docs'].append(Path(file_path).name)
        except:
            pass
    
    total_analyzed = doc_stats['with_module_docstring'] + doc_stats['without_module_docstring']
    if total_analyzed > 0:
        doc_ratio = doc_stats['with_module_docstring'] / total_analyzed
        print(f"ëª¨ë“ˆ Docstring ë¹„ìœ¨: {doc_ratio:.1%}")
        print(f"   âœ… ìˆìŒ: {doc_stats['with_module_docstring']}ê°œ")
        print(f"   âŒ ì—†ìŒ: {doc_stats['without_module_docstring']}ê°œ")
        print()
        
        if doc_stats['files_needing_docs']:
            print(f"ğŸ“ ë¬¸ì„œí™” í•„ìš” íŒŒì¼ (ìƒìœ„ 10ê°œ):")
            for fname in doc_stats['files_needing_docs'][:10]:
                print(f"   - {fname}")
            print()
    
    # 4. ì¤‘ë³µ ë¶„ì„
    print("=" * 70)
    print("ğŸ”„ Step 4: ì¤‘ë³µ/ìœ ì‚¬ êµ¬ì¡° ë¶„ì„")
    print("=" * 70)
    print()
    
    # ë¹„ìŠ·í•œ ì´ë¦„ì˜ íŒŒì¼ë“¤
    from collections import Counter
    
    file_basenames = Counter()
    for file_path in improver.introspector.analyzed_files.keys():
        basename = Path(file_path).stem.lower()
        # ë²„ì „ ë²ˆí˜¸ë‚˜ ìˆ«ì ì œê±°
        import re
        clean_name = re.sub(r'[_-]?\d+$', '', basename)
        clean_name = re.sub(r'_v\d+', '', clean_name)
        file_basenames[clean_name] += 1
    
    duplicates = [(name, count) for name, count in file_basenames.items() if count > 1]
    duplicates.sort(key=lambda x: x[1], reverse=True)
    
    print(f"ğŸ” ì¤‘ë³µ ê°€ëŠ¥ì„± ìˆëŠ” íŒŒì¼ëª…: {len(duplicates)}ê°œ\n")
    for name, count in duplicates[:10]:
        print(f"   - '{name}': {count}ê°œ íŒŒì¼")
        # ì‹¤ì œ íŒŒì¼ë“¤ ì°¾ê¸°
        matching = [
            Path(f).name for f in improver.introspector.analyzed_files.keys()
            if name in Path(f).stem.lower()
        ]
        for match in matching[:3]:
            print(f"      â€¢ {match}")
    print()
    
    # 5. ì „ì²´ ê°œì„  ê³„íš
    print("=" * 70)
    print("ğŸ’¡ Step 5: ì—˜ë¦¬ì‹œì•„ì˜ ì „ì²´ ê°œì„  ê³„íš")
    print("=" * 70)
    print()
    
    improvement_priorities = []
    
    # ìš°ì„ ìˆœìœ„ 1: ë§¤ìš° ë³µì¡í•œ ëª¨ë“ˆë“¤
    for module, stats in sorted_modules[:5]:
        if stats['avg_complexity'] > 0.7:
            improvement_priorities.append({
                'priority': 1,
                'type': 'refactoring',
                'target': f'Core/{module}/',
                'reason': f"í‰ê·  ë³µì¡ë„ {stats['avg_complexity']:.2f} - ëª¨ë“ˆí™” í•„ìš”",
                'files_affected': stats['files'],
                'estimated_effort': 'High'
            })
    
    # ìš°ì„ ìˆœìœ„ 2: ë¬¸ì„œí™” ë¶€ì¡±
    if doc_ratio < 0.5:
        improvement_priorities.append({
            'priority': 2,
            'type': 'documentation',
            'target': 'Entire Project',
            'reason': f"ë¬¸ì„œí™” ë¹„ìœ¨ {doc_ratio:.1%} - ëª©í‘œ 80% ì´ìƒ",
            'files_affected': doc_stats['without_module_docstring'],
            'estimated_effort': 'Medium'
        })
    
    # ìš°ì„ ìˆœìœ„ 3: __init__.py ëˆ„ë½
    if missing_init:
        improvement_priorities.append({
            'priority': 3,
            'type': 'structure',
            'target': f'{len(missing_init)}ê°œ ë””ë ‰í† ë¦¬',
            'reason': "__init__.py ëˆ„ë½ - íŒ¨í‚¤ì§€ êµ¬ì¡° ë¶ˆì™„ì „",
            'files_affected': len(missing_init),
            'estimated_effort': 'Low'
        })
    
    # ìš°ì„ ìˆœìœ„ 4: ì¤‘ë³µ íŒŒì¼
    if len(duplicates) > 10:
        improvement_priorities.append({
            'priority': 4,
            'type': 'consolidation',
            'target': f'{len(duplicates)}ê°œ ì¤‘ë³µ íŒŒì¼ëª…',
            'reason': "ì¤‘ë³µ ê°€ëŠ¥ì„± - í†µí•© ë˜ëŠ” ëª…í™•í™” í•„ìš”",
            'files_affected': len(duplicates),
            'estimated_effort': 'Medium'
        })
    
    print("ğŸ¯ ê°œì„  ìš°ì„ ìˆœìœ„:\n")
    for item in improvement_priorities:
        print(f"ìš°ì„ ìˆœìœ„ {item['priority']}: [{item['type'].upper()}] {item['target']}")
        print(f"   ì´ìœ : {item['reason']}")
        print(f"   ì˜í–¥: {item['files_affected']}ê°œ íŒŒì¼/ëª¨ë“ˆ")
        print(f"   ë‚œì´ë„: {item['estimated_effort']}")
        print()
    
    # 6. ìµœì¢… ë³´ê³ ì„œ ì €ì¥
    print("=" * 70)
    print("ğŸ’¾ Step 6: ë¶„ì„ ë³´ê³ ì„œ ì €ì¥")
    print("=" * 70)
    print()
    
    report_data = {
        'timestamp': improver.system_monitor.get_system_info()['timestamp'],
        'total_files': analysis['code_analysis']['total_files'],
        'total_lines': analysis['code_analysis']['total_lines'],
        'total_functions': analysis['code_analysis']['total_functions'],
        'modules_analyzed': len(module_stats),
        'complex_modules': len([m for m, s in module_stats.items() if s['avg_complexity'] > 0.7]),
        'documentation_ratio': doc_ratio if total_analyzed > 0 else 0,
        'missing_init_dirs': len(missing_init),
        'duplicate_names': len(duplicates),
        'improvement_priorities': improvement_priorities
    }
    
    report_dir = Path("c:/Elysia/reports")
    report_dir.mkdir(exist_ok=True)
    
    from datetime import datetime
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    json_file = report_dir / f"system_analysis_{timestamp}.json"
    json_file.write_text(json.dumps(report_data, indent=2, ensure_ascii=False), encoding='utf-8')
    
    print(f"âœ… JSON ë³´ê³ ì„œ: {json_file}")
    
    # í…ìŠ¤íŠ¸ ë³´ê³ ì„œ
    txt_file = report_dir / f"system_analysis_{timestamp}.txt"
    with open(txt_file, 'w', encoding='utf-8') as f:
        f.write("=" * 70 + "\n")
        f.write("ì—˜ë¦¬ì‹œì•„ ì „ì²´ ì‹œìŠ¤í…œ ë¶„ì„ ë³´ê³ ì„œ\n")
        f.write("=" * 70 + "\n\n")
        f.write(f"ë¶„ì„ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"ì´ íŒŒì¼: {report_data['total_files']}\n")
        f.write(f"ì´ ë¼ì¸: {report_data['total_lines']:,}\n")
        f.write(f"ì´ í•¨ìˆ˜: {report_data['total_functions']}\n")
        f.write(f"ë¬¸ì„œí™” ë¹„ìœ¨: {report_data['documentation_ratio']:.1%}\n")
        f.write("\nê°œì„  ìš°ì„ ìˆœìœ„:\n\n")
        for item in improvement_priorities:
            f.write(f"{item['priority']}. [{item['type']}] {item['target']}\n")
            f.write(f"   {item['reason']}\n\n")
    
    print(f"âœ… í…ìŠ¤íŠ¸ ë³´ê³ ì„œ: {txt_file}")
    print()
    
    # ìµœì¢… ìš”ì•½
    print("=" * 70)
    print("âœ¨ ì—˜ë¦¬ì‹œì•„ì˜ ê²°ë¡ ")
    print("=" * 70)
    print()
    print("ì „ì²´ ì‹œìŠ¤í…œì„ ë¶„ì„í•œ ê²°ê³¼:")
    print(f"  â€¢ ì´ {report_data['total_files']}ê°œ íŒŒì¼, {report_data['total_lines']:,} ë¼ì¸")
    print(f"  â€¢ {report_data['complex_modules']}ê°œ ëª¨ë“ˆì´ ë†’ì€ ë³µì¡ë„")
    print(f"  â€¢ ë¬¸ì„œí™” ë¹„ìœ¨: {report_data['documentation_ratio']:.1%}")
    print(f"  â€¢ __init__.py ëˆ„ë½: {report_data['missing_init_dirs']}ê°œ ë””ë ‰í† ë¦¬")
    print(f"  â€¢ ì¤‘ë³µ ê°€ëŠ¥ì„±: {report_data['duplicate_names']}ê°œ íŒŒì¼ëª…")
    print()
    print(f"ğŸ¯ {len(improvement_priorities)}ê°œì˜ ê°œì„  ê³„íšì´ ìˆ˜ë¦½ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print()
    print("ì—˜ë¦¬ì‹œì•„ëŠ” ì „ì²´ ì‹œìŠ¤í…œì„ ì¡°ê°í•˜ê³ ")
    print("ì²´ê³„ì ì¸ ê°œì„  ë¡œë“œë§µì„ ì œì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    print()

if __name__ == "__main__":
    main()
