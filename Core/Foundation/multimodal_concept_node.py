"""
Multimodal Concept Node (ë©€í‹°ëª¨ë‹¬ ê°œë… ë…¸ë“œ)
============================================

"ëª¨ë“  ê°ê°ì€ í•˜ë‚˜ì˜ ì˜ë¯¸ë¡œ ìˆ˜ë ´í•œë‹¤."

ì‚¬ê³¼ì˜ ëª¨ë“  ê²ƒ:
- í…ìŠ¤íŠ¸: "ê³¼ì¼, ë¹¨ê°„ìƒ‰"
- ì‹œê°: ë‘¥ê¸€ê³  ë¹¨ê°„ í˜•íƒœ
- ì´‰ê°: ì•„ì‚­í•œ ì‹ê°
- ë¯¸ê°: ì‹ ë§›
â†’ ëª¨ë‘ "ì‚¬ê³¼"ë¼ëŠ” í•˜ë‚˜ì˜ ê°œë… ë…¸ë“œë¡œ í†µí•©

[NEW 2025-12-15] ë©€í‹°ëª¨ë‹¬ ê°œë… í†µí•© ì‹œìŠ¤í…œ
"""

import logging
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, field
import numpy as np

# [PHASE 7.2] Import Phase Stratum for Holographic Layering
try:
    from Core.Intelligence.Topography.phase_stratum import PhaseStratum
except ImportError:
    PhaseStratum = None

logger = logging.getLogger("MultimodalConceptNode")


@dataclass
class ModalitySignal:
    """í•˜ë‚˜ì˜ ê°ê° ì‹ í˜¸"""
    modality_type: str  # visual, auditory, text, taste, texture, emotion
    frequency: float    # ì£¼íŒŒìˆ˜ (Hz)
    amplitude: float    # ê°•ë„ (0.0 ~ 1.0)
    description: str    # ì„¤ëª…
    raw_data: Any = None  # ì›ë³¸ ë°ì´í„°


@dataclass
class ConceptNode:
    """
    ë©€í‹°ëª¨ë‹¬ ê°œë… ë…¸ë“œ
    
    ëª¨ë“  ê°ê° ì •ë³´ê°€ í•˜ë‚˜ì˜ ê°œë…ìœ¼ë¡œ í†µí•©ë¨
    ìê¸° ìˆ˜ì • ë° ê°œë… ë¶„ë³„ ê¸°ëŠ¥ í¬í•¨
    """
    name: str
    name: str
    # [REFACTORED] Removed legacy 'modalities' dict. Data lives in PhaseStratum now.
    unified_frequency: float = 0.0
    unified_amplitude: float = 0.0
    related_concepts: List[str] = field(default_factory=list)
    change_history: List[Dict] = field(default_factory=list)  # ìˆ˜ì • ì´ë ¥
    
    # [NEW] Phase Stratum Engine for this node
    phase_stratum: Any = field(default=None)

    def __post_init__(self):
        if self.phase_stratum is None and PhaseStratum:
            self.phase_stratum = PhaseStratum(base_frequency=432.0)
    
    def add_modality(self, signal: ModalitySignal):
        """ê°ê° ì‹ í˜¸ ì¶”ê°€"""
        # [OLD REMOVED] self.modalities[signal.modality_type] = signal
        
        # [NEW] Fold into Holographic Layer (Sole Source of Truth)
        if self.phase_stratum:
            self.phase_stratum.fold_dimension(
                data=signal, 
                intent_frequency=signal.frequency
            )
            
        self._recalculate_unified()
    
    def update_modality(self, modality_type: str, new_description: str, new_frequency: float = None):
        """
        [NOTE] With PhaseStratum, we don't 'update' a slot. 
        We simply add a new wave that might supersede the old one in relevance,
        or we'd need a specific retrieval-deletion logic.
        For now, we append the new truth as a new layer.
        """
        # Simplification for Pure Wave: Just add the new signal
        # Finding the old one to log history is expensive in a pure wave system without indexing,
        # but we can do it if needed. For now, we trust the "Latest Resonance".
        pass # To be reimplemented if mutation is strictly needed.
    
    def _get_all_signals_dict(self) -> Dict[str, ModalitySignal]:
        """Helper: Extract all ModalitySignals from PhaseStratum into a dict by type"""
        if not self.phase_stratum:
            return {}
        
        signals = {}
        for _, _, payload in self.phase_stratum.inspect_all_layers():
            if isinstance(payload, ModalitySignal):
                signals[payload.modality_type] = payload
        return signals

    def compare_with(self, other: 'ConceptNode') -> Dict[str, Any]:
        """
        ê°œë… ë¶„ë³„: ë‘ ê°œë… ê°„ ìœ ì‚¬ì„±ê³¼ ì°¨ì´ ë¶„ì„
        (Supports PhaseStratum Architecture)
        """
        shared_modalities = []
        different_modalities = []
        only_self = []
        only_other = []
        
        # 1. Extract signals from Holographic Layers
        my_signals = self._get_all_signals_dict()
        other_signals = other._get_all_signals_dict()
        
        all_types = set(my_signals.keys()) | set(other_signals.keys())
        
        for m_type in all_types:
            self_has = m_type in my_signals
            other_has = m_type in other_signals
            
            if self_has and other_has:
                # ë‘˜ ë‹¤ ìˆìŒ â†’ ì£¼íŒŒìˆ˜ ì°¨ì´ ë¹„êµ
                self_freq = my_signals[m_type].frequency
                other_freq = other_signals[m_type].frequency
                diff = abs(self_freq - other_freq)
                
                if diff < 50:  # ìœ ì‚¬
                    shared_modalities.append({
                        "type": m_type,
                        "resonance": 1.0 - (diff / 500),
                        "self_desc": my_signals[m_type].description,
                        "other_desc": other_signals[m_type].description
                    })
                else:  # ë‹¤ë¦„
                    different_modalities.append({
                        "type": m_type,
                        "self_freq": self_freq,
                        "other_freq": other_freq,
                        "difference": diff
                    })
            elif self_has:
                only_self.append(m_type)
            else:
                only_other.append(m_type)
        
        # ì „ì²´ ìœ ì‚¬ë„ ê³„ì‚°
        total_resonance = self.get_resonance(other.unified_frequency)
        
        return {
            "overall_resonance": total_resonance,
            "shared": shared_modalities,
            "different": different_modalities,
            "only_in_self": only_self,
            "only_in_other": only_other,
            "is_same_category": total_resonance > 0.7,  # ê°™ì€ ë²”ì£¼
            "is_distinct": len(different_modalities) > 0  # êµ¬ë³„ ê°€ëŠ¥
        }
    
    def _recalculate_unified(self):
        """í†µí•© ì£¼íŒŒìˆ˜ ì¬ê³„ì‚° (Pure Wave Version)"""
        if not self.phase_stratum:
            return
            
        # Inspect all waves in the strata
        all_waves = self.phase_stratum.inspect_all_layers()
        if not all_waves:
            return
        
        total_weight = 0.0
        weighted_freq = 0.0
        weighted_amp = 0.0
        count = 0
        
        for freq, phase, signal in all_waves:
            if isinstance(signal, ModalitySignal):
                weight = signal.amplitude
                weighted_freq += freq * weight
                weighted_amp += signal.amplitude
                total_weight += weight
                count += 1
        
        if total_weight > 0:
            self.unified_frequency = weighted_freq / total_weight
            self.unified_amplitude = weighted_amp / count
    
    def get_resonance(self, other_freq: float) -> float:
        """ë‹¤ë¥¸ ì£¼íŒŒìˆ˜ì™€ì˜ ê³µëª…ë„ ê³„ì‚°"""
        if self.unified_frequency == 0:
            return 0.0
        
        # ì£¼íŒŒìˆ˜ ì°¨ì´ ê¸°ë°˜ ê³µëª… (ê°€ê¹Œìš¸ìˆ˜ë¡ 1.0)
        diff = abs(self.unified_frequency - other_freq)
        max_diff = 500.0  # ìµœëŒ€ ì°¨ì´
        resonance = max(0.0, 1.0 - (diff / max_diff))
        
        return resonance

    def get_perspective(self, query_frequency: float, tolerance: float = 10.0) -> List[Any]:
        """
        [Holographic Retrieval]
        ì£¼íŒŒìˆ˜ì— ë”°ë¼ ë…¸ë“œì˜ ë‹¤ë¥¸ 'ë‹¨ë©´'ì„ ë³´ì—¬ì¤Œ.
        
        ì˜ˆ: 
          - 640Hz (Red)ë¡œ ì˜ë©´ -> "Visual Red" ë°˜í™˜
          - 528Hz (Sweet)ë¡œ ì˜ë©´ -> "Taste Sweet" ë°˜í™˜
        """
        if self.phase_stratum:
            return self.phase_stratum.resonate(query_frequency, tolerance)
        return []


class MultimodalConceptIntegrator:
    """
    ë©€í‹°ëª¨ë‹¬ ê°œë… í†µí•©ê¸°
    
    SynesthesiaEngineì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë“  ê°ê°ì„ ì£¼íŒŒìˆ˜ë¡œ ë³€í™˜í•˜ê³ 
    í•˜ë‚˜ì˜ ê°œë… ë…¸ë“œë¡œ í†µí•©
    """
    
    def __init__(self):
        logger.info("ğŸ¨ Initializing Multimodal Concept Integrator...")
        
        # SynesthesiaEngine ì—°ê²°
        try:
            from Core.Foundation.synesthesia_engine import SynesthesiaEngine, SignalType
            self.synesthesia = SynesthesiaEngine()
            self.SignalType = SignalType
            logger.info("   âœ… SynesthesiaEngine connected")
        except Exception as e:
            logger.warning(f"   âš ï¸ SynesthesiaEngine not available: {e}")
            self.synesthesia = None
        
        # TextWaveConverter ì—°ê²°
        try:
            from Core.Foundation.text_wave_converter import TextWaveConverter
            self.text_wave = TextWaveConverter()
            logger.info("   âœ… TextWaveConverter connected")
        except Exception as e:
            logger.warning(f"   âš ï¸ TextWaveConverter not available: {e}")
            self.text_wave = None
        
        # ê°œë… ì €ì¥ì†Œ
        self.concepts: Dict[str, ConceptNode] = {}
        
        # ê°ê° í‚¤ì›Œë“œ â†’ ì£¼íŒŒìˆ˜ ë§¤í•‘ (í™•ì¥ ê°€ëŠ¥)
        self.sensory_keywords = {
            # ë¯¸ê°
            "taste": {
                "sweet": 528.0, "ë‹¨ë§›": 528.0, "ë‹¬ì½¤": 528.0,
                "sour": 396.0, "ì‹ ë§›": 396.0, "ìƒˆì½¤": 396.0,
                "bitter": 417.0, "ì“´ë§›": 417.0,
                "salty": 432.0, "ì§ ë§›": 432.0,
                "umami": 639.0, "ê°ì¹ ë§›": 639.0,
            },
            # ì´‰ê°/ì‹ê°
            "texture": {
                "crunchy": 412.0, "ì•„ì‚­": 412.0, "ë°”ì‚­": 412.0,
                "soft": 396.0, "ë¶€ë“œëŸ¬ìš´": 396.0, "ì´‰ì´‰": 396.0,
                "hard": 528.0, "ë”±ë”±": 528.0,
                "smooth": 432.0, "ë§¤ë„ëŸ¬ìš´": 432.0,
            },
            # ì‹œê° (ìƒ‰ìƒ)
            "visual": {
                "red": 640.0, "ë¹¨ê°„": 640.0, "ë¹¨ê°•": 640.0,
                "orange": 600.0, "ì£¼í™©": 600.0,
                "yellow": 560.0, "ë…¸ë€": 560.0, "ë…¸ë‘": 560.0,
                "green": 520.0, "ì´ˆë¡": 520.0, "ë…¹ìƒ‰": 520.0,
                "blue": 480.0, "íŒŒë€": 480.0, "íŒŒë‘": 480.0,
                "purple": 420.0, "ë³´ë¼": 420.0,
                "round": 432.0, "ë‘¥ê·¼": 432.0,
            },
        }
        
        logger.info("ğŸ¨ Multimodal Concept Integrator ready")
    
    def create_concept(self, name: str) -> ConceptNode:
        """ìƒˆ ê°œë… ë…¸ë“œ ìƒì„±"""
        if name not in self.concepts:
            self.concepts[name] = ConceptNode(name=name)
            logger.info(f"ğŸ“¦ Created concept: {name}")
        return self.concepts[name]
    
    def add_text_to_concept(self, concept_name: str, text: str) -> ModalitySignal:
        """í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ê°œë…ì— ì¶”ê°€"""
        concept = self.create_concept(concept_name)
        
        # TextWaveConverterë¡œ ì£¼íŒŒìˆ˜ ë³€í™˜
        if self.text_wave:
            wave = self.text_wave.sentence_to_wave(text)
            desc = self.text_wave.wave_to_text_descriptor(wave)
            freq = desc.get("dominant_frequency", 432.0)
        else:
            # í´ë°±: í•´ì‹œ ê¸°ë°˜
            freq = 200.0 + (hash(text) % 400)
        
        signal = ModalitySignal(
            modality_type="text",
            frequency=freq,
            amplitude=0.8,
            description=text[:100],
            raw_data=text
        )
        
        concept.add_modality(signal)
        logger.info(f"   ğŸ“ Text â†’ {freq:.0f}Hz: {text[:30]}...")
        
        return signal
    
    def add_sensory_to_concept(self, concept_name: str, modality: str, description: str) -> ModalitySignal:
        """
        ê°ê° ì •ë³´ë¥¼ ê°œë…ì— ì¶”ê°€ (ë¯¸ê°, ì´‰ê°, ì‹œê° ë“±)
        
        ì˜ˆ: add_sensory_to_concept("ì‚¬ê³¼", "taste", "ì‹ ë§›")
        """
        concept = self.create_concept(concept_name)
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ì£¼íŒŒìˆ˜ ê²°ì •
        freq = 432.0  # ê¸°ë³¸ê°’
        keywords = self.sensory_keywords.get(modality, {})
        
        for keyword, keyword_freq in keywords.items():
            if keyword in description.lower():
                freq = keyword_freq
                break
        
        signal = ModalitySignal(
            modality_type=modality,
            frequency=freq,
            amplitude=0.7,
            description=description,
            raw_data=None
        )
        
        concept.add_modality(signal)
        logger.info(f"   ğŸ­ {modality} â†’ {freq:.0f}Hz: {description}")
        
        return signal
    
    def add_visual_to_concept(self, concept_name: str, image_data: np.ndarray) -> ModalitySignal:
        """ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ê°œë…ì— ì¶”ê°€"""
        concept = self.create_concept(concept_name)
        
        if self.synesthesia:
            signal_data = self.synesthesia.from_vision(image_data)
            freq = signal_data.frequency
            amp = signal_data.amplitude
        else:
            freq = float(np.mean(image_data)) + 400
            amp = 0.5
        
        signal = ModalitySignal(
            modality_type="visual",
            frequency=freq,
            amplitude=amp,
            description=f"Image {image_data.shape}",
            raw_data=image_data.shape
        )
        
        concept.add_modality(signal)
        logger.info(f"   ğŸ‘ï¸ Visual â†’ {freq:.0f}Hz")
        
        return signal
    
    def build_concept_from_text(self, concept_name: str, text: str) -> ConceptNode:
        """
        í…ìŠ¤íŠ¸ì—ì„œ ìë™ìœ¼ë¡œ ë©€í‹°ëª¨ë‹¬ ê°œë… êµ¬ì¶•
        
        "ì‚¬ê³¼ëŠ” ë¹¨ê°„ìƒ‰ì´ê³  ì‹ ë§›ì´ ë‚˜ë©° ì•„ì‚­í•œ ì‹ê°ì„ ê°€ì§„ë‹¤"
        â†’ ìë™ìœ¼ë¡œ visual(ë¹¨ê°„), taste(ì‹ ë§›), texture(ì•„ì‚­) ì¶”ì¶œ
        """
        concept = self.create_concept(concept_name)
        
        # 1. í…ìŠ¤íŠ¸ ì „ì²´ ì¶”ê°€
        self.add_text_to_concept(concept_name, text)
        
        # 2. ê°ê° í‚¤ì›Œë“œ ìë™ ì¶”ì¶œ
        text_lower = text.lower()
        
        for modality, keywords in self.sensory_keywords.items():
            for keyword in keywords:
                if keyword in text_lower:
                    self.add_sensory_to_concept(concept_name, modality, keyword)
        
        logger.info(f"âœ¨ Built concept '{concept_name}' with {len(concept.modalities)} modalities")
        logger.info(f"   Unified frequency: {concept.unified_frequency:.0f}Hz")
        
        return concept
    
    def find_resonant_concepts(self, query_freq: float, threshold: float = 0.3) -> List[tuple]:
        """ì£¼íŒŒìˆ˜ì™€ ê³µëª…í•˜ëŠ” ëª¨ë“  ê°œë… ê²€ìƒ‰"""
        results = []
        
        for name, concept in self.concepts.items():
            resonance = concept.get_resonance(query_freq)
            if resonance >= threshold:
                results.append((name, concept, resonance))
        
        # ê³µëª…ë„ ìˆœ ì •ë ¬
        results.sort(key=lambda x: x[2], reverse=True)
        
        return results
    
    def get_concept_summary(self, concept_name: str) -> Dict[str, Any]:
        """ê°œë… ë…¸ë“œ ìš”ì•½"""
        if concept_name not in self.concepts:
            return {"error": "Concept not found"}
        
        concept = self.concepts[concept_name]
        
        return {
            "name": concept.name,
            "unified_frequency": concept.unified_frequency,
            "modalities": {
                # Reconstruct dict for summary view only
                str(idx): {
                    "frequency": item[0],
                    "description": item[2].description if isinstance(item[2], ModalitySignal) else str(item[2])
                }
                for idx, item in enumerate(concept.phase_stratum.inspect_all_layers())
            },
            "modality_count": len(concept.phase_stratum.inspect_all_layers())
        }
        }


# Singleton
_integrator = None

def get_multimodal_integrator() -> MultimodalConceptIntegrator:
    global _integrator
    if _integrator is None:
        _integrator = MultimodalConceptIntegrator()
    return _integrator


# Demo
if __name__ == "__main__":
    import sys
    sys.path.insert(0, "c:\\Elysia")
    
    logging.basicConfig(level=logging.INFO, format='%(message)s')
    
    print("\n" + "="*60)
    print("ğŸ¨ MULTIMODAL CONCEPT INTEGRATION DEMO")
    print("="*60)
    
    integrator = get_multimodal_integrator()
    
    # ì‚¬ê³¼ ê°œë… êµ¬ì¶•
    print("\nğŸ“¦ Building concept: ì‚¬ê³¼")
    print("-"*40)
    
    apple = integrator.build_concept_from_text(
        "ì‚¬ê³¼",
        "ì‚¬ê³¼ëŠ” ë¹¨ê°„ìƒ‰ì´ê³  ì‹ ë§›ì´ ë‚˜ë©° ì•„ì‚­í•œ ì‹ê°ì„ ê°€ì§„ ë‘¥ê·¼ ê³¼ì¼ì´ë‹¤"
    )
    
    # ê²°ê³¼ ì¶œë ¥
    summary = integrator.get_concept_summary("ì‚¬ê³¼")
    
    print("\nğŸ“Š Concept Summary:")
    print(f"   Name: {summary['name']}")
    print(f"   Unified Frequency: {summary['unified_frequency']:.0f}Hz")
    print(f"   Modalities: {summary['modality_count']}")
    for m_type, data in summary['modalities'].items():
        print(f"      {m_type}: {data['frequency']:.0f}Hz - {data['description']}")
    
    # ê³µëª… ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print("\nğŸ” Resonance Search (640Hz - red color):")
    results = integrator.find_resonant_concepts(640.0)
    for name, concept, resonance in results:
        print(f"   {name}: {resonance:.2f}")
    
    print("\n" + "="*60)
    print("âœ… Demo complete")
    print("="*60)
