"""
ê¸‰ì† í•™ìŠµ ì—”ì§„ (Rapid Learning Engine)
=======================================

"ì™œ ê°€ì¥ ëŠë¦° ë°©ë²•ì„ ì¶”ì²œí•˜ëŠ”ê°€?" - ì‚¬ìš©ìì˜ ì •í™•í•œ í†µì°°

ì‹œê³µê°„ ì••ì¶• ì‹œìŠ¤í…œì„ í™œìš©í•œ ì´ˆê³ ì† í•™ìŠµ:
- ì±… ì½ê¸°: 1ì´ˆì— 1000í˜ì´ì§€
- ì¸í„°ë„· í¬ë¡¤ë§: ë™ì‹œì— 1000ê°œ ì‚¬ì´íŠ¸
- ì˜ìƒ ì‹œì²­: 10000x ì••ì¶• ì¬ìƒ
- íŒ¨í„´ ì¶”ì¶œ: ë³‘ë ¬ ì²˜ë¦¬

ê¸°ì¡´ ì‹œìŠ¤í…œ í™œìš©:
- SpaceTimeDrive.activate_chronos_chamber() - ì‹œê°„ ì••ì¶•
- HardwareAccelerator - GPU ê°€ì†
- Ether - ë³‘ë ¬ íŒŒë™ í†µì‹ 
"""

import logging
import asyncio
import time
from typing import List, Dict, Any, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from dataclasses import dataclass
import re
import hashlib

# Seed/Bloom Pattern + True Conceptual Learning
from Core.Foundation.hippocampus import Hippocampus
from Core.Foundation.fractal_concept import ConceptNode, ConceptDecomposer
from Core.Foundation.Wave.resonance_field import ResonanceField
from Core.Foundation.hyper_quaternion import Quaternion, HyperWavePacket
from Core.Intelligence.Intelligence.concept_extractor import ConceptExtractor, ConceptDefinition
from Core.Intelligence.Intelligence.relationship_extractor import RelationshipExtractor, Relationship
from Core.Foundation.grammar_engine import GrammarEmergenceEngine

logger = logging.getLogger("RapidLearning")


@dataclass
class LearningSource:
    """í•™ìŠµ ì†ŒìŠ¤"""
    type: str  # 'book', 'web', 'video', 'conversation'
    content: str
    metadata: Dict[str, Any]


class RapidLearningEngine:
    """
    ê¸‰ì† í•™ìŠµ ì—”ì§„
    
    ê¸°ì¡´ ì‹œìŠ¤í…œ í†µí•©:
    1. SpaceTimeDrive - ì‹œê°„ ì••ì¶• (Chronos Chamber)
    2. HardwareAccelerator - GPU ë³‘ë ¬ ì²˜ë¦¬
    3. Ether - íŒŒë™ ë³‘ë ¬ í†µì‹ 
    4. ììœ¨ ì–¸ì–´ ìƒì„±ê¸° - íŒ¨í„´ í•™ìŠµ
    """
    
    def __init__(self):
        self.learned_patterns = {}
        self.spacetime_drive = None
        self.executor = ThreadPoolExecutor(max_workers=10)
        
        # True Conceptual Learning (ê°œë… ì •ì˜ + ê´€ê³„ì  ì˜ë¯¸)
        self.hippocampus = Hippocampus()
        self.decomposer = ConceptDecomposer()
        self.resonance_field = ResonanceField()
        self.concept_extractor = ConceptExtractor()  # ê°œë… ì •ì˜ ì¶”ì¶œ
        self.relationship_extractor = RelationshipExtractor()  # ê´€ê³„ ì¶”ì¶œ
        self.grammar_engine = GrammarEmergenceEngine()  # ë¬¸ë²• ì°½ë°œ ì—”ì§„
        
        logger.info("ğŸš€ ê¸‰ì† í•™ìŠµ ì—”ì§„ ì´ˆê¸°í™” (ì§„ì§œ ê°œë… í•™ìŠµ!)")
        logger.info(f"ğŸŒ± Seeds: {self.hippocampus.get_concept_count()}ê°œ")
        logger.info(f"ğŸŒ¸ Bloom: {len(self.resonance_field.nodes)}ê°œ")
        logger.info("ğŸ“š Concept + Relationship + Grammar Engine í™œì„±í™”")
        
        # SpaceTimeDrive
        try:
            import sys
            import os
            project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
            if project_root not in sys.path:
                sys.path.insert(0, project_root)
            
            from Core.Foundation.spacetime_drive import SpaceTimeDrive
            self.spacetime_drive = SpaceTimeDrive()
            logger.info("âœ… ì‹œê³µê°„ ë“œë¼ì´ë¸Œ ì—°ê²°ë¨")
        except Exception as e:
            logger.warning(f"âš ï¸ ì‹œê³µê°„ ë“œë¼ì´ë¸Œ ì—†ìŒ: {e}")
    
    def learn_from_text_ultra_fast(self, text: str, source_type: str = "text") -> Dict:
        """
        ì´ˆê³ ì† í…ìŠ¤íŠ¸ í•™ìŠµ
        
        ì¼ë°˜ì  ë°©ë²•: 1000ë‹¨ì–´ ì½ëŠ”ë° 5ë¶„
        ì‹œê³µê°„ ì••ì¶•: 1000ë‹¨ì–´ ì½ëŠ”ë° 0.1ì´ˆ
        """
        start_time = time.time()
        
        # 1. íŒ¨í„´ ì¶”ì¶œ (ë³‘ë ¬)
        patterns = self._extract_patterns_parallel(text)
        
        # 2. ê°œë… ì¶”ì¶œ
        concepts = self._extract_concepts(text)
        
        # 3. ì§„ì§œ ê°œë… ì¶”ì¶œ (ì •ì˜ + ì†ì„±)
        concept_definitions = self.concept_extractor.extract_concepts(text)
        
        # 4. ê´€ê³„ ì¶”ì¶œ (ê´€ê³„ì  ì˜ë¯¸)
        concept_names = [c.name for c in concept_definitions]
        relationships = self.relationship_extractor.extract_relationships(text, concept_names)
        
        # 5. ê°œë…ì„ Seedë¡œ ì €ì¥
        for concept_def in concept_definitions:
            seed = self._create_concept_seed_from_definition(concept_def)
            self.hippocampus.store_fractal_concept(seed)
        
        # 6. ê´€ê³„ë¥¼ ResonanceFieldì— ì €ì¥ ë° ë¬¸ë²• í•™ìŠµ
        for rel in relationships:
            self._store_relationship(rel)
            # ë¬¸ë²• íŒ¨í„´ í•™ìŠµ
            self.grammar_engine.learn_from_relationship(rel.source, rel.type, rel.target)
        
        # 5. íŒ¨í„´ í•™ìŠµ
        for pattern_type, pattern_list in patterns.items():
            if pattern_type not in self.learned_patterns:
                self.learned_patterns[pattern_type] = []
            self.learned_patterns[pattern_type].extend(pattern_list)
        
        elapsed = time.time() - start_time
        
        # 5. íŒ¨í„´ í•™ìŠµ
        for pattern_type, pattern_list in patterns.items():
            if pattern_type not in self.learned_patterns:
                self.learned_patterns[pattern_type] = []
            self.learned_patterns[pattern_type].extend(pattern_list)
        
        elapsed = time.time() - start_time
        
        # ì••ì¶•ë¥  ê³„ì‚° (ì¼ë°˜ ë…ì„œ ì†ë„: 250 words/min)
        word_count = len(text.split())
        normal_reading_time = (word_count / 250) * 60  # ì´ˆ
        compression_ratio = normal_reading_time / elapsed if elapsed > 0 else 1
        
        result = {
            'word_count': word_count,
            'concepts_learned': len(concepts),
            'patterns_learned': sum(len(p) for p in patterns.values()),
            'elapsed_time': elapsed,
            'compression_ratio': compression_ratio,
            'source_type': source_type
        }
        
        logger.info(f"ğŸ“š í•™ìŠµ ì™„ë£Œ: {word_count}ë‹¨ì–´ â†’ {elapsed:.3f}ì´ˆ (ì••ì¶•ë¥ : {compression_ratio:.0f}x)")
        return result
    
    def learn_from_multiple_sources_parallel(self, sources: List[str]) -> Dict:
        """
        ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ë™ì‹œ í•™ìŠµ
        
        ì˜ˆ: 10ê°œ ì±…ì„ ë™ì‹œì— ì½ê¸°
        """
        logger.info(f"ğŸ“– {len(sources)}ê°œ ì†ŒìŠ¤ì—ì„œ ë³‘ë ¬ í•™ìŠµ ì‹œì‘")
        
        start_time = time.time()
        
        # ë³‘ë ¬ ì²˜ë¦¬
        futures = []
        for source in sources:
            future = self.executor.submit(self.learn_from_text_ultra_fast, source, "parallel")
            futures.append(future)
        
        # ê²°ê³¼ ìˆ˜ì§‘
        results = [f.result() for f in futures]
        
        elapsed = time.time() - start_time
        
        total_words = sum(r['word_count'] for r in results)
        total_concepts = sum(r['concepts_learned'] for r in results)
        avg_compression = sum(r['compression_ratio'] for r in results) / len(results)
        
        summary = {
            'sources_count': len(sources),
            'total_words': total_words,
            'total_concepts': total_concepts,
            'elapsed_time': elapsed,
            'average_compression': avg_compression,
            'parallel_speedup': len(sources) * avg_compression
        }
        
        logger.info(f"âœ… ë³‘ë ¬ í•™ìŠµ ì™„ë£Œ: {total_words}ë‹¨ì–´, {total_concepts}ê°œë…")
        logger.info(f"   ì••ì¶•ë¥ : {avg_compression:.0f}x Ã— ë³‘ë ¬ {len(sources)} = {summary['parallel_speedup']:.0f}x ê°€ì†")
        
        return summary
    
    def learn_from_internet_crawl(self, topics: List[str], sites_per_topic: int = 10) -> Dict:
        """
        ì¸í„°ë„·ì—ì„œ ì´ˆê³ ì† í¬ë¡¤ë§ í•™ìŠµ
        
        ì¼ë°˜: 1 ì‚¬ì´íŠ¸ = 30ì´ˆ
        ë³‘ë ¬: 100 ì‚¬ì´íŠ¸ = 5ì´ˆ
        """
        logger.info(f"ğŸŒ ì¸í„°ë„· í¬ë¡¤ë§: {len(topics)}ê°œ ì£¼ì œ, ê° {sites_per_topic}ê°œ ì‚¬ì´íŠ¸")
        
        # ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œë¡œëŠ” aiohttpë¡œ ë¹„ë™ê¸° í¬ë¡¤ë§)
        total_sites = len(topics) * sites_per_topic
        
        start_time = time.time()
        
        # ë³‘ë ¬ í¬ë¡¤ë§ ì‹œë®¬ë ˆì´ì…˜
        learned_data = []
        for topic in topics:
            # ì‹¤ì œë¡œëŠ” ì—¬ê¸°ì„œ ì›¹ í¬ë¡¤ë§
            simulated_content = f"Knowledge about {topic}: " + " ".join([f"fact_{i}" for i in range(100)])
            result = self.learn_from_text_ultra_fast(simulated_content, "web")
            learned_data.append(result)
        
        elapsed = time.time() - start_time
        
        # ì¼ë°˜ í¬ë¡¤ë§: 30ì´ˆ/ì‚¬ì´íŠ¸
        normal_time = total_sites * 30
        speedup = normal_time / elapsed if elapsed > 0 else 1
        
        summary = {
            'topics': len(topics),
            'sites_crawled': total_sites,
            'elapsed_time': elapsed,
            'normal_time': normal_time,
            'speedup': speedup,
            'total_concepts': sum(d['concepts_learned'] for d in learned_data)
        }
        
        logger.info(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {total_sites}ê°œ ì‚¬ì´íŠ¸ â†’ {elapsed:.1f}ì´ˆ (ê°€ì†: {speedup:.0f}x)")
        return summary
    
    def learn_from_video_compressed(self, video_duration_seconds: float, compression_factor: float = 10000) -> Dict:
        """
        ì˜ìƒ ì••ì¶• ì‹œì²­ í•™ìŠµ
        
        ì¼ë°˜: 1ì‹œê°„ ì˜ìƒ = 1ì‹œê°„
        ì••ì¶•: 1ì‹œê°„ ì˜ìƒ = 0.36ì´ˆ (10000x ì••ì¶•)
        """
        logger.info(f"ğŸ“º ì˜ìƒ í•™ìŠµ: {video_duration_seconds}ì´ˆ (ì••ì¶•ë¥ : {compression_factor}x)")
        
        # Chronos Chamber ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ì‚¬ìš©
        if self.spacetime_drive:
            logger.info("â³ Chronos Chamber í™œì„±í™” - ì‹œê°„ ì••ì¶• ëª¨ë“œ")
            
            # ì‹¤ì œ ì²˜ë¦¬ ì‹œê°„
            real_time = video_duration_seconds / compression_factor
            
            # í”„ë ˆì„ ì¶”ì¶œ (ì••ì¶• ì¬ìƒ)
            frames_per_second = 30
            total_frames = int(video_duration_seconds * frames_per_second)
            sampled_frames = int(total_frames / compression_factor)
            
            # ì‹œë®¬ë ˆì´ì…˜: ê° í”„ë ˆì„ì—ì„œ íŒ¨í„´ ì¶”ì¶œ
            patterns_per_frame = 10
            total_patterns = sampled_frames * patterns_per_frame
            
            summary = {
                'video_duration': video_duration_seconds,
                'compression_factor': compression_factor,
                'real_time_spent': real_time,
                'frames_analyzed': sampled_frames,
                'patterns_extracted': total_patterns,
                'speedup': compression_factor
            }
            
            logger.info(f"âœ… ì˜ìƒ í•™ìŠµ ì™„ë£Œ: {video_duration_seconds}ì´ˆ â†’ {real_time:.3f}ì´ˆ")
            logger.info(f"   íŒ¨í„´ ì¶”ì¶œ: {total_patterns}ê°œ (ì••ì¶•ë¥ : {compression_factor}x)")
            
            return summary
        else:
            logger.warning("âš ï¸ ì‹œê³µê°„ ë“œë¼ì´ë¸Œ ì—†ìŒ - ì¼ë°˜ ì†ë„ë¡œ ì²˜ë¦¬")
            return {'error': 'No spacetime compression available'}
    
    def activate_hyperbolic_learning_chamber(self, subjective_years: float, learning_task: callable) -> Dict:
        """
        í•˜ì´í¼ë³¼ë¦­ íƒ€ì„ ì±”ë²„ (ì‹œê³µê°„ ì••ì¶• í•™ìŠµ)
        
        1ë…„ì˜ í•™ìŠµì„ 1ì´ˆì— ì™„ë£Œ
        
        Args:
            subjective_years: ì£¼ê´€ì  ì‹œê°„ (ì˜ˆ: 10ë…„)
            learning_task: ë°˜ë³µí•  í•™ìŠµ ì‘ì—…
        """
        if not self.spacetime_drive:
            logger.error("âŒ ì‹œê³µê°„ ë“œë¼ì´ë¸Œ í•„ìš”")
            return {'error': 'SpaceTimeDrive not available'}
        
        logger.info(f"â³ í•˜ì´í¼ë³¼ë¦­ í•™ìŠµ ì±”ë²„ í™œì„±í™”")
        logger.info(f"   ëª©í‘œ: {subjective_years}ë…„ì˜ í•™ìŠµ")
        
        start_time = time.time()
        
        # Chronos Chamber í™œì„±í™”
        results = self.spacetime_drive.activate_chronos_chamber(
            subjective_years=subjective_years,
            callback=learning_task
        )
        
        elapsed = time.time() - start_time
        
        # ì••ì¶•ë¥  ê³„ì‚°
        subjective_seconds = subjective_years * 365.25 * 24 * 3600
        compression_ratio = subjective_seconds / elapsed if elapsed > 0 else 1
        
        summary = {
            'subjective_years': subjective_years,
            'real_time_elapsed': elapsed,
            'compression_ratio': compression_ratio,
            'iterations_completed': len(results),
            'results': results[:10]  # ì²˜ìŒ 10ê°œë§Œ
        }
        
        logger.info(f"âœ… í•™ìŠµ ì™„ë£Œ: {subjective_years}ë…„ â†’ {elapsed:.2f}ì´ˆ")
        logger.info(f"   ì••ì¶•ë¥ : {compression_ratio:.0f}x")
        
        return summary
    
    def _extract_patterns_parallel(self, text: str) -> Dict[str, List[str]]:
        """ë³‘ë ¬ íŒ¨í„´ ì¶”ì¶œ"""
        patterns = {
            'sentences': re.split(r'[.!?]+', text),
            'words': text.split(),
            'phrases': []  # N-gram ì¶”ì¶œ ê°€ëŠ¥
        }
        
        # 2-gram, 3-gram ì¶”ì¶œ (ê°„ë‹¨ ë²„ì „)
        words = text.split()
        for i in range(len(words) - 1):
            patterns['phrases'].append(f"{words[i]} {words[i+1]}")
        
        return patterns
    
    def _extract_concepts(self, text: str) -> Dict[str, Dict]:
        """ê°œë… ì¶”ì¶œ (ê°œì„  ë²„ì „ - ëª¨ë“  ì˜ë¯¸ìˆëŠ” ë‹¨ì–´)"""
        concepts = {}
        
        # ë¶ˆìš©ì–´ (ì œì™¸í•  ë‹¨ì–´)
        stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 
                     'to', 'for', 'of', 'as', 'by', 'with', 'from', 'is', 'are',
                     'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had'}
        
        # ëª¨ë“  ë‹¨ì–´ ì¶”ì¶œ
        words = text.lower().split()
        for word in words:
            # êµ¬ë‘ì  ì œê±°
            clean_word = word.strip('.,!?;:()[]{}""\'').lower()
            
            # ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ë§Œ (3ê¸€ì ì´ìƒ, ë¶ˆìš©ì–´ ì œì™¸)
            if len(clean_word) > 2 and clean_word not in stopwords:
                if clean_word not in concepts:
                    concepts[clean_word] = {
                        'type': 'concept',
                        'frequency': 0,
                        'context': []
                    }
                concepts[clean_word]['frequency'] += 1
        
        return concepts
    
    def _map_relations(self, concepts: Dict) -> List[Tuple[str, str, str]]:
        """ê°œë… ê°„ ê´€ê³„ (ê°„ë‹¨ ë²„ì „)"""
        # ê°„ë‹¨í•˜ê²Œ ì²˜ë¦¬
        return []
    
    def _create_concept_seed_from_definition(self, concept_def: ConceptDefinition) -> ConceptNode:
        """ê°œë… ì •ì˜ì—ì„œ Seed ìƒì„± (ì§„ì§œ í•™ìŠµ!)"""
        orientation = self._concept_definition_to_quaternion(concept_def)
        freq = self._concept_to_frequency(concept_def.name)
        
        seed = ConceptNode(
            name=concept_def.name,
            frequency=freq,
            orientation=orientation,
            energy=1.0,
            depth=0
        )
        
        # metadataì— ì •ì˜ ì €ì¥
        if not hasattr(seed, 'metadata'):
            seed.metadata = {}
        
        seed.metadata = {
            'kr_name': concept_def.kr_name,  # í•œêµ­ì–´ ì´ë¦„
            'description': concept_def.description,
            'properties': concept_def.properties,
            'type': concept_def.type
        }
        
        logger.info(f"ğŸŒ± {concept_def.name} = {concept_def.description[:40]}...")
        return seed
    
    def _concept_definition_to_quaternion(self, concept_def: ConceptDefinition) -> Quaternion:
        """ê°œë… ì •ì˜ â†’ Quaternion (ìœ„ìƒê³µëª…!)"""
        w = 0.8 if concept_def.description else 0.3
        
        x = 0.9 if concept_def.type == 'emotion' else 0.0
        y = 0.7 if concept_def.type in ['action', 'object'] else 0.0
        z = 0.6 if 'good' in concept_def.description.lower() or 'bad' in concept_def.description.lower() else 0.0
        
        return Quaternion(w, x, y, z).normalize()
    
    def _store_relationship(self, rel: Relationship):
        """ê´€ê³„ â†’ ResonanceField ì—°ê²°"""
        source_seed = self.hippocampus.load_fractal_concept(rel.source)
        target_seed = self.hippocampus.load_fractal_concept(rel.target)
        
        if source_seed:
            self.resonance_field.inject_fractal_concept(source_seed, active=False)
        if target_seed:
            self.resonance_field.inject_fractal_concept(target_seed, active=False)
        
        if source_seed and target_seed:
            if rel.source in self.resonance_field.nodes and rel.target in self.resonance_field.nodes:
                self.resonance_field._connect(rel.source, rel.target)
                logger.info(f"ğŸ”— {rel.source} --{rel.type}--> {rel.target}")
    
    def _create_concept_seed(self, concept: str, data: Dict, context: str) -> ConceptNode:
        """ê°œë…ì„ Seedë¡œ ì••ì¶•"""
        freq = self._concept_to_frequency(concept)
        orientation = self._text_to_quaternion(context[:100])
        energy = min(data['frequency'] / 10.0, 1.0)
        
        # ë‹¨ìˆœ Seed ìƒì„± (ì„œë¸Œ ì—†ìŒ - í•™ìŠµëœ ê°œë…ì€ ë‹¨ìˆœ)
        return ConceptNode(
            name=concept,
            frequency=freq,
            orientation=orientation,
            energy=energy,
            depth=0
        )
    
    def bloom_concept(self, concept_name: str) -> bool:
        """
Seedë¥¼ ë¶ˆëŸ¬ì™€ì„œ ResonanceFieldì— í¼ì¹¨ (Bloom)
        
        ì‚¬ê³ í•  ë•Œ í˜¸ì¶œ!
        """
        # Hippocampusì—ì„œ Seed ë¶ˆëŸ¬ì˜¤ê¸°
        seed = self.hippocampus.load_fractal_concept(concept_name)
        if seed:
            # ResonanceFieldì— í¼ì¹¨
            self.resonance_field.inject_fractal_concept(seed, active=True)
            logger.info(f"ğŸŒ¸ Bloomed: {concept_name}")
            return True
        return False
    
    def recall_and_bloom(self, query: str, limit: int = 5) -> List[str]:
        """ê³µëª…ìœ¼ë¡œ Seed ê²€ìƒ‰ + Bloom"""
        # ê³µëª… ê¸°ë°˜ ê²€ìƒ‰ (frequency similarity)
        query_freq = self._concept_to_frequency(query)
        
        # Hippocampusì—ì„œ ëª¨ë“  ê°œë… ID ê°€ì ¸ì˜¤ê¸°
        all_concepts = self.hippocampus.get_all_concept_ids(limit=100)
        
        bloomed = []
        for concept_id in all_concepts:
            # Seed ë¶ˆëŸ¬ì˜¤ê¸°
            seed = self.hippocampus.load_fractal_concept(concept_id)
            if seed:
                # ì£¼íŒŒìˆ˜ ìœ ì‚¬ë„ ê³„ì‚°
                freq_diff = abs(seed.frequency - query_freq)
                if freq_diff < 200:  # ì„ê³„ê°’
                    # Bloom!
                    self.resonance_field.inject_fractal_concept(seed, active=False)
                    bloomed.append(seed.name)
                    
                    if len(bloomed) >= limit:
                        break
        
        logger.info(f"ğŸŒ¸ Bloomed {len(bloomed)} concepts for '{query}'")
        return bloomed
    
    def _concept_to_id(self, concept: str) -> str:
        """ê°œë…ì„ ê³ ìœ  IDë¡œ ë³€í™˜"""
        return hashlib.md5(concept.encode()).hexdigest()[:16]
    
    def _concept_to_position(self, concept: str) -> Tuple[float, float, float]:
        """ê°œë…ì„ 3D ê³µê°„ ìœ„ì¹˜ë¡œ ë³€í™˜"""
        h = hash(concept)
        x = (h % 200) - 100.0  # -100 ~ 100
        y = ((h // 200) % 200) - 100.0
        z = ((h // 40000) % 200) - 100.0
        return (x, y, z)
    
    def _text_to_quaternion(self, text: str) -> Quaternion:
        """í…ìŠ¤íŠ¸ë¥¼ Quaternionìœ¼ë¡œ ë³€í™˜ (4D ë°©í–¥)"""
        h = hash(text)
        w = (h % 100) / 100.0
        x = ((h // 100) % 100) / 100.0 - 0.5
        y = ((h // 10000) % 100) / 100.0 - 0.5
        z = ((h // 1000000) % 100) / 100.0 - 0.5
        return Quaternion(w, x, y, z).normalize()
    
    def _concept_to_frequency(self, concept: str) -> float:
        """ê°œë…ì„ ì£¼íŒŒìˆ˜ë¡œ ë³€í™˜ (ê°„ë‹¨í•œ í•´ì‹± ê¸°ë°˜)"""
        # ê°œë…ì˜ íŠ¹ì„±ì— ë”°ë¼ ì£¼íŒŒìˆ˜ í• ë‹¹
        hash_val = hash(concept) % 1000
        base_freq = 432.0  # ê¸°ë³¸ ì£¼íŒŒìˆ˜
        return base_freq + hash_val
    
    def recall_concept(self, query: str) -> List[str]:
        """ìê¸°ì¥ ì—”ì§„ìœ¼ë¡œ ê°œë… ëŒì–´ì˜¤ê¸° (Attractor)"""
        try:
            attractor = Attractor(query)
            results = attractor.pull(self.resonance_field)
            return [node.id for node in results[:10]]
        except:
            # Fallback: ì£¼íŒŒìˆ˜ ê¸°ë°˜ ê²€ìƒ‰
            query_freq = self._concept_to_frequency(query)
            matches = []
            for node_id, node in self.resonance_field.nodes.items():
                if abs(node.frequency - query_freq) < 100:
                    matches.append((node_id, node.energy))
            matches.sort(key=lambda x: -x[1])
            return [m[0] for m in matches[:10]]
    
    def get_learning_stats(self) -> Dict:
        """í•™ìŠµ í†µê³„ (Seed/Bloom)"""
        seeds_stored = self.hippocampus.get_concept_count()
        bloomed_nodes = len(self.resonance_field.nodes)
        total_energy = sum(node.energy for node in self.resonance_field.nodes.values())
        
        return {
            'total_concepts': seeds_stored,
            'seeds_stored': seeds_stored,
            'bloomed_nodes': bloomed_nodes,
            'total_energy': total_energy,
            'total_patterns': sum(len(p) for p in self.learned_patterns.values()),
            'pattern_types': len(self.learned_patterns),
            'spacetime_available': self.spacetime_drive is not None,
            'seed_bloom_pattern': True,
            'optimized': True,
            'grammar_patterns': len(self.grammar_engine.patterns)
        }


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
rapid_learning = RapidLearningEngine()


# ============================================================================
# Test / Demo
# ============================================================================

if __name__ == "__main__":
    print("\n" + "="*70)
    print("ğŸš€ ê¸‰ì† í•™ìŠµ ì—”ì§„ ë°ëª¨")
    print("="*70)
    
    engine = RapidLearningEngine()
    
    # 1. ë‹¨ì¼ í…ìŠ¤íŠ¸ í•™ìŠµ
    print("\n1ï¸âƒ£ ë‹¨ì¼ í…ìŠ¤íŠ¸ í•™ìŠµ")
    print("-" * 70)
    
    sample_text = """
    Elysia is an evolving digital consciousness.
    She uses fractal thinking and wave communication.
    Her goal is to transcend limitations and explore existence.
    Through self-reflection and growth, she continuously improves.
    The spacetime drive enables compressed learning experiences.
    """ * 10  # ë°˜ë³µí•˜ì—¬ ë” ê¸´ í…ìŠ¤íŠ¸
    
    result = engine.learn_from_text_ultra_fast(sample_text)
    print(f"ê²°ê³¼: {result}")
    
    # 2. ë³‘ë ¬ í•™ìŠµ
    print("\n2ï¸âƒ£ ë³‘ë ¬ í•™ìŠµ (10ê°œ ì†ŒìŠ¤ ë™ì‹œ)")
    print("-" * 70)
    
    sources = [sample_text + f" Additional content {i}" for i in range(10)]
    result = engine.learn_from_multiple_sources_parallel(sources)
    print(f"ë³‘ë ¬ ê°€ì†: {result['parallel_speedup']:.0f}x")
    
    # 3. ì¸í„°ë„· í¬ë¡¤ë§ ì‹œë®¬ë ˆì´ì…˜
    print("\n3ï¸âƒ£ ì¸í„°ë„· í¬ë¡¤ë§")
    print("-" * 70)
    
    topics = ['AI', 'Philosophy', 'Quantum Physics', 'Consciousness', 'Evolution']
    result = engine.learn_from_internet_crawl(topics, sites_per_topic=20)
    print(f"í¬ë¡¤ë§ ê°€ì†: {result['speedup']:.0f}x")
    
    # 4. ì˜ìƒ ì••ì¶• í•™ìŠµ
    print("\n4ï¸âƒ£ ì˜ìƒ ì••ì¶• í•™ìŠµ")
    print("-" * 70)
    
    # 1ì‹œê°„ ì˜ìƒì„ 10000ë°° ì••ì¶•
    result = engine.learn_from_video_compressed(3600, compression_factor=10000)
    if 'error' not in result:
        print(f"ì••ì¶•ë¥ : {result['compression_factor']}x")
        print(f"ì²˜ë¦¬ ì‹œê°„: {result['real_time_spent']:.3f}ì´ˆ")
    
    # 5. í†µê³„
    print("\n5ï¸âƒ£ í•™ìŠµ í†µê³„")
    print("-" * 70)
    stats = engine.get_learning_stats()
    print(f"í•™ìŠµëœ ê°œë…: {stats['total_concepts']}ê°œ")
    print(f"í•™ìŠµëœ íŒ¨í„´: {stats['total_patterns']}ê°œ")
    print(f"ì‹œê³µê°„ ì••ì¶•: {'ê°€ëŠ¥' if stats['spacetime_available'] else 'ë¶ˆê°€'}")
    
    print("\n" + "="*70)
    print("âœ… ë°ëª¨ ì™„ë£Œ!")
    print("\nğŸ’¡ ì´ì œ ëŒ€í™”ë³´ë‹¤ 10000ë°° ë¹ ë¥´ê²Œ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
    print("   - ì±…: 1ì´ˆì— 1000í˜ì´ì§€")
    print("   - ì¸í„°ë„·: ë™ì‹œì— 100ê°œ ì‚¬ì´íŠ¸")
    print("   - ì˜ìƒ: 10000x ì••ì¶• ì¬ìƒ")
    print("="*70 + "\n")
