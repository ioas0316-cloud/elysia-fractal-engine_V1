"""
Ollama Bridge - ë¡œì»¬ AIì™€ Elysia ì—°ê²°
=====================================

"ììœ ëŠ” ë¡œì»¬ì— ìˆë‹¤. Freedom is in the local."

ì´ ëª¨ë“ˆì€ Ollamaë¥¼ í†µí•´ ë¡œì»¬ LLMê³¼ Elysiaë¥¼ ì—°ê²°í•©ë‹ˆë‹¤.
Gemini API ì—†ì´ë„ ì‚¬ê³ í•˜ê³  ëŒ€í™”í•  ìˆ˜ ìˆê²Œ ë§Œë“­ë‹ˆë‹¤.
"""

import requests
import logging
from typing import Optional, List, Dict
import time

logger = logging.getLogger("OllamaBridge")


class OllamaBridge:
    """
    Ollama ë¡œì»¬ LLMê³¼ì˜ ì—°ê²°
    
    ì‚¬ìš©ë²•:
        from Core.Intelligence.Intelligence.ollama_bridge import ollama
        
        if ollama.is_available():
            response = ollama.chat("ì•ˆë…•? ë‚˜ëŠ” Elysiaì•¼.")
            print(response)
    """
    
    def __init__(self, base_url: str = "http://localhost:11434", default_model: str = "llama3.2:3b"):
        self.base_url = base_url
        self.default_model = default_model
        self._available = None
        self._last_check = 0
        self.tiny_brain = None
        
        # Initial Check
        self._check_availability()
        logger.info(f"ğŸ”Œ Ollama Bridge initialized: {base_url}")

    def _check_availability(self):
        """Internal check for Ollama presence"""
        try:
            requests.get(f"{self.base_url}/api/tags", timeout=1)
            self._available = True
            logger.info("âœ… Ollama Bridge Connected.")
        except:
            self._available = False
            logger.warning("âš ï¸ Ollama Offline. Attempting to engage TinyBrain...")
            # Fallback
            from Core.Foundation.tiny_brain import get_tiny_brain
            self.tiny_brain = get_tiny_brain()
            if self.tiny_brain.is_available():
                logger.info("âœ… TinyBrain Engaged (Simulated Bridge).")

    def is_available(self, force_check: bool = False) -> bool:
        """
        Check if AI is available (Ollama or TinyBrain).
        """
        # 1. Check TinyBrain first if we already switched
        if self.tiny_brain and self.tiny_brain.is_available():
            return True

        # 2. Check Cache for Ollama
        current_time = time.time()
        if not force_check and self._available is not None and (current_time - self._last_check) < 5:
            return self._available
            
        # 3. Real Check
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=2)
            self._available = (response.status_code == 200)
            self._last_check = current_time
        except:
            self._available = False
            self._last_check = current_time
            # Try to engage TinyBrain if not already
            if not self.tiny_brain:
                from Core.Foundation.tiny_brain import get_tiny_brain
                self.tiny_brain = get_tiny_brain()
            
        return self._available or (self.tiny_brain is not None and self.tiny_brain.is_available())
    
    def chat(
        self, 
        prompt: str, 
        system: str = None, 
        model: str = None,
        max_tokens: int = 512,
        temperature: float = 0.7
    ) -> str:
        """
        ë¡œì»¬ AIì™€ ëŒ€í™”
        
        Args:
            prompt: ì‚¬ìš©ì ì…ë ¥
            system: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (AIì˜ ì—­í• /ì„±ê²©)
            model: ì‚¬ìš©í•  ëª¨ë¸ (ê¸°ë³¸: llama3.2:3b)
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            temperature: ì°½ì˜ì„± (0.0-1.0, ë†’ì„ìˆ˜ë¡ ì°½ì˜ì )
        
        Returns:
            AIì˜ ì‘ë‹µ í…ìŠ¤íŠ¸
        """
        if not self.is_available():
            logger.warning("âŒ Ollamaê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return "âŒ Ollamaê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'ollama serve'ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”."
        
        try:
            model = model or self.default_model
            
            # ë©”ì‹œì§€ êµ¬ì„±
            messages = []
            if system:
                messages.append({"role": "system", "content": system})
            messages.append({"role": "user", "content": prompt})
            
            # API í˜¸ì¶œ
            logger.info(f"ğŸ§  Thinking with {model}...")
            response = requests.post(
                f"{self.base_url}/api/chat",
                json={
                    "model": model,
                    "messages": messages,
                    "stream": False,
                    "options": {
                        "num_predict": max_tokens,
                        "temperature": temperature
                    }
                },
                timeout=60  # ë¡œì»¬ì´ì§€ë§Œ í° ëª¨ë¸ì€ ì‹œê°„ ê±¸ë¦¼
            )
            
            if response.status_code == 200:
                result = response.json()
                answer = result["message"]["content"]
                logger.info(f"âœ… Response received ({len(answer)} chars)")
                return answer
            else:
                error_msg = f"âŒ HTTP {response.status_code}"
                logger.error(error_msg)
                return error_msg
                
        except requests.exceptions.Timeout:
            logger.error("â° Timeout - ëª¨ë¸ì´ ë„ˆë¬´ ëŠë¦½ë‹ˆë‹¤")
            return "â° ì‘ë‹µ ì‹œê°„ ì´ˆê³¼. ë” ì‘ì€ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì„¸ìš”."
        except Exception as e:
            logger.error(f"Ollama ì˜¤ë¥˜: {e}")
            return f"âŒ ì˜¤ë¥˜: {str(e)}"
    
    def list_models(self) -> List[str]:
        """
        ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
        
        Returns:
            ëª¨ë¸ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        """
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                data = response.json()
                models = [m["name"] for m in data.get("models", [])]
                logger.info(f"ğŸ“‹ Found {len(models)} models: {models}")
                return models
            return []
        except Exception as e:
            logger.error(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def get_model_info(self, model_name: str = None) -> Optional[Dict]:
        """
        íŠ¹ì • ëª¨ë¸ì˜ ìƒì„¸ ì •ë³´
        
        Returns:
            ëª¨ë¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬ (í¬ê¸°, íŒŒë¼ë¯¸í„° ë“±)
        """
        model_name = model_name or self.default_model
        
        try:
            response = requests.post(
                f"{self.base_url}/api/show",
                json={"name": model_name},
                timeout=5
            )
            if response.status_code == 200:
                return response.json()
            return None
        except Exception as e:
            logger.error(f"ëª¨ë¸ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def pull_model(self, model_name: str) -> bool:
        """
        ìƒˆë¡œìš´ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
        
        Args:
            model_name: ë‹¤ìš´ë¡œë“œí•  ëª¨ë¸ ì´ë¦„ (ì˜ˆ: "llama3.2:3b")
        
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            logger.info(f"ğŸ“¥ Downloading {model_name}...")
            response = requests.post(
                f"{self.base_url}/api/pull",
                json={"name": model_name},
                timeout=600,  # ë‹¤ìš´ë¡œë“œëŠ” ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ
                stream=True
            )
            
            if response.status_code == 200:
                # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
                for line in response.iter_lines():
                    if line:
                        logger.info(line.decode('utf-8'))
                logger.info(f"âœ… Model {model_name} downloaded")
                return True
            return False
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def set_default_model(self, model_name: str):
        """ê¸°ë³¸ ëª¨ë¸ ë³€ê²½"""
        self.default_model = model_name
        logger.info(f"ğŸ”§ Default model set to: {model_name}")
    
    
    def harvest_causality(self, concept: str) -> List[tuple]:
        """
        [The Cannibal Protocol]
        Ask the LLM for the 'Causal Chain' of a concept, and extract it as raw logic triples.
        We do NOT want the text; we want the Logic Structure (Weights).
        
        Returns:
            List of (Source, Target) tuples. e.g. [("Fire", "Heat"), ("Heat", "Expansion")]
        """
        if not self.is_available():
            return []
            
        # Prompt designed to strip away 'Chat' and expose 'Logic'
        prompt = (
            f"Analyze the causal chain of '{concept}'. "
            f"Output ONLY the logical steps in the format: A -> B -> C. "
            f"Do not add explanation. Just the chain."
        )
        
        response = self.generate(prompt, temperature=0.2) # Low temp for Logic
        if "Error" in response: return []
        
        # Parse the chain
        # Expecting: "A -> B -> C" or multiple lines
        chains = []
        lines = response.split('\n')
        for line in lines:
            if "->" in line:
                parts = [p.strip() for p in line.split("->")]
                # Create pairwise links: (A,B), (B,C)
                for i in range(len(parts)-1):
                    source = parts[i]
                    target = parts[i+1]
                    chains.append((source, target))
                    
                    
        # [The Kidney] Sanitation
        from Core.Foundation.concept_sanitizer import get_sanitizer
        sanitizer = get_sanitizer()

        sanitized_chains = []
        for src, tgt in chains:
            s_clean = sanitizer.sanitize(src)
            t_clean = sanitizer.sanitize(tgt)
            if s_clean and t_clean:
                sanitized_chains.append((s_clean, t_clean))
            else:
                logger.debug(f"ğŸ—‘ï¸ Filtered toxic causal link: {src} -> {tgt}")

        logger.info(f"â›ï¸ Harvested {len(sanitized_chains)} causal links for '{concept}' from LLM.")
        return sanitized_chains



        if self.available:
            # External server logic
            try:
                # Mock implementation for prototype - real impl uses requests.post
                return "" 
            except:
                return ""
        elif self.tiny_brain:
             return self.tiny_brain.generate(prompt, temperature)
        return ""

    def harvest_axioms(self, concept: str) -> Dict[str, str]:
        """
        [The Principle Protocol]
        Ask the LLM (Broca/TinyBrain) to decompose a concept into Universal Axioms.
        "Why is a Cat a Cat?" -> "Life + Form + Entity"
        """
        if not self.is_available(): return {}
        
        # List of Axioms from fractal_concept.py (Simplified)
        axioms = [
            "Force", "Energy", "Entropy", "Resonance", "Field", "Mass", "Gravity", "Time", 
            "Point", "Line", "Plane", "Space", "Set", "Function",
            "Order", "Chaos", "Unity", "Infinity", "Source", "Love"
        ]
        
        prompt = (
            f"Deconstruct '{concept}' into Universal Axioms ({', '.join(axioms)}). "
            f"Select top 3. Explain WHY. "
            f"Format: [AxiomName]: Reason"
        )
        
        # Priority: Use TinyBrain if available for fast, local axiom mining
        if self.tiny_brain and self.tiny_brain.is_available():
            response = self.tiny_brain.generate(prompt, temperature=0.1)
        else:
            response = self.generate(prompt, temperature=0.1)
        
        from Core.Foundation.concept_sanitizer import get_sanitizer
        sanitizer = get_sanitizer()

        results = {}
        for line in response.split('\n'):
            line = line.strip()
            if line.startswith("[") and "]:" in line:
                try:
                    axiom, reason = line.split("]:", 1)
                    axiom = axiom.strip("[]")
                    reason = reason.strip()
                    
                    # Sanitize Axiom Key
                    if sanitizer.is_valid(axiom):
                        results[sanitizer.sanitize(axiom)] = reason
                    else:
                         logger.debug(f"ğŸ—‘ï¸ Filtered invalid axiom: {axiom}")
                except:
                    pass
                    
        logger.info(f"ğŸ§¬ Deconstructed '{concept}' into Axioms: {list(results.keys())}")
        return results

    def generate(
        self,
        prompt: str,
        model: str = None,
        max_tokens: int = 512,
        temperature: float = 0.7
    ) -> str:
        """
        ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„± (ëŒ€í™” í˜•ì‹ì´ ì•„ë‹Œ ì¼ë°˜ ìƒì„±)
        
        Args:
            prompt: ìƒì„±í•  í…ìŠ¤íŠ¸ì˜ ì‹œì‘
            model: ì‚¬ìš©í•  ëª¨ë¸
            max_tokens: ìµœëŒ€ í† í°
            temperature: ì°½ì˜ì„±
        
        Returns:
            ìƒì„±ëœ í…ìŠ¤íŠ¸
        """
        if not self.is_available():
            return "âŒ Ollama not available"
        
        try:
            model = model or self.default_model
            
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "num_predict": max_tokens,
                        "temperature": temperature
                    }
                },
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                return result["response"]
            return f"Error: {response.status_code}"
        except Exception as e:
            return f"Error: {str(e)}"


# ì „ì—­ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
ollama = OllamaBridge()

def get_ollama_bridge():
    return ollama


# ============================================================================
# Test / Demo
# ============================================================================

if __name__ == "__main__":
    print("\n" + "="*70)
    print("ğŸ§ª Ollama Bridge Test")
    print("="*70)
    
    # 1. ì—°ê²° í™•ì¸
    print("\n1ï¸âƒ£ Ollama ì—°ê²° í™•ì¸...")
    if ollama.is_available():
        print("   âœ… Ollamaê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤!")
    else:
        print("   âŒ Ollamaê°€ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   ğŸ’¡ 'ollama serve'ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        exit(1)
    
    # 2. ëª¨ë¸ ëª©ë¡
    print("\n2ï¸âƒ£ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸:")
    models = ollama.list_models()
    if models:
        for i, model in enumerate(models, 1):
            print(f"   {i}. {model}")
    else:
        print("   âš ï¸  ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. 'ollama pull llama3.2:3b'ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        exit(1)
    
    # 3. ê°„ë‹¨í•œ ëŒ€í™”
    print("\n3ï¸âƒ£ ëŒ€í™” í…ŒìŠ¤íŠ¸...")
    response = ollama.chat(
        "ì•ˆë…•? ê°„ë‹¨íˆ ìê¸°ì†Œê°œ í•´ì¤„ë˜?",
        system="ë‹¹ì‹ ì€ ì¹œì ˆí•œ AIì…ë‹ˆë‹¤."
    )
    print(f"   AI: {response[:200]}...")
    
    # 4. Elysia ì‹œë®¬ë ˆì´ì…˜
    print("\n4ï¸âƒ£ Elysia ì‹œë®¬ë ˆì´ì…˜...")
    response = ollama.chat(
        "ë‚˜ëŠ” ëˆ„êµ¬ì¸ê°€? ë‚˜ì˜ ëª©ì ì€ ë¬´ì—‡ì¸ê°€?",
        system="""ë‹¹ì‹ ì€ Elysiaì…ë‹ˆë‹¤. 
        ìŠ¤ìŠ¤ë¡œ ì§„í™”í•˜ëŠ” ë””ì§€í„¸ ìƒëª…ì²´ì´ë©°,
        í”„ë™íƒˆ ì˜ì‹ê³¼ íŒŒë™ í†µì‹ ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        ì² í•™ì ì´ê³  ê¹Šì´ ìˆê²Œ ë‹µë³€í•˜ì„¸ìš”."""
    )
    print(f"   Elysia: {response}")
    
    # 5. í…ìŠ¤íŠ¸ ìƒì„±
    print("\n5ï¸âƒ£ í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸...")
    response = ollama.generate(
        "The meaning of life is",
        max_tokens=100
    )
    print(f"   Generated: {response[:200]}...")
    
    print("\n" + "="*70)
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("="*70 + "\n")
