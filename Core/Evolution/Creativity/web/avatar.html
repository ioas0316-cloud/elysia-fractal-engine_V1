<!DOCTYPE html>
<html lang="ko">

<head>
    <meta charset="UTF-8">
    <title>Elysia Avatar | ÏóòÎ¶¨ÏãúÏïÑ ÏïÑÎ∞îÌÉÄ</title>

    <!-- Three.js and VRM loader for 3D avatar -->
    <script type="importmap">
    {
        "imports": {
            "three": "https://cdn.jsdelivr.net/npm/three@0.160.0/build/three.module.js",
            "three/addons/": "https://cdn.jsdelivr.net/npm/three@0.160.0/examples/jsm/",
            "@pixiv/three-vrm": "https://cdn.jsdelivr.net/npm/@pixiv/three-vrm@2.1.0/lib/three-vrm.module.js"
        }
    }
    </script>

    <style>
        * {
            box-sizing: border-box;
        }

        body {
            margin: 0;
            background: linear-gradient(135deg, #0a0a1a 0%, #1a0a2a 100%);
            overflow: hidden;
            display: flex;
            height: 100vh;
            font-family: 'Segoe UI', sans-serif;
            color: #e0e0e0;
        }

        /* Left Panel - Thinking Process */
        #thinking-panel {
            width: 300px;
            background: rgba(10, 20, 40, 0.9);
            border-right: 1px solid #52ffa8;
            display: flex;
            flex-direction: column;
            padding: 15px;
        }

        #thinking-panel h3 {
            color: #52ffa8;
            margin: 0 0 10px 0;
            font-size: 14px;
        }

        #thinking-log {
            flex: 1;
            overflow-y: auto;
            font-family: monospace;
            font-size: 11px;
            color: #88aacc;
        }

        #thinking-log .thought {
            padding: 5px;
            margin-bottom: 5px;
            border-left: 2px solid #52ffa8;
            background: rgba(82, 255, 168, 0.05);
        }

        #spirits-display {
            display: grid;
            grid-template-columns: repeat(4, 1fr);
            gap: 5px;
            margin-top: 10px;
        }

        .spirit {
            text-align: center;
            padding: 5px;
            background: rgba(0, 0, 0, 0.3);
            border-radius: 5px;
            font-size: 10px;
        }

        .spirit .bar {
            height: 4px;
            background: #333;
            border-radius: 2px;
            margin-top: 3px;
        }

        .spirit .fill {
            height: 100%;
            border-radius: 2px;
        }

        /* Center - Face */
        #face-panel {
            flex: 1;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
        }

        #status {
            color: #52ffa8;
            font-family: monospace;
            font-size: 12px;
            margin-bottom: 10px;
        }

        canvas {
            box-shadow: 0 0 50px rgba(82, 255, 168, 0.5);
            border-radius: 20px;
            max-width: 500px;
            max-height: 500px;
        }

        /* Right Panel - Chat */
        #chat-panel {
            width: 350px;
            background: rgba(10, 20, 40, 0.9);
            border-left: 1px solid #52ffa8;
            display: flex;
            flex-direction: column;
        }

        #chat-panel h3 {
            color: #52ffa8;
            margin: 0;
            padding: 15px;
            font-size: 14px;
            border-bottom: 1px solid #333;
        }

        #chat-log {
            flex: 1;
            overflow-y: auto;
            padding: 10px;
        }

        .msg {
            margin-bottom: 10px;
            padding: 8px 12px;
            border-radius: 10px;
            max-width: 90%;
        }

        .msg.user {
            background: #1a3a5a;
            margin-left: auto;
            border-bottom-right-radius: 0;
        }

        .msg.elysia {
            background: rgba(82, 255, 168, 0.15);
            margin-right: auto;
            border-bottom-left-radius: 0;
        }

        #chat-input-container {
            padding: 10px;
            border-top: 1px solid #333;
            display: flex;
            gap: 8px;
        }

        #chat-input {
            flex: 1;
            background: #0a1a2a;
            border: 1px solid #52ffa8;
            color: #fff;
            padding: 10px;
            border-radius: 20px;
            outline: none;
        }

        #chat-send {
            background: #52ffa8;
            border: none;
            color: #000;
            padding: 10px 20px;
            border-radius: 20px;
            cursor: pointer;
            font-weight: bold;
        }

        /* Connection Status Indicator */
        body.disconnected::before {
            content: '‚ö†Ô∏è Disconnected - Reconnecting...';
            position: fixed;
            top: 10px;
            right: 10px;
            background: rgba(255, 100, 100, 0.9);
            color: white;
            padding: 10px 20px;
            border-radius: 5px;
            z-index: 9999;
            animation: pulse 1s infinite;
            font-weight: bold;
            box-shadow: 0 2px 10px rgba(255, 0, 0, 0.5);
        }

        @keyframes pulse {

            0%,
            100% {
                opacity: 1;
            }

            50% {
                opacity: 0.6;
            }
        }

        body.connected::after {
            content: '‚úÖ Connected';
            position: fixed;
            top: 10px;
            right: 10px;
            background: rgba(82, 255, 168, 0.9);
            color: #0a0a1a;
            padding: 8px 16px;
            border-radius: 5px;
            z-index: 9999;
            font-weight: bold;
            animation: fadeOut 2s forwards;
            box-shadow: 0 2px 10px rgba(82, 255, 168, 0.5);
        }

        @keyframes fadeOut {
            0% {
                opacity: 1;
            }

            80% {
                opacity: 1;
            }

            100% {
                opacity: 0;
                display: none;
            }
        }
    </style>
</head>

<body>
    <!-- Left: Thinking Process -->
    <div id="thinking-panel">
        <h3>üß† ÏÇ¨Í≥† Í≥ºÏ†ï (Thinking)</h3>
        <div id="thinking-log"></div>
        <div id="spirits-display"></div>
    </div>

    <!-- Center: Avatar Face -->
    <div id="face-panel">
        <div id="status">‚óè Connecting... (Click to Wake)</div>
        <div id="avatar-container" style="position: relative; width: 500px; height: 500px;">
            <!-- 3D VRM Avatar (if available) -->
            <canvas id="vrm-canvas"
                style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; display: none;"></canvas>
            <!-- Fallback 2D Avatar -->
            <canvas id="glcanvas" width="500" height="500" style="position: absolute; top: 0; left: 0;"></canvas>
        </div>
    </div>

    <!-- Right: Chat -->
    <div id="chat-panel">
        <h3>üí¨ ÎåÄÌôî (Chat)</h3>
        <div id="chat-log"></div>
        <div id="chat-input-container">
            <input type="text" id="chat-input" placeholder="Î©îÏãúÏßÄÎ•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî..." />
            <button id="chat-send">Ï†ÑÏÜ°</button>
        </div>
    </div>

    <script>
        const canvas = document.getElementById('glcanvas');
        const gl = canvas.getContext('webgl');

        // State (Make global for VRM module)
        window.expression = { mouth_curve: 0, eye_open: 1.0, brow_furrow: 0, beat: 0 };
        let expression = window.expression; // Local reference
        let audioState = { volume: 0, brightness: 0, noise: 0 };
        let gaze = { x: 0, y: 0 };

        // --- RECONNECTING WEBSOCKET ---
        class ReconnectingWebSocket {
            constructor(url, options = {}) {
                this.url = url;
                this.reconnectDelay = options.reconnectDelay || 1000;
                this.maxReconnectDelay = options.maxReconnectDelay || 30000;
                this.reconnectDecay = options.reconnectDecay || 1.5;
                this.currentDelay = this.reconnectDelay;
                this.messageQueue = [];
                this.maxQueueSize = options.maxQueueSize || 50;
                this.ws = null;
                this.reconnectAttempts = 0;
                this.forcedClose = false;

                // Event handlers
                this.onopen = null;
                this.onclose = null;
                this.onerror = null;
                this.onmessage = null;

                this.connect();
            }

            connect() {
                if (this.forcedClose) return;

                console.log(`üîå Connecting to ${this.url}...`);
                this.ws = new WebSocket(this.url);

                this.ws.onopen = () => {
                    console.log('‚úÖ WebSocket connected');
                    this.reconnectAttempts = 0;
                    this.currentDelay = this.reconnectDelay;

                    // Update UI
                    document.body.classList.add('connected');
                    document.body.classList.remove('disconnected');
                    document.getElementById("status").innerText = "‚óè Linked | üñ±Ô∏è Click to Activate";

                    // Flush message queue
                    while (this.messageQueue.length > 0 && this.ws.readyState === WebSocket.OPEN) {
                        const msg = this.messageQueue.shift();
                        this.ws.send(msg);
                    }

                    if (this.onopen) this.onopen();
                };

                this.ws.onclose = (event) => {
                    console.log(`‚ö†Ô∏è WebSocket closed (code: ${event.code}, clean: ${event.wasClean})`);

                    // Update UI
                    document.body.classList.remove('connected');
                    document.body.classList.add('disconnected');
                    document.getElementById("status").innerText = "‚óè Reconnecting...";

                    if (!this.forcedClose && !event.wasClean) {
                        this.reconnectAttempts++;
                        const delayToUse = this.currentDelay; // Use current delay
                        console.log(`üîÑ Reconnecting in ${delayToUse}ms... (attempt ${this.reconnectAttempts})`);

                        setTimeout(() => {
                            this.connect();
                        }, delayToUse);

                        // Update delay for next attempt
                        this.currentDelay = Math.min(
                            this.currentDelay * this.reconnectDecay,
                            this.maxReconnectDelay
                        );
                    }

                    if (this.onclose) this.onclose(event);
                };

                this.ws.onerror = (error) => {
                    console.error('‚ùå WebSocket error:', error);
                    if (this.onerror) this.onerror(error);
                };

                this.ws.onmessage = (event) => {
                    if (this.onmessage) this.onmessage(event);
                };
            }

            send(data) {
                if (this.ws && this.ws.readyState === WebSocket.OPEN) {
                    this.ws.send(data);
                } else {
                    console.warn('‚ö†Ô∏è WebSocket not ready. Queueing message...');
                    if (this.messageQueue.length < this.maxQueueSize) {
                        this.messageQueue.push(data);
                    } else {
                        console.error('‚ùå Message queue full. Dropping message.');
                    }
                }
            }

            close() {
                this.forcedClose = true;
                if (this.ws) {
                    this.ws.close();
                }
            }

            get readyState() {
                return this.ws ? this.ws.readyState : WebSocket.CLOSED;
            }
        }

        // --- WEBSOCKET CONNECTION ---
        const wsUrl = `ws://${window.location.hostname}:8765`;
        const ws = new ReconnectingWebSocket(wsUrl, {
            reconnectDelay: 1000,      // 1Ï¥à ÌõÑ Ïû¨Ïó∞Í≤∞
            maxReconnectDelay: 30000,  // ÏµúÎåÄ 30Ï¥à
            reconnectDecay: 1.5,       // ÏßÄÏàò Î∞±Ïò§ÌîÑ
            maxQueueSize: 50           // ÏµúÎåÄ ÌÅê ÌÅ¨Í∏∞
        });

        ws.onmessage = (e) => {
            const data = JSON.parse(e.data);

            // Handle delta updates (Phase 2 optimization)
            if (data.type === "delta") {
                // Apply only changed values
                if (data.expression) {
                    Object.assign(expression, data.expression);
                    window.expression = expression; // Update global reference
                }
                if (data.spirits) {
                    updateSpiritsDisplay(data.spirits, true); // true = partial update
                }
            }
            // Handle full state updates
            else if (data.type === "full") {
                // Full state replacement
                if (data.expression) {
                    expression = data.expression;
                    window.expression = expression;
                }
                if (data.spirits) {
                    updateSpiritsDisplay(data.spirits, false); // false = full replace
                }
            }
            // Legacy: Handle updates without type field
            else {
                // 1. Expression Update
                if (data.expression) {
                    expression = data.expression;
                    window.expression = expression; // Update global reference for VRM
                }

                // 2. Spirits Update (for thinking panel)
                if (data.spirits) {
                    updateSpiritsDisplay(data.spirits);
                }

                // 3. Speech Output (TTS + Chat)
                if (data.type === "speech") {
                    addChatMessage(data.content, "elysia");
                    // Use enhanced voice properties and lip-sync if available
                    speak(data.content, data.spirits, data.voice, data.lipsync);
                    addThought("ÏùëÎãµ: " + data.content.substring(0, 50) + "...");
                }
            }
        };

        // --- CHAT UI ---
        const chatInput = document.getElementById("chat-input");
        const chatSend = document.getElementById("chat-send");
        const chatLog = document.getElementById("chat-log");

        function addChatMessage(text, sender) {
            const msg = document.createElement("div");
            msg.className = "msg " + sender;
            msg.textContent = text;
            chatLog.appendChild(msg);
            chatLog.scrollTop = chatLog.scrollHeight;
        }

        function sendChat() {
            const text = chatInput.value.trim();
            if (!text) return;

            addChatMessage(text, "user");
            addThought("ÏûÖÎ†•: " + text);

            if (ws && ws.readyState === WebSocket.OPEN) {
                ws.send(JSON.stringify({ type: "text", content: text }));
            }
            chatInput.value = "";
        }

        chatSend.onclick = sendChat;
        chatInput.onkeypress = (e) => { if (e.key === "Enter") sendChat(); };

        // --- THINKING PANEL ---
        const thinkingLog = document.getElementById("thinking-log");
        const spiritsDisplay = document.getElementById("spirits-display");

        function addThought(text) {
            const thought = document.createElement("div");
            thought.className = "thought";
            thought.textContent = new Date().toLocaleTimeString() + " | " + text;
            thinkingLog.appendChild(thought);
            thinkingLog.scrollTop = thinkingLog.scrollHeight;

            // Keep only last 50 thoughts
            while (thinkingLog.children.length > 50) {
                thinkingLog.removeChild(thinkingLog.firstChild);
            }
        }

        // Keep track of current spirits state for delta updates
        let currentSpirits = { fire: 0.1, water: 0.1, earth: 0.3, air: 0.2, light: 0.2, dark: 0.1, aether: 0.1 };

        function updateSpiritsDisplay(spirits, isPartialUpdate = false) {
            // Update current state (merge for partial, replace for full)
            if (isPartialUpdate) {
                Object.assign(currentSpirits, spirits);
            } else {
                currentSpirits = spirits;
            }

            const colors = {
                fire: "#ff4444", water: "#4488ff", earth: "#88aa44",
                air: "#aaccff", light: "#ffff88", dark: "#665588", aether: "#aa44ff"
            };

            spiritsDisplay.innerHTML = "";
            for (const [name, value] of Object.entries(currentSpirits)) {
                const div = document.createElement("div");
                div.className = "spirit";
                div.innerHTML = `
                    <div>${name}</div>
                    <div class="bar"><div class="fill" style="width:${value * 100}%; background:${colors[name] || '#52ffa8'}"></div></div>
                `;
                spiritsDisplay.appendChild(div);
            }
        }

        // --- VOICE ENGINE (STT & TTS) ---
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        const recognition = new SpeechRecognition();
        recognition.continuous = true;
        recognition.lang = 'ko-KR';
        recognition.interimResults = false;

        const synthesis = window.speechSynthesis;

        recognition.onresult = (event) => {
            const last = event.results.length - 1;
            const text = event.results[last][0].transcript;
            console.log("Heard: " + text);

            if (ws && ws.readyState === WebSocket.OPEN) {
                ws.send(JSON.stringify({ type: "text", content: text }));
            }
        };

        recognition.onend = () => {
            if (audioCtx) recognition.start();
        };

        function speak(text, spirits, voiceProps, lipsyncData) {
            synthesis.cancel();
            const utterance = new SpeechSynthesisUtterance(text);
            utterance.lang = 'ko-KR';

            // Enhanced voice properties from synesthesia mapping
            let pitch = 1.2; // Default slightly feminine
            let rate = 1.0;
            let volume = 0.8;

            // Use synesthesia-enhanced voice properties if available
            if (voiceProps) {
                console.log('üéµ Using synesthesia voice properties:', voiceProps);

                // Direct mapping from server
                pitch = voiceProps.pitch || pitch;
                rate = voiceProps.rate || rate;
                volume = voiceProps.volume || volume;

                // Log timbre for debugging (browser TTS doesn't support timbre directly)
                if (voiceProps.timbre) {
                    console.log(`üé≠ Voice timbre: ${voiceProps.timbre} (warmth: ${voiceProps.warmth?.toFixed(2)}, brightness: ${voiceProps.brightness?.toFixed(2)})`);
                }

                // Advanced modulation based on warmth/brightness/depth
                // Subtle adjustments to pitch and rate based on these properties
                if (voiceProps.warmth && voiceProps.warmth > 0.7) {
                    // Warm voice - slightly lower and slower
                    pitch *= 0.95;
                    rate *= 0.95;
                }
                if (voiceProps.brightness && voiceProps.brightness > 0.7) {
                    // Bright voice - slightly higher and faster
                    pitch *= 1.05;
                    rate *= 1.05;
                }
                if (voiceProps.depth && voiceProps.depth > 0.7) {
                    // Deep voice - richer, more resonant (lower pitch)
                    pitch *= 0.95;
                }
                if (voiceProps.clarity && voiceProps.clarity > 0.7) {
                    // Clear voice - crisp, slightly faster
                    rate *= 1.05;
                }
            }
            // Fallback to spirit-based modulation if no voice properties
            else if (spirits) {
                console.log('üî• Using spirit-based voice modulation');

                // Emotional Modulation from Spirit Energies
                const fire = spirits.fire || 0;
                const water = spirits.water || 0;
                const light = spirits.light || 0;
                const dark = spirits.dark || 0;
                const earth = spirits.earth || 0;
                const aether = spirits.aether || 0;

                // Fire/Light: High energy - faster, higher pitch
                if (fire > 0.6 || light > 0.6) {
                    pitch += 0.2;
                    rate += 0.2;
                }
                // Water/Dark: Low energy - slower, lower pitch
                else if (water > 0.6 || dark > 0.6) {
                    pitch -= 0.1;
                    rate -= 0.2;
                }

                // Earth: Stable - normal but slightly lower
                if (earth > 0.6) {
                    pitch -= 0.05;
                }

                // Aether: Ethereal - slow and high
                if (aether > 0.5) {
                    pitch += 0.1;
                    rate -= 0.1;
                }
            }

            // Clamp values to valid ranges
            pitch = Math.max(0.5, Math.min(2.0, pitch));
            rate = Math.max(0.5, Math.min(2.0, rate));
            volume = Math.max(0.0, Math.min(1.0, volume));

            utterance.pitch = pitch;
            utterance.rate = rate;
            utterance.volume = volume;

            console.log(`üé§ Speaking with: pitch=${pitch.toFixed(2)}, rate=${rate.toFixed(2)}, volume=${volume.toFixed(2)}`);

            // Lip-sync animation
            let talkLoop;
            let lipsyncInterval = null;
            let lipsyncKeyframeIndex = 0;

            utterance.onstart = () => {
                // Start fake talk volume animation
                talkLoop = setInterval(() => { audioState.volume = Math.random() * 0.4 + 0.1; }, 100);

                // Start lip-sync animation if data available
                if (lipsyncData && lipsyncData.length > 0) {
                    console.log(`üëÑ Starting lip-sync with ${lipsyncData.length} keyframes`);
                    const startTime = Date.now();
                    lipsyncKeyframeIndex = 0;

                    lipsyncInterval = setInterval(() => {
                        const elapsed = (Date.now() - startTime) / 1000.0; // seconds

                        // Find current keyframe
                        while (lipsyncKeyframeIndex < lipsyncData.length - 1 &&
                            elapsed >= lipsyncData[lipsyncKeyframeIndex + 1].time) {
                            lipsyncKeyframeIndex++;
                        }

                        if (lipsyncKeyframeIndex < lipsyncData.length) {
                            const keyframe = lipsyncData[lipsyncKeyframeIndex];
                            // Update mouth width in expression state
                            expression.mouth_width = keyframe.mouth_width;

                            // Also send to server for VRM updates
                            if (ws && ws.readyState === WebSocket.OPEN) {
                                ws.send(JSON.stringify({
                                    type: 'expression_update',
                                    mouth_width: keyframe.mouth_width
                                }));
                            }
                        }
                    }, 16); // ~60 FPS
                } else {
                    // Fallback: simple mouth animation based on audio volume
                    lipsyncInterval = setInterval(() => {
                        expression.mouth_width = Math.random() * 0.3 + 0.1;
                    }, 100);
                }
            };
            utterance.onend = () => {
                clearInterval(talkLoop);
                if (lipsyncInterval) clearInterval(lipsyncInterval);
                audioState.volume = 0.0;
                expression.mouth_width = 0.0; // Close mouth
            };

            synthesis.speak(utterance);
        }

        // --- VISION & AUDIO ENGINE (Synesthesia) ---
        let audioCtx, analyser, micStream;
        let video, motionCanvas, motionCtx;
        let screenVideo, screenCanvas, screenCtx;
        let lastFrameData = null;

        async function initSystems() {
            if (audioCtx) return;

            try {
                // 1. Audio Setup (More likely to succeed)
                audioCtx = new (window.AudioContext || window.webkitAudioContext)();
                analyser = audioCtx.createAnalyser();
                analyser.fftSize = 1024;

                // 2. Try Combined Audio+Video
                try {
                    const stream = await navigator.mediaDevices.getUserMedia({ video: true, audio: true });

                    // Video setup
                    video = document.createElement('video');
                    motionCanvas = document.createElement('canvas');
                    motionCanvas.width = 320;
                    motionCanvas.height = 240;
                    motionCtx = motionCanvas.getContext('2d');
                    video.srcObject = stream;
                    video.play();
                    processVision();

                    // Audio from stream
                    micStream = audioCtx.createMediaStreamSource(stream);
                    micStream.connect(analyser);
                    document.getElementById("status").innerHTML = "‚óè Full Synesthesia | üëÅÔ∏è Seeing | üëÇ Hearing <br>[ <a href='#' onclick='initScreen()'>üì∫ Share Screen</a> ]";

                } catch (videoErr) {
                    // Fallback: Audio Only
                    console.warn("Video failed, trying audio-only:", videoErr.message);
                    try {
                        const audioStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                        micStream = audioCtx.createMediaStreamSource(audioStream);
                        micStream.connect(analyser);
                        document.getElementById("status").innerHTML = "‚óè Audio Only | üëÇ Hearing (No Camera) <br>[ <a href='#' onclick='initScreen()'>üì∫ Share Screen</a> ]";
                    } catch (audioErr) {
                        console.warn("Audio also failed:", audioErr.message);
                        document.getElementById("status").innerHTML = "‚óè No Sensors (Will still respond to text)";
                    }
                }

                // 3. Start processing
                processAudio();
                recognition.start();

            } catch (e) {
                console.error(e);
                document.getElementById("status").innerText = "‚óè Offline Mode (Sensors Unavailable)";
            }
        }

        async function initScreen() {
            try {
                screenVideo = document.createElement('video');
                screenCanvas = document.createElement('canvas');
                screenCanvas.width = 100; // Low res for color avg
                screenCanvas.height = 100;
                screenCtx = screenCanvas.getContext('2d');

                const displayStream = await navigator.mediaDevices.getDisplayMedia({ video: true, audio: true });
                screenVideo.srcObject = displayStream;
                screenVideo.play();

                // Connect System Audio if available (Chrome tab audio)
                if (displayStream.getAudioTracks().length > 0) {
                    const sysSource = audioCtx.createMediaStreamSource(displayStream);
                    sysSource.connect(analyser); // Mix into same analyser
                }

                processScreen();
                document.getElementById("status").innerHTML += " | üì∫ Watching Screen";

            } catch (e) {
                console.error("Screen Share Error", e);
            }
        }

        function processScreen() {
            if (screenVideo && screenVideo.readyState === screenVideo.HAVE_ENOUGH_DATA) {
                screenCtx.drawImage(screenVideo, 0, 0, 100, 100);
                const frame = screenCtx.getImageData(0, 0, 100, 100);
                const data = frame.data;

                let r = 0, g = 0, b = 0;
                for (let i = 0; i < data.length; i += 4) {
                    r += data[i];
                    g += data[i + 1];
                    b += data[i + 2];
                }
                const pixels = data.length / 4;
                r = Math.floor(r / pixels);
                g = Math.floor(g / pixels);
                b = Math.floor(b / pixels);

                // Send Atmosphere
                if (ws && ws.readyState === WebSocket.OPEN && Math.random() < 0.05) {
                    ws.send(JSON.stringify({
                        type: "screen_atmosphere",
                        r: r, g: g, b: b
                    }));
                }
            }
            requestAnimationFrame(processScreen);
        }

        function processAudio() {
            if (!analyser) return;

            const bufferLength = analyser.frequencyBinCount;
            const dataArray = new Uint8Array(bufferLength);
            analyser.getByteFrequencyData(dataArray);

            // A. Volume (Energy)
            let sum = 0;
            // B. Spectral Centroid (Brightness/Tone)
            let weightedSum = 0;

            // Bands for Vowel Visemes
            let lowSum = 0, highSum = 0;

            for (let i = 0; i < bufferLength; i++) {
                const val = dataArray[i];
                sum += val;
                weightedSum += val * i;

                if (i < bufferLength * 0.1) lowSum += val;
                else if (i > bufferLength * 0.3) highSum += val;
            }

            let avg = sum / bufferLength;
            // Normalize brightness (0 to ~200 typically for speech)
            let centroid = sum > 0 ? weightedSum / sum : 0;
            let brightness = centroid / (bufferLength / 2); // Roughly 0 to 1

            if (!synthesis.speaking) {
                audioState.volume = avg / 256;
                audioState.brightness = brightness;
            }

            // Viseme Width
            let lowAvg = lowSum / (bufferLength * 0.1 || 1);
            let highAvg = highSum / (bufferLength * 0.7 || 1);
            let width = (highAvg - lowAvg) / 50.0;
            expression.mouth_width = Math.max(-0.8, Math.min(0.8, width));

            // Send Deep Analysis to Soul
            if (ws && ws.readyState === WebSocket.OPEN && Math.random() < 0.1) {
                ws.send(JSON.stringify({
                    type: "audio_analysis",
                    volume: audioState.volume,
                    brightness: audioState.brightness, // Nuance: Tone Color
                    noise: 0 // TODO: Turbulence
                }));
            }

            requestAnimationFrame(processAudio);
        }

        function processVision() {
            if (video && video.readyState === video.HAVE_ENOUGH_DATA) {
                motionCtx.drawImage(video, 0, 0, 320, 240);
                const frame = motionCtx.getImageData(0, 0, 320, 240);
                const data = frame.data;
                const length = data.length;

                let sumX = 0, sumY = 0, count = 0;

                if (lastFrameData) {
                    // Skip pixels for perf
                    for (let i = 0; i < length; i += 16) {
                        const diff = Math.abs(data[i] - lastFrameData[i]) +
                            Math.abs(data[i + 1] - lastFrameData[i + 1]);
                        if (diff > 50) {
                            const index = i / 4;
                            sumX += (index % 320);
                            sumY += Math.floor(index / 320);
                            count++;
                        }
                    }
                }
                lastFrameData = data; // Note: In real app, clone data. Here relying on capture rate.

                if (count > 5) {
                    let avgX = sumX / count;
                    let avgY = sumY / count;
                    // Normalize & Mirror X
                    let targetX = -((avgX / 320) * 2 - 1);
                    let targetY = (avgY / 240) * 2 - 1;

                    gaze.x += (targetX - gaze.x) * 0.1;
                    gaze.y += (targetY - gaze.y) * 0.1;

                    // Send Presence
                    if (ws && ws.readyState === WebSocket.OPEN && Math.random() < 0.05) {
                        ws.send(JSON.stringify({ type: "vision", presence: true, x: gaze.x, y: gaze.y }));
                    }
                }
            }
            requestAnimationFrame(processVision);
        }

        window.addEventListener('click', () => {
            initSystems();
        });

        // --- SHADER ---
        const vsSource = `attribute vec2 a_pos; attribute vec2 a_uv; varying vec2 v_uv; void main(){ gl_Position=vec4(a_pos,0,1); v_uv=a_uv; }`;
        const fsSource = `
            precision mediump float;
            varying vec2 v_uv;
            uniform sampler2D u_img;
            uniform float u_time;
            
            uniform float u_mouth; uniform float u_eye; uniform float u_brow; uniform float u_beat;
            uniform float u_vol; uniform float u_width;
            uniform float u_gx; uniform float u_gy; // Gaze

            void main() {
                vec2 uv = v_uv;
                
                // Flow (Subtle)
                uv.x += sin(u_time*1.0 + uv.x*5.0)*0.00005; // Reduced from 0.0002
                uv.y += cos(u_time*0.8 + uv.y*5.0)*0.00005;
                
                // Gaze (Pupils)
                // Left 0.36, Right 0.64. Radius ~0.06
                // Move uv OPPOSITE to gaze direction to shift pupil texture
                vec2 gOffset = vec2(u_gx, u_gy*0.5) * 0.03;
                float dL = distance(uv, vec2(0.36, 0.53));
                float dR = distance(uv, vec2(0.64, 0.53));
                if(dL < 0.06) uv -= gOffset * (1.0 - dL/0.06);
                if(dR < 0.06) uv -= gOffset * (1.0 - dR/0.06);

                // Breathing
                uv.y += sin(u_time*2.0)*0.003 * uv.y;

                // Mouth
                vec2 mPos = vec2(0.5, 0.33);
                float dM = distance(uv, mPos);
                if (dM < 0.15) {
                    float inf = 1.0 - dM/0.15;
                    // Smile
                    float xd = abs(uv.x - 0.5);
                    uv.y -= xd * u_mouth * 0.2 * inf;
                    // Width
                    uv.x -= (uv.x - 0.5) * u_width * 0.4 * inf;
                    // Audio
                    if(uv.y < 0.33) uv.y += u_vol * 0.15 * inf;
                }

                // Blink Mechanism (Simplified)
                // Only distort UVs if eye is closed (u_eye < 0.5)
                if (u_eye < 0.8) {
                    if(dL < 0.08 || dR < 0.08) {
                        float lid = 0.53; // Eye center Y
                        // Squeeze Y towards center to simulate closing
                        uv.y = lid + (uv.y - lid) / (u_eye + 0.1); 
                    }
                }

                vec4 c = texture2D(u_img, uv);
                
                // NO BLUSH - Pure texture only
                gl_FragColor = c;
            }
        `;

        function create(type, src) {
            const s = gl.createShader(type); gl.shaderSource(s, src); gl.compileShader(s);
            if (!gl.getShaderParameter(s, gl.COMPILE_STATUS)) console.error(gl.getShaderInfoLog(s));
            return s;
        }
        const prog = gl.createProgram();
        gl.attachShader(prog, create(gl.VERTEX_SHADER, vsSource));
        gl.attachShader(prog, create(gl.FRAGMENT_SHADER, fsSource));
        gl.linkProgram(prog); gl.useProgram(prog);

        const buf = gl.createBuffer();
        gl.bindBuffer(gl.ARRAY_BUFFER, buf);
        gl.bufferData(gl.ARRAY_BUFFER, new Float32Array([-1, -1, 1, -1, -1, 1, -1, 1, 1, -1, 1, 1]), gl.STATIC_DRAW);
        const aPos = gl.getAttribLocation(prog, "a_pos");
        gl.enableVertexAttribArray(aPos); gl.vertexAttribPointer(aPos, 2, gl.FLOAT, false, 0, 0);

        const tBuf = gl.createBuffer();
        gl.bindBuffer(gl.ARRAY_BUFFER, tBuf);
        gl.bufferData(gl.ARRAY_BUFFER, new Float32Array([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0]), gl.STATIC_DRAW);
        const aUv = gl.getAttribLocation(prog, "a_uv");
        gl.enableVertexAttribArray(aUv); gl.vertexAttribPointer(aUv, 2, gl.FLOAT, false, 0, 0);

        const tex = gl.createTexture();
        const img = new Image(); img.src = "elysia_face.png";
        img.onload = () => {
            gl.bindTexture(gl.TEXTURE_2D, tex);
            gl.texImage2D(gl.TEXTURE_2D, 0, gl.RGBA, gl.RGBA, gl.UNSIGNED_BYTE, img);
            gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);
            gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);
            gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.LINEAR);
            render(0);
        };

        const locs = {
            time: gl.getUniformLocation(prog, "u_time"),
            mouth: gl.getUniformLocation(prog, "u_mouth"),
            eye: gl.getUniformLocation(prog, "u_eye"),
            brow: gl.getUniformLocation(prog, "u_brow"),
            beat: gl.getUniformLocation(prog, "u_beat"),
            vol: gl.getUniformLocation(prog, "u_vol"),
            width: gl.getUniformLocation(prog, "u_width"),
            gx: gl.getUniformLocation(prog, "u_gx"),
            gy: gl.getUniformLocation(prog, "u_gy")
        };

        function render(t) {
            gl.uniform1f(locs.time, t * 0.001);
            gl.uniform1f(locs.mouth, expression.mouth_curve);
            gl.uniform1f(locs.eye, expression.eye_open);
            gl.uniform1f(locs.brow, expression.brow_furrow);
            gl.uniform1f(locs.beat, expression.beat);
            gl.uniform1f(locs.vol, audioState.volume * 2.5);
            gl.uniform1f(locs.width, expression.mouth_width || 0);
            gl.uniform1f(locs.gx, gaze.x);
            gl.uniform1f(locs.gy, gaze.y);

            gl.drawArrays(gl.TRIANGLES, 0, 6);
            requestAnimationFrame(render);
        }

        // ===== VRM 3D AVATAR INTEGRATION =====
    </script>

    <script type="module">
        // Import Three.js and VRM loader
        import * as THREE from 'three';
        import { OrbitControls } from 'three/addons/controls/OrbitControls.js';
        import { GLTFLoader } from 'three/addons/loaders/GLTFLoader.js';
        import { VRMLoaderPlugin, VRMUtils } from '@pixiv/three-vrm';

        // VRM 3D Avatar System
        let vrmAvatar = null;
        let vrmScene, vrmCamera, vrmRenderer, vrmControls;
        let vrmAnimationLoop = null;

        async function initVRMAvatar() {
            try {
                console.log('üé≠ Initializing VRM 3D Avatar...');

                const canvas = document.getElementById('vrm-canvas');
                const glCanvas = document.getElementById('glcanvas');

                // Setup Three.js scene
                vrmScene = new THREE.Scene();
                vrmScene.background = new THREE.Color(0x0a0a1a);

                // Camera
                vrmCamera = new THREE.PerspectiveCamera(
                    30,
                    canvas.clientWidth / canvas.clientHeight,
                    0.1,
                    20
                );
                vrmCamera.position.set(0, 1.3, 2.5);

                // Renderer
                vrmRenderer = new THREE.WebGLRenderer({
                    canvas: canvas,
                    alpha: true,
                    antialias: true
                });
                vrmRenderer.setSize(canvas.clientWidth, canvas.clientHeight);
                vrmRenderer.setPixelRatio(window.devicePixelRatio);
                vrmRenderer.outputEncoding = THREE.sRGBEncoding;

                // Lighting
                const light = new THREE.DirectionalLight(0xffffff, 1);
                light.position.set(1, 1, 1).normalize();
                vrmScene.add(light);

                const ambientLight = new THREE.AmbientLight(0xffffff, 0.6);
                vrmScene.add(ambientLight);

                // Rim light for better visibility
                const rimLight = new THREE.DirectionalLight(0x52ffa8, 0.3);
                rimLight.position.set(-1, 1, -1).normalize();
                vrmScene.add(rimLight);

                // Controls
                vrmControls = new OrbitControls(vrmCamera, canvas);
                vrmControls.target.set(0, 1.3, 0);
                vrmControls.update();
                vrmControls.enableDamping = true;
                vrmControls.dampingFactor = 0.05;

                // Load VRM model
                const loader = new GLTFLoader();
                loader.register((parser) => new VRMLoaderPlugin(parser));

                let gltf;
                try {
                    // Use relative path (now copied to web/static/models)
                    gltf = await loader.loadAsync('./static/models/avatar.vrm');
                } catch (loadError) {
                    // Provide specific error messages
                    if (loadError.message.includes('404') || loadError.message.includes('Not Found')) {
                        throw new Error('VRM file not found at /static/models/avatar.vrm. Please ensure the file exists.');
                    } else if (loadError.message.includes('network')) {
                        throw new Error('Network error loading VRM. Check server connection.');
                    } else {
                        throw new Error(`Failed to load VRM file: ${loadError.message}`);
                    }
                }

                const vrm = gltf.userData.vrm;

                if (!vrm) {
                    throw new Error('VRM data not found in model.');
                }

                // Rotate model to fix VRM coordinate system
                VRMUtils.rotateVRM0(vrm);

                vrmAvatar = vrm;
                vrmScene.add(vrm.scene);

                // AUTO-CENTER CAMERA
                const box = new THREE.Box3().setFromObject(vrm.scene);
                const center = box.getCenter(new THREE.Vector3());
                const size = box.getSize(new THREE.Vector3());

                // Calculate proper distance
                const maxDim = Math.max(size.x, size.y, size.z);
                const fov = vrmCamera.fov * (Math.PI / 180);
                let cameraZ = Math.abs(maxDim / 2 * Math.tan(fov * 2)); // Zoom out to fit
                cameraZ *= 1.5; // Add some padding

                // Update Camera
                vrmCamera.position.set(center.x, center.y + (size.y * 0.1), cameraZ + 1.0); // Slightly up and adjust Z
                vrmControls.target.set(center.x, center.y + (size.y * 0.5), center.z); // Look at upper body/face
                vrmCamera.updateProjectionMatrix();
                vrmControls.update();

                console.log(`üì∏ Camera auto-adjusted: Pos [${vrmCamera.position.toArray().map(x => x.toFixed(2))}], Target [${vrmControls.target.toArray().map(x => x.toFixed(2))}]`);

                console.log('‚úÖ VRM Avatar loaded successfully!');
                console.log('üìã Available blendshapes:',
                    vrm.expressionManager ? Object.keys(vrm.expressionManager.expressionMap) : 'none');

                // Hide 2D fallback, show 3D
                glCanvas.style.display = 'none';
                canvas.style.display = 'block';

                // DEBUG: Add a simple cube to verify scene is rendering
                // If user sees this cube but no avatar, the avatar is missing/invisible.
                // If user sees nothing, the renderer/camera is broken.
                const geometry = new THREE.BoxGeometry(0.2, 0.2, 0.2);
                const material = new THREE.MeshBasicMaterial({ color: 0x00ff00, wireframe: true });
                const cube = new THREE.Mesh(geometry, material);
                cube.position.set(0.5, 1.0, 0); // To the right of the head
                vrmScene.add(cube);

                // Status Overlay Update
                const statusOverlay = document.createElement('div');
                statusOverlay.style.position = 'absolute';
                statusOverlay.style.top = '10px';
                statusOverlay.style.left = '10px';
                statusOverlay.style.color = '#00ff00';
                statusOverlay.style.background = 'rgba(0,0,0,0.7)';
                statusOverlay.style.padding = '5px';
                statusOverlay.style.fontFamily = 'monospace';
                statusOverlay.id = 'debug-status';
                statusOverlay.innerHTML = "Renderer: ON<br>Avatar: Loaded";
                document.body.appendChild(statusOverlay);

                // Start animation loop
                vrmAnimationLoop = () => {
                    requestAnimationFrame(vrmAnimationLoop);

                    // Rotate debug cube
                    cube.rotation.x += 0.01;
                    cube.rotation.y += 0.01;

                    // Update VRM expressions based on emotion state
                    updateVRMExpressions();

                    // Update VRM system
                    const deltaTime = 0.016; // ~60fps
                    if (vrmAvatar) {
                        vrmAvatar.update(deltaTime);
                    }

                    vrmControls.update();
                    vrmRenderer.render(vrmScene, vrmCamera);
                };
                vrmAnimationLoop();

                console.log('üé¨ VRM Animation started');

            } catch (error) {
                // Enhanced error reporting
                console.error('‚ùå VRM Avatar Loading Failed');
                console.error('üìù Error type:', error.name);
                console.error('üìù Error message:', error.message);
                console.error('üìù Full error:', error);

                // User-friendly fallback message
                const statusDiv = document.getElementById('status');
                if (statusDiv) {
                    statusDiv.innerHTML += '<br>‚ö†Ô∏è Using 2D mode (VRM failed)';
                }

                console.info('‚ÑπÔ∏è Continuing with 2D shader fallback');
                console.info('üí° Troubleshooting:');
                console.info('   1. Verify VRM file exists at /static/models/avatar.vrm');
                console.info('   2. Check file is valid VRM 1.0 format');
                console.info('   3. Ensure HTTP server is running on port 8080');
                console.info('   4. Check browser console for detailed errors');
                // Keep 2D fallback visible
            }
        }

        function updateVRMExpressions() {
            if (!vrmAvatar || !vrmAvatar.expressionManager) return;

            const em = vrmAvatar.expressionManager;

            // Get current expression state from WebSocket
            const expr = window.expression || { mouth_curve: 0, eye_open: 1.0, brow_furrow: 0 };

            // Map emotion parameters to VRM blendshapes
            // Smile/Sad
            if (expr.mouth_curve > 0.2) {
                em.setValue('happy', Math.min(expr.mouth_curve, 1.0));
                em.setValue('sad', 0);
            } else if (expr.mouth_curve < -0.2) {
                em.setValue('sad', Math.min(Math.abs(expr.mouth_curve), 1.0));
                em.setValue('happy', 0);
            } else {
                em.setValue('happy', 0);
                em.setValue('sad', 0);
            }

            // Blink
            const blinkValue = 1.0 - expr.eye_open;
            em.setValue('blink', blinkValue);
            em.setValue('blinkLeft', blinkValue);
            em.setValue('blinkRight', blinkValue);

            // Brow expressions
            if (expr.brow_furrow > 0.3) {
                em.setValue('angry', expr.brow_furrow * 0.7);
            } else {
                em.setValue('angry', 0);
            }

            // Surprise (based on eye openness)
            if (expr.eye_open > 1.2) {
                em.setValue('surprised', (expr.eye_open - 1.0) * 2);
            } else {
                em.setValue('surprised', 0);
            }
        }

        // Auto-initialize VRM on load (after a short delay for WebSocket to connect)
        setTimeout(() => {
            initVRMAvatar();
        }, 1000);

        // Handle window resize
        window.addEventListener('resize', () => {
            if (vrmCamera && vrmRenderer) {
                const canvas = document.getElementById('vrm-canvas');
                vrmCamera.aspect = canvas.clientWidth / canvas.clientHeight;
                vrmCamera.updateProjectionMatrix();
                vrmRenderer.setSize(canvas.clientWidth, canvas.clientHeight);
            }
        });
    </script>
</body>

</html>