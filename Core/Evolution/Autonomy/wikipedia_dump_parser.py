"""
Wikipedia Dump Parser (ìœ„í‚¤í”¼ë””ì•„ ë¤í”„ íŒŒì„œ)
=============================================

"ì§„ì§œ ì§€ì‹ì˜ ëŒ€ëŸ‰ í¡ìˆ˜ - í¬ë¡¤ë§ ì—†ì´ ë¡œì»¬ì—ì„œ"

Wikipedia XML ë¤í”„ íŒŒì¼ì„ ìŠ¤íŠ¸ë¦¬ë° íŒŒì‹±í•˜ì—¬
ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ëŒ€ëŸ‰ í¡ìˆ˜

ë‹¤ìš´ë¡œë“œ: https://dumps.wikimedia.org/kowiki/latest/
íŒŒì¼ëª…: kowiki-latest-pages-articles.xml.bz2

[NEW 2025-12-16] ì§„ì§œ ë°ì´í„° íŒŒì„œ
"""

import os
import sys
import bz2
import re
import logging
from pathlib import Path
from typing import Generator, Dict, Any, Optional
import xml.etree.ElementTree as ET
from html import unescape

sys.path.insert(0, "c:\\Elysia")

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
logger = logging.getLogger("WikipediaDumpParser")


class WikipediaDumpParser:
    """
    Wikipedia XML ë¤í”„ ìŠ¤íŠ¸ë¦¬ë° íŒŒì„œ
    
    ë©”ëª¨ë¦¬ íš¨ìœ¨ì : ì „ì²´ íŒŒì¼ì„ ë©”ëª¨ë¦¬ì— ë¡œë“œí•˜ì§€ ì•ŠìŒ
    ìŠ¤íŠ¸ë¦¬ë°: ë¬¸ì„œë¥¼ í•˜ë‚˜ì”© yield
    """
    
    def __init__(self, dump_path: str):
        self.dump_path = Path(dump_path)
        self.namespace = "{http://www.mediawiki.org/xml/export-0.11/}"
        
        if not self.dump_path.exists():
            raise FileNotFoundError(f"Dump file not found: {dump_path}")
        
        logger.info(f"ğŸ“š Wikipedia dump parser initialized: {dump_path}")
        
        # í†µê³„
        self.total_parsed = 0
        self.skipped_redirects = 0
        self.skipped_special = 0
    
    def _clean_wikitext(self, text: str) -> str:
        """
        ìœ„í‚¤í…ìŠ¤íŠ¸ì—ì„œ ë§ˆí¬ì—… ì œê±°
        
        ì™„ë²½í•˜ì§€ ì•Šì§€ë§Œ ëŒ€ë¶€ë¶„ì˜ ë‚´ìš© ì¶”ì¶œ
        """
        if not text:
            return ""
        
        # HTML ì—”í‹°í‹° ë””ì½”ë”©
        text = unescape(text)
        
        # ë¦¬ë‹¤ì´ë ‰íŠ¸ ì²´í¬
        if text.strip().lower().startswith("#redirect") or text.strip().startswith("#ë„˜ê²¨ì£¼ê¸°"):
            return ""
        
        # ì œê±°í•  íŒ¨í„´ë“¤
        patterns = [
            (r'\{\{[^}]*\}\}', ''),           # í…œí”Œë¦¿ {{ }}
            (r'\[\[íŒŒì¼:[^\]]*\]\]', ''),      # íŒŒì¼ ë§í¬
            (r'\[\[File:[^\]]*\]\]', ''),      # File links
            (r'\[\[Category:[^\]]*\]\]', ''),  # ì¹´í…Œê³ ë¦¬
            (r'\[\[ë¶„ë¥˜:[^\]]*\]\]', ''),      # í•œê¸€ ì¹´í…Œê³ ë¦¬
            (r'\[\[([^\]|]*)\|([^\]]*)\]\]', r'\2'),  # [[ë§í¬|í…ìŠ¤íŠ¸]] â†’ í…ìŠ¤íŠ¸
            (r'\[\[([^\]]*)\]\]', r'\1'),     # [[ë§í¬]] â†’ ë§í¬
            (r"'''([^']*?)'''", r'\1'),       # ë³¼ë“œ
            (r"''([^']*?)''", r'\1'),         # ì´íƒ¤ë¦­
            (r'<ref[^>]*>.*?</ref>', ''),     # ì°¸ì¡°
            (r'<ref[^/>]*/>', ''),            # ë‹¨ì¼ ì°¸ì¡°
            (r'<[^>]+>', ''),                 # HTML íƒœê·¸
            (r'\{\|[^}]*\|\}', ''),           # í…Œì´ë¸”
            (r'^\*+\s*', '', re.MULTILINE),   # ë¦¬ìŠ¤íŠ¸ ë§ˆì»¤
            (r'^#+\s*', '', re.MULTILINE),    # ë²ˆí˜¸ ë¦¬ìŠ¤íŠ¸
            (r'^=+\s*([^=]+)\s*=+', r'\1', re.MULTILINE),  # í—¤ë”
            (r'\n{3,}', '\n\n'),              # ê³¼ë‹¤ ì¤„ë°”ê¿ˆ
        ]
        
        for pattern, replacement, *flags in patterns:
            flag = flags[0] if flags else 0
            text = re.sub(pattern, replacement, text, flags=flag)
        
        # ê³µë°± ì •ë¦¬
        text = ' '.join(text.split())
        
        return text.strip()
    
    def _is_valid_article(self, title: str) -> bool:
        """ìœ íš¨í•œ ë¬¸ì„œì¸ì§€ í™•ì¸ (íŠ¹ìˆ˜ í˜ì´ì§€ ì œì™¸)"""
        invalid_prefixes = [
            "ìœ„í‚¤ë°±ê³¼:", "Wikipedia:", "í‹€:", "Template:",
            "ë¶„ë¥˜:", "Category:", "íŒŒì¼:", "File:",
            "ë„ì›€ë§:", "Help:", "ì‚¬ìš©ì:", "User:",
            "í† ë¡ :", "Talk:", "ëª¨ë“ˆ:", "Module:"
        ]
        
        for prefix in invalid_prefixes:
            if title.startswith(prefix):
                return False
        
        return True
    
    def stream_articles(self, max_articles: int = None, min_length: int = 100) -> Generator[Dict[str, str], None, None]:
        """
        ë¬¸ì„œ ìŠ¤íŠ¸ë¦¬ë°
        
        max_articles: ìµœëŒ€ ë¬¸ì„œ ìˆ˜ (None=ì „ì²´)
        min_length: ìµœì†Œ ë³¸ë¬¸ ê¸¸ì´
        
        Yields: {"title": str, "content": str}
        """
        logger.info(f"ğŸ”„ Starting to stream articles (max: {max_articles or 'unlimited'})...")
        
        # bz2 ì••ì¶• ë˜ëŠ” ì¼ë°˜ XML
        if str(self.dump_path).endswith('.bz2'):
            file_handle = bz2.open(self.dump_path, 'rt', encoding='utf-8')
        else:
            file_handle = open(self.dump_path, 'r', encoding='utf-8')
        
        try:
            # ìŠ¤íŠ¸ë¦¬ë° íŒŒì‹±
            context = ET.iterparse(file_handle, events=('end',))
            
            # [CRITICAL PATCH] Handle Truncated BZ2 Files gracefully
            # Instead of crashing on EOFError, we stop and yield what we have.
            try:
                for event, elem in context:
                    # <page> íƒœê·¸ ì™„ë£Œ ì‹œ

                    # [ROBUST PATCH] Namespace-agnostic tag check
                    tag_name = elem.tag.split('}')[-1] if '}' in elem.tag else elem.tag
                    
                    if tag_name == "page":
                        title = ""
                        raw_text = ""
                        
                        # Navigate children manually to find title and text
                        # Structure: page -> title, page -> revision -> text
                        for child in elem:
                            child_tag = child.tag.split('}')[-1] if '}' in child.tag else child.tag
                            if child_tag == "title":
                                title = child.text or ""
                            elif child_tag == "revision":
                                for sub in child:
                                    sub_tag = sub.tag.split('}')[-1] if '}' in sub.tag else sub.tag
                                    if sub_tag == "text":
                                        raw_text = sub.text or ""
                        
                        if title and raw_text:
                            # ìœ íš¨ì„± ê²€ì‚¬ (ê¸°ì¡´ Prefix)
                            if not self._is_valid_article(title):
                                self.skipped_special += 1
                                elem.clear()
                                continue
                            
                            # [NEW] Concept Sanitizer Inclusion
                            from Core.Foundation.concept_sanitizer import get_sanitizer
                            sanitizer = get_sanitizer()
                            if not sanitizer.is_valid(title):
                                elem.clear()
                                continue

                            # ìœ„í‚¤í…ìŠ¤íŠ¸ ì •ì œ
                            content = self._clean_wikitext(raw_text)
                            
                            # ë¦¬ë‹¤ì´ë ‰íŠ¸ ìŠ¤í‚µ
                            if not content:
                                self.skipped_redirects += 1
                                elem.clear()
                                continue
                            
                            # ìµœì†Œ ê¸¸ì´ ì²´í¬
                            if len(content) < min_length:
                                elem.clear()
                                continue
                            
                            self.total_parsed += 1
                            
                            # ì§„í–‰ ë¡œê·¸
                            if self.total_parsed % 1000 == 0:
                                logger.info(f"   ğŸ“„ Parsed {self.total_parsed} articles...")
                            
                            yield {
                                "title": title,
                                "content": content[:2000]  # ìµœëŒ€ 2000ì
                            }
                            
                            # ìµœëŒ€ ë¬¸ì„œ ìˆ˜ ì²´í¬
                            if max_articles and self.total_parsed >= max_articles:
                                break
                    
                    # ë©”ëª¨ë¦¬ ì •ë¦¬
                    elem.clear()
            except (EOFError, OSError) as e:
                logger.warning(f"âš ï¸ Compressed file truncated or corrupted: {e}")
                logger.warning("   Stopping stream gracefully and preserving processed data.")
                    
        finally:
            file_handle.close()
        
        logger.info(f"âœ… Parsing complete: {self.total_parsed} articles")
        logger.info(f"   Skipped: {self.skipped_redirects} redirects, {self.skipped_special} special pages")
    
    def absorb_to_universe(self, max_articles: int = 1000, batch_size: int = 100) -> Dict[str, int]:
        """
        Wikipedia ë¤í”„ë¥¼ ElysiaCoreë¥¼ í†µí•´ í¡ìˆ˜ (4-Thread Orchestra)
        """
        from Core.Foundation.Core_Logic.Elysia.elysia_core import get_elysia_core
        
        core = get_elysia_core()
        
        results = {"total": 0, "processed": 0, "failed": 0}
        
        logger.info("ğŸ» Starting Orchestral Absorption...")
        
        for article in self.stream_articles(max_articles=max_articles):
            title = article['title']
            content = article['content']
            
            try:
                # ElysiaCoreì˜ learn() ë©”ì†Œë“œ í˜¸ì¶œ -> 4-Thread Orchestra íŠ¸ë¦¬ê±°
                core.learn(content, title)
                
                results["processed"] += 1
                if results["processed"] % 10 == 0:
                    logger.info(f"   ğŸµ Processed {results['processed']} articles...")
                    
            except Exception as e:
                logger.error(f"Failed to process '{title}': {e}")
                results["failed"] += 1
                
            results["total"] += 1
        
        logger.info(f"ğŸ‰ Orchestral Absorption Complete!")
        logger.info(f"   Total: {results['total']}, Processed: {results['processed']}, Failed: {results['failed']}")
        
        return results

# CLI
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Wikipedia Dump Parser")
    parser.add_argument("dump_path", help="Path to Wikipedia XML dump (can be .bz2)")
    parser.add_argument("--max", type=int, default=1000, help="Max articles to process")
    parser.add_argument("--batch", type=int, default=100, help="Batch size")
    parser.add_argument("--preview", action="store_true", help="Preview first 5 articles only")
    
    args = parser.parse_args()
    
    try:
        wiki_parser = WikipediaDumpParser(args.dump_path)
        
        if args.preview:
            print("\n" + "="*60)
            print("ğŸ“– PREVIEW MODE - First 5 articles")
            print("="*60)
            
            for i, article in enumerate(wiki_parser.stream_articles(max_articles=5)):
                print(f"\n--- {article['title']} ---")
                print(article['content'][:300] + "...")
                
        else:
            print("\n" + "="*60)
            print("ğŸ§  ABSORBING TO INTERNAL UNIVERSE")
            print("="*60)
            
            results = wiki_parser.absorb_to_universe(
                max_articles=args.max,
                batch_size=args.batch
            )
            
            print(f"\nâœ… Done! Absorbed {results['absorbed']} articles from Wikipedia")
            
    except FileNotFoundError as e:
        print(f"âŒ Error: {e}")
        print("\nğŸ’¡ Download Wikipedia dump from:")
        print("   Korean: https://dumps.wikimedia.org/kowiki/latest/kowiki-latest-pages-articles.xml.bz2")
        print("   English: https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2")
