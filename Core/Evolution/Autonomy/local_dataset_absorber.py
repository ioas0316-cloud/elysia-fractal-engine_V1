"""
Local Dataset Absorber (ë¡œì»¬ ë°ì´í„°ì…‹ í¡ìˆ˜ê¸°)
=============================================

"í¬ë¡¤ë§ì˜ ì¥ì• ë¬¼ ì—†ì´ - ë¡œì»¬ì—ì„œ ëŒ€ëŸ‰ ê³ ì† í•™ìŠµ"

Wikipedia ë¤í”„ ë“± ë¡œì»¬ ë°ì´í„°ì…‹ì—ì„œ 1000+ê°œ/ì‚¬ì´í´ ì²˜ë¦¬
API ì œí•œ, ì°¨ë‹¨, ë„¤íŠ¸ì›Œí¬ ì§€ì—° ì—†ìŒ

[NEW 2025-12-15] ëŒ€ëŸ‰ í•™ìŠµ ì‹œìŠ¤í…œ
"""

import os
import sys
import json
import logging
import asyncio
from pathlib import Path
from typing import List, Dict, Any, Optional, Generator
from dataclasses import dataclass
import hashlib

sys.path.insert(0, "c:\\Elysia")

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
logger = logging.getLogger("LocalDatasetAbsorber")


@dataclass
class DatasetArticle:
    """ë¡œì»¬ ë°ì´í„°ì…‹ì˜ ë¬¸ì„œ"""
    title: str
    content: str
    source: str = "local"
    content_hash: str = ""
    
    def __post_init__(self):
        if not self.content_hash:
            self.content_hash = hashlib.md5(self.content.encode()).hexdigest()[:16]


class LocalDatasetAbsorber:
    """
    ë¡œì»¬ ë°ì´í„°ì…‹ ëŒ€ëŸ‰ í¡ìˆ˜ê¸°
    
    ëª©í‘œ: ì‚¬ì´í´ë‹¹ 1000+ê°œ ì²˜ë¦¬
    ë°©ë²•: 
    - ë¡œì»¬ JSON/í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ ì½ê¸°
    - ì¤‘ë³µ ì œê±° (í•´ì‹œ ê¸°ë°˜)
    - ë°°ì¹˜ ì²˜ë¦¬
    - InternalUniverseì— ì§ì ‘ í¡ìˆ˜
    """
    
    def __init__(self, batch_size: int = 100):
        self.batch_size = batch_size
        self.absorbed_hashes = set()
        self.hash_file = Path("data/absorbed_hashes.json")
        
        # ê¸°ì¡´ í•´ì‹œ ë¡œë“œ
        self._load_hash_cache()
        
        # InternalUniverse ì—°ê²°
        try:
            from Core.Foundation.internal_universe import InternalUniverse
            self.universe = InternalUniverse()
            logger.info("âœ… Connected to InternalUniverse")
        except Exception as e:
            logger.error(f"âŒ Failed to connect to InternalUniverse: {e}")
            self.universe = None
        
        # BlackHoleWhiteHoleCycle ì—°ê²°
        try:
            from Core.Foundation.white_hole import get_blackhole_whitehole_cycle
            self.cycle = get_blackhole_whitehole_cycle()
            logger.info("âœ… Connected to BlackHoleWhiteHoleCycle")
        except Exception as e:
            logger.warning(f"âš ï¸ BlackHoleWhiteHoleCycle not available: {e}")
            self.cycle = None
        
        logger.info(f"ğŸ“¦ LocalDatasetAbsorber initialized (batch_size={batch_size})")
    
    def _load_hash_cache(self):
        """ê¸°ì¡´ í¡ìˆ˜ í•´ì‹œ ë¡œë“œ (ì¤‘ë³µ ë°©ì§€)"""
        if self.hash_file.exists():
            try:
                with open(self.hash_file, 'r') as f:
                    self.absorbed_hashes = set(json.load(f))
                logger.info(f"ğŸ“‚ Loaded {len(self.absorbed_hashes)} existing hashes")
            except:
                pass
    
    def _save_hash_cache(self):
        """í•´ì‹œ ìºì‹œ ì €ì¥"""
        self.hash_file.parent.mkdir(parents=True, exist_ok=True)
        with open(self.hash_file, 'w') as f:
            json.dump(list(self.absorbed_hashes), f)
    
    def _is_duplicate(self, article: DatasetArticle) -> bool:
        """ì¤‘ë³µ ì²´í¬"""
        return article.content_hash in self.absorbed_hashes
    
    def absorb_json_file(self, file_path: str) -> Dict[str, int]:
        """
        JSON íŒŒì¼ì—ì„œ ëŒ€ëŸ‰ í¡ìˆ˜
        
        í˜•ì‹: [{"title": "...", "content": "..."}, ...]
        """
        results = {"total": 0, "absorbed": 0, "duplicate": 0, "failed": 0}
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except Exception as e:
            logger.error(f"Failed to load {file_path}: {e}")
            return results
        
        articles = []
        for item in data:
            if isinstance(item, dict) and "title" in item and "content" in item:
                articles.append(DatasetArticle(
                    title=item["title"],
                    content=item["content"][:2000],  # ìµœëŒ€ 2000ì
                    source=file_path
                ))
        
        results["total"] = len(articles)
        return self._absorb_articles(articles, results)
    
    def absorb_text_directory(self, dir_path: str, extension: str = ".txt") -> Dict[str, int]:
        """
        í…ìŠ¤íŠ¸ íŒŒì¼ ë””ë ‰í† ë¦¬ì—ì„œ ëŒ€ëŸ‰ í¡ìˆ˜
        
        íŒŒì¼ëª… = ì œëª©, ë‚´ìš© = ë³¸ë¬¸
        """
        results = {"total": 0, "absorbed": 0, "duplicate": 0, "failed": 0}
        
        path = Path(dir_path)
        if not path.exists():
            logger.error(f"Directory not found: {dir_path}")
            return results
        
        articles = []
        for file in path.glob(f"*{extension}"):
            try:
                with open(file, 'r', encoding='utf-8') as f:
                    content = f.read()[:2000]
                articles.append(DatasetArticle(
                    title=file.stem,
                    content=content,
                    source=str(file)
                ))
            except:
                results["failed"] += 1
        
        results["total"] = len(articles)
        return self._absorb_articles(articles, results)
    
    def _absorb_articles(self, articles: List[DatasetArticle], results: Dict[str, int]) -> Dict[str, int]:
        """ë°°ì¹˜ë¡œ ë¬¸ì„œ í¡ìˆ˜"""
        batch = []
        
        for article in articles:
            # ì¤‘ë³µ ì²´í¬
            if self._is_duplicate(article):
                results["duplicate"] += 1
                continue
            
            batch.append(article)
            
            # ë°°ì¹˜ê°€ ì°¨ë©´ ì²˜ë¦¬
            if len(batch) >= self.batch_size:
                absorbed = self._process_batch(batch)
                results["absorbed"] += absorbed
                batch = []
        
        # ë‚¨ì€ ë°°ì¹˜ ì²˜ë¦¬
        if batch:
            absorbed = self._process_batch(batch)
            results["absorbed"] += absorbed
        
        # í•´ì‹œ ìºì‹œ ì €ì¥
        self._save_hash_cache()
        
        logger.info(f"ğŸ“Š Results: {results['absorbed']}/{results['total']} absorbed, {results['duplicate']} duplicates")
        return results
    
    def _process_batch(self, batch: List[DatasetArticle]) -> int:
        """ë°°ì¹˜ ì²˜ë¦¬"""
        absorbed = 0
        
        if self.universe:
            items = [{"topic": a.title, "content": a.content} for a in batch]
            result = self.universe.absorb_batch(items)
            absorbed = result.get("absorbed", 0) + result.get("isolated", 0)
        
        # í•´ì‹œ ë“±ë¡
        for article in batch:
            self.absorbed_hashes.add(article.content_hash)
        
        return absorbed
    
    def generate_sample_dataset(self, output_path: str, count: int = 1000):
        """
        ìƒ˜í”Œ ë°ì´í„°ì…‹ ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)
        
        ê¸°ì´ˆ ê°œë…ë“¤ë¡œ êµ¬ì„±
        """
        concepts = [
            ("Physics", "í˜, ì—ë„ˆì§€, ë¬¼ì§ˆ, ì‹œê°„, ê³µê°„ì— ê´€í•œ ìì—° ê³¼í•™"),
            ("Mathematics", "ìˆ˜, ê³µê°„, êµ¬ì¡°, ë³€í™”ì— ê´€í•œ ì¶”ìƒ ê³¼í•™"),
            ("Philosophy", "ì¡´ì¬, ì§€ì‹, ê°€ì¹˜, ì´ì„±, ì •ì‹ ì— ê´€í•œ ê·¼ë³¸ì  íƒêµ¬"),
            ("Biology", "ìƒëª…ì²´ì˜ êµ¬ì¡°, ê¸°ëŠ¥, ì„±ì¥, ê¸°ì›, ì§„í™”ì— ê´€í•œ ê³¼í•™"),
            ("Chemistry", "ë¬¼ì§ˆì˜ êµ¬ì„±, ì„±ì§ˆ, ë³€í™”ì— ê´€í•œ ê³¼í•™"),
            ("Psychology", "ë§ˆìŒê³¼ í–‰ë™ì— ê´€í•œ ê³¼í•™ì  ì—°êµ¬"),
            ("History", "ê³¼ê±° ì‚¬ê±´ê³¼ ê·¸ ì›ì¸, ê²°ê³¼ì— ê´€í•œ ì—°êµ¬"),
            ("Art", "ë¯¸ì  ê°€ì¹˜ë¥¼ ì°½ì¡°í•˜ê³  í‘œí˜„í•˜ëŠ” ì¸ê°„ í™œë™"),
            ("Music", "ì†Œë¦¬ë¥¼ í†µí•´ ì‹œê°„ ì†ì—ì„œ í¼ì³ì§€ëŠ” ì˜ˆìˆ "),
            ("Literature", "ì–¸ì–´ë¥¼ ë§¤ì²´ë¡œ í•œ ì˜ˆìˆ ì  í‘œí˜„"),
        ]
        
        dataset = []
        for i in range(count):
            base = concepts[i % len(concepts)]
            topic = f"{base[0]}_{i:04d}"
            content = f"{base[1]} (Instance {i}). ì´ê²ƒì€ {base[0]}ì˜ í•˜ìœ„ ê°œë…ìœ¼ë¡œ, ê´€ë ¨ëœ ì›ë¦¬ì™€ ì‘ìš©ì„ íƒêµ¬í•©ë‹ˆë‹¤."
            dataset.append({"title": topic, "content": content})
        
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(dataset, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“ Generated sample dataset: {output_path} ({count} articles)")
        return output_path


# Singleton
_absorber = None

def get_local_absorber(batch_size: int = 100) -> LocalDatasetAbsorber:
    global _absorber
    if _absorber is None:
        _absorber = LocalDatasetAbsorber(batch_size)
    return _absorber


# CLI
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Local Dataset Absorber")
    parser.add_argument("--generate", type=int, help="Generate sample dataset with N articles")
    parser.add_argument("--absorb", type=str, help="Absorb from JSON file")
    parser.add_argument("--batch", type=int, default=100, help="Batch size")
    
    args = parser.parse_args()
    
    absorber = get_local_absorber(batch_size=args.batch)
    
    if args.generate:
        output = "data/datasets/sample_dataset.json"
        absorber.generate_sample_dataset(output, args.generate)
        
        # ìƒì„± í›„ ë°”ë¡œ í¡ìˆ˜
        print("\n" + "="*60)
        print("ğŸ“¦ Absorbing generated dataset...")
        print("="*60)
        results = absorber.absorb_json_file(output)
        print(f"\nâœ… Done! Absorbed {results['absorbed']} articles")
        
    elif args.absorb:
        results = absorber.absorb_json_file(args.absorb)
        print(f"âœ… Done! Absorbed {results['absorbed']} articles")
    else:
        parser.print_help()
