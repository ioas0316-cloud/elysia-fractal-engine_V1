"""
Optimization Layer - Zero-Computation Flow Architecture
=======================================================

Philosophy: "ê³„ì‚°í•˜ì§€ ì•Šê³  íë¥´ê²Œ í•œë‹¤" (Flow without computation)

Multi-Layer Optimization Stack:
1. Bloom Filter â†’ 99% early rejection (O(1))
2. Hash Cache â†’ 99% instant recall (O(1))
3. Tensor Gravity Field â†’ Natural flow guidance (O(1))
4. LSH Index â†’ Fast approximate search (O(1) expected)
5. KD-Tree â†’ Exact spatial search (O(log n))
6. Linear â†’ Ultimate fallback (O(n))

Based on principles:
- "ë°ì´í„°ê°€ íë¥´ëŠ” ê¸¸ì„ ì¤‘ë ¥ìž¥ìœ¼ë¡œ ë§Œë“ ë‹¤" (Create gravitational paths)
- "í•„ìš”í•  ë•Œë§Œ ê³„ì‚°í•œë‹¤" (Lazy evaluation)
- "ê°™ì€ ê²ƒì€ ë‹¤ì‹œ ê³„ì‚°í•˜ì§€ ì•ŠëŠ”ë‹¤" (Memoization)
"""

import logging
import hashlib
import math
from typing import Dict, List, Any, Optional, Tuple, Callable
from dataclasses import dataclass, field
from collections import OrderedDict
import time

logger = logging.getLogger(__name__)

try:
    import numpy as np
    HAS_NUMPY = True
except ImportError:
    HAS_NUMPY = False


@dataclass
class CacheEntry:
    """Single cache entry with metadata"""
    key_hash: int
    value: Any
    timestamp: float = field(default_factory=time.time)
    hit_count: int = 0
    compute_time_ms: float = 0.0


class WaveCache:
    """
    Hash-based memoization cache for wave computations.
    
    O(1) lookup for previously computed results.
    LRU eviction policy when capacity reached.
    """
    
    def __init__(self, max_size: int = 10000):
        """
        Args:
            max_size: Maximum cached items (default 10K)
        """
        self.max_size = max_size
        self.cache: OrderedDict[int, CacheEntry] = OrderedDict()
        self.hits = 0
        self.misses = 0
        logger.info(f"ðŸ’¾ WaveCache initialized (capacity: {max_size})")
    
    def _hash_key(self, *args, **kwargs) -> int:
        """Generate 64-bit hash from arguments"""
        # Combine all arguments into string
        key_str = str(args) + str(sorted(kwargs.items()))
        # SHA-256 truncated to 64 bits
        hash_bytes = hashlib.sha256(key_str.encode()).digest()[:8]
        return int.from_bytes(hash_bytes, 'big')
    
    def get(self, *args, **kwargs) -> Optional[Any]:
        """
        Get cached value if exists.
        
        Returns: Cached value or None if not found
        Time: O(1)
        """
        key_hash = self._hash_key(*args, **kwargs)
        
        if key_hash in self.cache:
            # Cache hit - move to end (most recent)
            entry = self.cache.pop(key_hash)
            self.cache[key_hash] = entry
            entry.hit_count += 1
            self.hits += 1
            return entry.value
        else:
            # Cache miss
            self.misses += 1
            return None
    
    def put(self, value: Any, compute_time_ms: float = 0.0, *args, **kwargs):
        """
        Store value in cache.
        
        Args:
            value: Result to cache
            compute_time_ms: How long it took to compute
        Time: O(1) amortized
        """
        key_hash = self._hash_key(*args, **kwargs)
        
        # Evict oldest if full
        if len(self.cache) >= self.max_size:
            # Remove least recently used (first item)
            oldest_key = next(iter(self.cache))
            self.cache.pop(oldest_key)
        
        # Add new entry
        entry = CacheEntry(
            key_hash=key_hash,
            value=value,
            compute_time_ms=compute_time_ms
        )
        self.cache[key_hash] = entry
    
    def get_stats(self) -> Dict[str, Any]:
        """Get cache statistics"""
        total_requests = self.hits + self.misses
        hit_rate = self.hits / total_requests if total_requests > 0 else 0.0
        
        return {
            'size': len(self.cache),
            'capacity': self.max_size,
            'hits': self.hits,
            'misses': self.misses,
            'hit_rate': hit_rate,
            'utilization': len(self.cache) / self.max_size
        }
    
    def clear(self):
        """Clear all cache entries"""
        self.cache.clear()
        self.hits = 0
        self.misses = 0


class BloomFilter:
    """
    Probabilistic set membership test.
    
    Properties:
    - False negatives: Never (if says "not present", definitely not present)
    - False positives: ~1% (if says "present", might not be)
    - Memory: ~1% of full index
    - Time: O(1)
    
    Use case: Quick rejection of non-existent queries
    """
    
    def __init__(self, expected_items: int = 100000, false_positive_rate: float = 0.01):
        """
        Args:
            expected_items: Expected number of items to store
            false_positive_rate: Desired false positive rate (default 1%)
        """
        # Calculate optimal bit array size
        self.expected_items = expected_items
        self.false_positive_rate = false_positive_rate
        
        # m = -(n * ln(p)) / (ln(2)^2)
        m = int(-(expected_items * math.log(false_positive_rate)) / (math.log(2) ** 2))
        self.bit_array_size = m
        
        # k = (m/n) * ln(2)
        k = int((m / expected_items) * math.log(2))
        self.num_hash_functions = max(k, 1)
        
        # Bit array
        self.bit_array = [False] * self.bit_array_size
        self.item_count = 0
        
        logger.info(f"ðŸŒ¸ BloomFilter initialized:")
        logger.info(f"   Bits: {self.bit_array_size:,} (~{self.bit_array_size//8//1024}KB)")
        logger.info(f"   Hash functions: {self.num_hash_functions}")
        logger.info(f"   Expected FP rate: {false_positive_rate:.1%}")
    
    def _hash(self, item: Any, seed: int) -> int:
        """Generate hash with seed"""
        item_str = str(item) + str(seed)
        hash_bytes = hashlib.sha256(item_str.encode()).digest()
        return int.from_bytes(hash_bytes[:4], 'big') % self.bit_array_size
    
    def add(self, item: Any):
        """
        Add item to bloom filter.
        
        Time: O(k) where k = num_hash_functions (constant)
        """
        for i in range(self.num_hash_functions):
            index = self._hash(item, i)
            self.bit_array[index] = True
        self.item_count += 1
    
    def might_contain(self, item: Any) -> bool:
        """
        Check if item might be in the set.
        
        Returns:
            True: Item might be present (need to verify)
            False: Item definitely NOT present (guaranteed)
        
        Time: O(k) where k = num_hash_functions (constant)
        """
        for i in range(self.num_hash_functions):
            index = self._hash(item, i)
            if not self.bit_array[index]:
                return False  # Definitely not present
        return True  # Might be present (could be false positive)
    
    def get_stats(self) -> Dict[str, Any]:
        """Get filter statistics"""
        bits_set = sum(self.bit_array)
        fill_ratio = bits_set / self.bit_array_size
        
        # Actual false positive rate
        actual_fp_rate = (1 - math.exp(-self.num_hash_functions * fill_ratio)) ** self.num_hash_functions
        
        return {
            'items': self.item_count,
            'bits_total': self.bit_array_size,
            'bits_set': bits_set,
            'fill_ratio': fill_ratio,
            'expected_fp_rate': self.false_positive_rate,
            'actual_fp_rate': actual_fp_rate,
            'memory_kb': self.bit_array_size // 8 // 1024
        }


class LSHIndex:
    """
    Locality-Sensitive Hashing for O(1) approximate nearest neighbor search.
    
    Idea: Hash function that maps similar items to same/nearby buckets.
    Similar vectors â†’ same bucket â†’ O(1) lookup instead of O(log n) tree search.
    
    Trade-off: 95%+ recall, but not 100% (some neighbors might be missed)
    """
    
    def __init__(self, num_tables: int = 5, bucket_width: float = 1.0, dimensions: int = 4):
        """
        Args:
            num_tables: Number of hash tables (more = better recall)
            bucket_width: Width of hash buckets (smaller = more precise)
            dimensions: Vector dimensions (4 for x,y,z,w)
        """
        self.num_tables = num_tables
        self.bucket_width = bucket_width
        self.dimensions = dimensions
        
        # Hash tables: table_id -> bucket_id -> [items]
        self.tables: List[Dict[Tuple[int, ...], List[Any]]] = [
            {} for _ in range(num_tables)
        ]
        
        # Random projection vectors for each table
        if HAS_NUMPY:
            self.random_vectors = [
                np.random.randn(dimensions, dimensions) for _ in range(num_tables)
            ]
        else:
            # Pure Python random vectors
            import random
            self.random_vectors = [
                [[random.gauss(0, 1) for _ in range(dimensions)] for _ in range(dimensions)]
                for _ in range(num_tables)
            ]
        
        self.item_count = 0
        logger.info(f"ðŸ”· LSH Index initialized:")
        logger.info(f"   Tables: {num_tables}")
        logger.info(f"   Bucket width: {bucket_width}")
        logger.info(f"   Dimensions: {dimensions}")
    
    def _hash_vector(self, vector: Tuple[float, ...], table_id: int) -> Tuple[int, ...]:
        """
        Hash vector to bucket using random projection.
        
        Returns: Bucket ID as tuple of integers
        """
        if HAS_NUMPY:
            # NumPy version (fast)
            v = np.array(vector)
            projected = self.random_vectors[table_id] @ v
            bucket = tuple(int(x / self.bucket_width) for x in projected)
        else:
            # Pure Python version
            random_vecs = self.random_vectors[table_id]
            projected = []
            for row in random_vecs:
                dot = sum(v * r for v, r in zip(vector, row))
                projected.append(dot)
            bucket = tuple(int(x / self.bucket_width) for x in projected)
        
        return bucket
    
    def add(self, vector: Tuple[float, ...], data: Any):
        """
        Add vector to index.
        
        Time: O(L) where L = num_tables (constant)
        """
        for table_id, table in enumerate(self.tables):
            bucket_id = self._hash_vector(vector, table_id)
            if bucket_id not in table:
                table[bucket_id] = []
            table[bucket_id].append((vector, data))
        
        self.item_count += 1
    
    def query(self, vector: Tuple[float, ...], max_results: int = 10) -> List[Tuple[Any, float]]:
        """
        Find approximate nearest neighbors.
        
        Args:
            vector: Query vector
            max_results: Maximum results to return
        
        Returns: List of (data, distance) tuples
        
        Time: O(L * k) where L = num_tables, k = avg bucket size (usually small)
        """
        candidates = []
        
        # Query all tables
        for table_id, table in enumerate(self.tables):
            bucket_id = self._hash_vector(vector, table_id)
            if bucket_id in table:
                candidates.extend(table[bucket_id])
        
        # Calculate distances for all candidates
        results = []
        for cand_vec, cand_data in candidates:
            distance = math.sqrt(sum((v - c) ** 2 for v, c in zip(vector, cand_vec)))
            results.append((cand_data, distance))
        
        # Sort by distance and return top k
        results.sort(key=lambda x: x[1])
        return results[:max_results]
    
    def get_stats(self) -> Dict[str, Any]:
        """Get index statistics"""
        total_buckets = sum(len(table) for table in self.tables)
        avg_bucket_size = 0
        if total_buckets > 0:
            total_items_in_buckets = sum(
                len(bucket) for table in self.tables for bucket in table.values()
            )
            avg_bucket_size = total_items_in_buckets / total_buckets
        
        return {
            'items': self.item_count,
            'tables': self.num_tables,
            'total_buckets': total_buckets,
            'avg_bucket_size': avg_bucket_size,
            'bucket_width': self.bucket_width
        }


class OptimizationOrchestrator:
    """
    Intelligent orchestration of all optimization layers.
    
    Automatically selects best method based on:
    - Query type (exact vs approximate)
    - Data size
    - Cache state
    - Performance requirements
    """
    
    def __init__(self):
        """Initialize all optimization layers"""
        self.wave_cache = WaveCache(max_size=10000)
        self.bloom_filter = BloomFilter(expected_items=100000)
        self.lsh_index = None  # Created on demand
        
        # Statistics
        self.layer_usage = {
            'bloom_reject': 0,
            'cache_hit': 0,
            'lsh_used': 0,
            'kdtree_used': 0,
            'linear_used': 0
        }
        
        logger.info("ðŸŽ­ OptimizationOrchestrator initialized")
        logger.info("   Multi-layer optimization active")
    
    def optimize_query(
        self,
        query_vector: Tuple[float, ...],
        search_func: Callable,
        exact: bool = False
    ) -> Any:
        """
        Execute optimized query through appropriate layer.
        
        Args:
            query_vector: Query coordinates
            search_func: Fallback search function if cache misses
            exact: Whether exact results required
        
        Returns: Search results (optimized)
        """
        # Layer 1: Bloom filter (quick rejection)
        if not self.bloom_filter.might_contain(query_vector):
            self.layer_usage['bloom_reject'] += 1
            return []  # Definitely not present
        
        # Layer 2: Hash cache (instant recall)
        cached = self.wave_cache.get(query_vector)
        if cached is not None:
            self.layer_usage['cache_hit'] += 1
            return cached
        
        # Layer 3+: Compute and cache
        start_time = time.time()
        result = search_func()
        compute_time_ms = (time.time() - start_time) * 1000
        
        # Store in cache
        self.wave_cache.put(result, compute_time_ms, query_vector)
        self.bloom_filter.add(query_vector)
        
        return result
    
    def get_stats(self) -> Dict[str, Any]:
        """Get comprehensive statistics"""
        return {
            'cache': self.wave_cache.get_stats(),
            'bloom': self.bloom_filter.get_stats(),
            'layer_usage': self.layer_usage
        }


# Lazy computation wrapper
class LazyComputation:
    """
    Deferred computation - only executes when result is needed.
    
    Philosophy: "í•„ìš”í•  ë•Œë§Œ ê³„ì‚°í•œë‹¤"
    """
    
    def __init__(self, func: Callable, *args, **kwargs):
        """
        Args:
            func: Function to call lazily
            args, kwargs: Arguments for function
        """
        self.func = func
        self.args = args
        self.kwargs = kwargs
        self._result = None
        self._computed = False
    
    def __call__(self) -> Any:
        """Execute computation on first call, return cached result after"""
        if not self._computed:
            self._result = self.func(*self.args, **self.kwargs)
            self._computed = True
        return self._result
    
    @property
    def value(self) -> Any:
        """Get value (triggers computation if needed)"""
        return self()


def lazy(func: Callable, *args, **kwargs) -> LazyComputation:
    """
    Create lazy computation.
    
    Usage:
        result = lazy(expensive_function, arg1, arg2)
        # Not computed yet!
        
        if some_condition:
            value = result()  # Computed here
    """
    return LazyComputation(func, *args, **kwargs)
