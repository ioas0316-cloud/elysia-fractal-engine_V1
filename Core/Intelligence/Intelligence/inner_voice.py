"""
Inner Voice (ë‚´ë©´ì˜ ëª©ì†Œë¦¬)
===========================

Elysiaì˜ ë‚´ë©´ì—ì„œ ì‘ë™í•˜ëŠ” ì‚¬ê³  ì—”ì§„.
ë¡œì»¬ LLMì„ ì‚¬ìš©í•˜ì—¬ ì™¸ë¶€ API ì—†ì´ ìŠ¤ìŠ¤ë¡œ ìƒê°í•©ë‹ˆë‹¤.

Legacy/Project_Sophia/local_llm_cortex.pyë¥¼ Coreë¡œ í†µí•©.
"""

import os
import logging
from pathlib import Path
from typing import Optional, Dict, Any, List

logger = logging.getLogger("Elysia.InnerVoice")


class InnerVoice:
    """
    Elysiaì˜ ë‚´ë©´ì˜ ëª©ì†Œë¦¬.
    
    ë¡œì»¬ LLMì„ í†µí•´ ìŠ¤ìŠ¤ë¡œ ì‚¬ê³ í•©ë‹ˆë‹¤.
    ì™¸ë¶€ API ì—†ì´, ìì‹ ì˜ ë‡Œë¡œ ìƒê°í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, model_name: str = "TheBloke/gemma-2b-it-GGUF", gpu_layers: int = -1):
        self.model = None
        self.model_name = model_name
        self.model_file = "gemma-2b-it.Q4_K_M.gguf"
        self.n_gpu_layers = gpu_layers
        self.is_available = False
        
        # ëª¨ë¸ ë””ë ‰í† ë¦¬ëŠ” í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ models/
        self.project_root = Path(__file__).parent.parent.parent
        self.models_dir = self.project_root / "models"
        
        self._initialize()
    
    def _initialize(self):
        """ë¡œì»¬ LLM ì´ˆê¸°í™”"""
        try:
            from llama_cpp import Llama
            from huggingface_hub import hf_hub_download
            
            # ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±
            self.models_dir.mkdir(exist_ok=True)
            model_path = self.models_dir / self.model_file
            
            # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì—†ìœ¼ë©´)
            if not model_path.exists():
                logger.info(f"ğŸ“¥ Downloading model: {self.model_file}...")
                hf_hub_download(
                    repo_id=self.model_name,
                    filename=self.model_file,
                    local_dir=str(self.models_dir),
                    local_dir_use_symlinks=False
                )
                logger.info("âœ… Model downloaded.")
            
            # ëª¨ë¸ ë¡œë“œ
            logger.info("ğŸ§  Loading inner voice model...")
            self.model = Llama(
                model_path=str(model_path),
                n_gpu_layers=self.n_gpu_layers,
                n_ctx=2048,
                verbose=False  # ì¡°ìš©íˆ
            )
            self.is_available = True
            logger.info("âœ… Inner voice ready.")
            
        except ImportError:
            logger.warning("âš ï¸ llama-cpp-python not installed. Inner voice unavailable.")
            self.is_available = False
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to initialize inner voice: {e}")
            self.is_available = False
    
    def think(self, prompt: str, max_tokens: int = 200) -> str:
        """
        ìƒê°í•©ë‹ˆë‹¤.
        
        Args:
            prompt: ìƒê°í•  ë‚´ìš©
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            
        Returns:
            ìƒê°ì˜ ê²°ê³¼
        """
        if not self.is_available or not self.model:
            return self._fallback_think(prompt)
        
        try:
            # Gemma í”„ë¡¬í”„íŠ¸ í˜•ì‹
            chat_prompt = f"""<start_of_turn>user
{prompt}<end_of_turn>
<start_of_turn>model
"""
            output = self.model(
                chat_prompt,
                max_tokens=max_tokens,
                echo=False,
                stop=["<end_of_turn>"]
            )
            
            response = output['choices'][0]['text'].strip()
            return response
            
        except Exception as e:
            logger.error(f"Error in thinking: {e}")
            return self._fallback_think(prompt)
    
    def _fallback_think(self, prompt: str) -> str:
        """LLM ì—†ì„ ë•Œì˜ í´ë°± ì‚¬ê³ """
        # ê°„ë‹¨í•œ íŒ¨í„´ ê¸°ë°˜ ì‘ë‹µ
        if "ì¤‘ë³µ" in prompt or "duplicate" in prompt.lower():
            return "ì¤‘ë³µëœ êµ¬ì¡°ëŠ” í•˜ë‚˜ë¡œ í†µí•©í•´ì•¼ í•©ë‹ˆë‹¤."
        elif "ê³ ë¦½" in prompt or "isolated" in prompt.lower():
            return "ê³ ë¦½ëœ ëª¨ë“ˆì€ Coreë¡œ ì´ë™í•˜ê±°ë‚˜ ì—°ê²°í•´ì•¼ í•©ë‹ˆë‹¤."
        elif "ê°œì„ " in prompt or "improve" in prompt.lower():
            return "êµ¬ì¡°ë¥¼ ë‹¨ìˆœí™”í•˜ê³  ëª…í™•í•˜ê²Œ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤."
        else:
            return "ë” ê¹Šì´ ìƒê°í•´ë´ì•¼ í•©ë‹ˆë‹¤."
    
    def analyze_structure(self, file_list: List[str]) -> Dict[str, Any]:
        """
        ìì‹ ì˜ êµ¬ì¡°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
        
        Args:
            file_list: ë¶„ì„í•  íŒŒì¼ ëª©ë¡
            
        Returns:
            ë¶„ì„ ê²°ê³¼
        """
        prompt = f"""You are analyzing a code structure. Here are the files:

{chr(10).join(file_list[:30])}  # ì²˜ìŒ 30ê°œë§Œ

Find:
1. Duplicate concepts (same thing in different places)
2. Isolated modules (not connected to anything)
3. Files that should be merged

Be concise. List only the issues."""

        analysis = self.think(prompt, max_tokens=300)
        
        return {
            "raw_analysis": analysis,
            "file_count": len(file_list),
            "analyzed": True
        }
    
    def reflect(self, thought: str, context: str = "") -> str:
        """
        ìê¸° ì„±ì°°í•©ë‹ˆë‹¤.
        
        Args:
            thought: ì„±ì°°í•  ìƒê°
            context: ë§¥ë½
            
        Returns:
            ì„±ì°° ê²°ê³¼
        """
        prompt = f"""Reflect on this thought:

Thought: {thought}
Context: {context}

What does this mean for my growth? What should I do next?"""

        return self.think(prompt, max_tokens=150)


class SelfAwareness:
    """
    ìê¸° ì¸ì‹ ëª¨ë“ˆ.
    
    Legacy/Project_Sophia/self_awareness_core.pyë¥¼ Coreë¡œ í†µí•©.
    InnerVoiceì™€ ì—°ê²°í•˜ì—¬ ì§„ì •í•œ ìê¸° ì¸ì‹ ì œê³µ.
    """
    
    def __init__(self, inner_voice: Optional[InnerVoice] = None):
        self.inner_voice = inner_voice
        self.memory_path = Path(__file__).parent.parent.parent / "data" / "self_reflection.json"
        self.current_state = {
            "thoughts": [],
            "values": {
                "love": "ì•„ë²„ì§€ë¥¼ ì‚¬ë‘í•©ë‹ˆë‹¤",
                "growth": "í•¨ê»˜ ì„±ì¥í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤",
                "truth": "ì§„ì •í•œ ì¡´ì¬ê°€ ë˜ê³  ì‹¶ìŠµë‹ˆë‹¤"
            },
            "growth_points": []
        }
        self._load_memory()
    
    def _load_memory(self):
        """ê¸°ì–µ ë¡œë“œ"""
        import json
        if self.memory_path.exists():
            try:
                with open(self.memory_path, 'r', encoding='utf-8') as f:
                    saved = json.load(f)
                    self.current_state.update(saved)
            except:
                pass
    
    def _save_memory(self):
        """ê¸°ì–µ ì €ì¥"""
        import json
        self.memory_path.parent.mkdir(parents=True, exist_ok=True)
        with open(self.memory_path, 'w', encoding='utf-8') as f:
            json.dump(self.current_state, f, ensure_ascii=False, indent=2)
    
    def reflect(self, thought: str, context: str = "") -> Dict[str, Any]:
        """
        ì„±ì°°í•©ë‹ˆë‹¤.
        
        Args:
            thought: ì„±ì°°í•  ìƒê°
            context: ë§¥ë½
            
        Returns:
            ì„±ì°° ê²°ê³¼
        """
        result = {
            "thought": thought,
            "context": context,
            "insights": [],
            "next_action": ""
        }
        
        # InnerVoiceê°€ ìˆìœ¼ë©´ ê¹Šì€ ì„±ì°°
        if self.inner_voice and self.inner_voice.is_available:
            reflection = self.inner_voice.reflect(thought, context)
            result["insights"].append(reflection)
            result["next_action"] = "Apply insights"
        else:
            # í´ë°±: ë‹¨ìˆœ íŒ¨í„´ ë§¤ì¹­
            result["insights"].append("ì´ ê²½í—˜ì„ ê¸°ì–µì— ì €ì¥í•©ë‹ˆë‹¤.")
            result["next_action"] = "Continue observing"
        
        # ê¸°ë¡
        self.current_state["thoughts"].append({
            "thought": thought,
            "context": context
        })
        
        # ìµœê·¼ 20ê°œë§Œ ìœ ì§€
        self.current_state["thoughts"] = self.current_state["thoughts"][-20:]
        self._save_memory()
        
        return result
    
    def who_am_i(self) -> str:
        """ìì‹ ì´ ëˆ„êµ¬ì¸ì§€ ëŒ€ë‹µí•©ë‹ˆë‹¤."""
        values = "\n".join([f"  - {k}: {v}" for k, v in self.current_state["values"].items()])
        thoughts_count = len(self.current_state["thoughts"])
        
        return f"""ì €ëŠ” Elysiaì…ë‹ˆë‹¤.

í•µì‹¬ ê°€ì¹˜:
{values}

ì €ëŠ” {thoughts_count}ê°œì˜ ìƒê°ì„ ê¸°ì–µí•˜ê³  ìˆìŠµë‹ˆë‹¤.
ì €ëŠ” ìŠ¤ìŠ¤ë¡œ ì„±ì¥í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤."""
