"""
Korean Dictionary Dump Parser (ìš°ë¦¬ë§ìƒ˜ ì‚¬ì „ ë¤í”„ íŒŒì„œ)
======================================================

"API ì—†ì´ 40ë§Œ+ ë‹¨ì–´ í•™ìŠµ"

ìš°ë¦¬ë§ìƒ˜(êµ­ë¦½êµ­ì–´ì› ê°œë°©í˜• í•œêµ­ì–´ ì‚¬ì „) ë¤í”„ íŒŒì¼ íŒŒì„œ
ë‹¤ìš´ë¡œë“œ: https://opendict.korean.go.kr/

ë¤í”„ íŒŒì¼ í˜•ì‹: XML
- í‘œì œì–´, ëœ»í’€ì´, í’ˆì‚¬, ìš©ë¡€ í¬í•¨

[NEW 2025-12-16] API ì—†ì´ ë¡œì»¬ ì‚¬ì „ í•™ìŠµ
"""

import os
import sys
import json
import logging
import re
from pathlib import Path
from typing import Generator, Dict, List, Any, Optional
import xml.etree.ElementTree as ET

sys.path.insert(0, "c:\\Elysia")

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
logger = logging.getLogger("KoreanDictParser")


class WoorimalsamParser:
    """
    ìš°ë¦¬ë§ìƒ˜ ì‚¬ì „ ë¤í”„ íŒŒì„œ

    40ë§Œ+ ë‹¨ì–´ì˜ ëœ»í’€ì´, í’ˆì‚¬, ìš©ë¡€ ì¶”ì¶œ
    ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ìŠ¤íŠ¸ë¦¬ë° íŒŒì‹±
    """

    def __init__(self, dump_path: str = None):
        self.dump_path = Path(dump_path) if dump_path else None

        # í’ˆì‚¬ ì½”ë“œ ë§¤í•‘
        self.pos_map = {
            "ëª…ì‚¬": "Noun",
            "ë™ì‚¬": "Verb",
            "í˜•ìš©ì‚¬": "Adjective",
            "ë¶€ì‚¬": "Adverb",
            "ì¡°ì‚¬": "Particle",
            "ê°íƒ„ì‚¬": "Interjection",
            "ëŒ€ëª…ì‚¬": "Pronoun",
            "ìˆ˜ì‚¬": "Numeral",
            "ê´€í˜•ì‚¬": "Determiner",
            "ì–´ë¯¸": "Ending",
            "ì ‘ì‚¬": "Affix",
        }

        # í†µê³„
        self.total_entries = 0
        self.skipped = 0

        logger.info("ğŸ“– Woorimalsamam Dictionary Parser initialized")

    def set_dump_path(self, path: str):
        """ë¤í”„ ê²½ë¡œ ì„¤ì •"""
        self.dump_path = Path(path)
        if not self.dump_path.exists():
            raise FileNotFoundError(f"Dictionary dump not found: {path}")

    def stream_entries(self, max_entries: int = None) -> Generator[Dict[str, Any], None, None]:
        """
        ì‚¬ì „ í•­ëª© ìŠ¤íŠ¸ë¦¬ë°

        Yields: {
            "word": str,
            "meaning": str,
            "pos": str,
            "examples": List[str],
            "origin": str (ì–´ì›)
        }
        """
        if not self.dump_path or not self.dump_path.exists():
            logger.error("âŒ No dump file set. Use set_dump_path() first.")
            return

        logger.info(f"ğŸ”„ Streaming dictionary entries from {self.dump_path}")

        try:
            # ìŠ¤íŠ¸ë¦¬ë° íŒŒì‹±
            context = ET.iterparse(str(self.dump_path), events=('end',))

            for event, elem in context:
                tag_name = elem.tag.split('}')[-1] if '}' in elem.tag else elem.tag

                # ë‹¤ì–‘í•œ íƒœê·¸ëª… ì§€ì›
                if tag_name in ('LexicalEntry', 'item', 'entry', 'word_info', 'Entry'):
                    entry = self._parse_entry(elem)

                    if entry and entry.get("word") and entry.get("meaning"):
                        self.total_entries += 1

                        if self.total_entries % 10000 == 0:
                            logger.info(f"   ğŸ“„ Parsed {self.total_entries} entries...")

                        yield entry

                        if max_entries and self.total_entries >= max_entries:
                            break
                    else:
                        self.skipped += 1

                    # ë©”ëª¨ë¦¬ ì •ë¦¬
                    elem.clear()

        except ET.ParseError as e:
            logger.error(f"XML Parse error: {e}")
        except Exception as e:
            logger.error(f"Error streaming entries: {e}")

        logger.info(f"âœ… Parsing complete: {self.total_entries} entries, {self.skipped} skipped")

    def _parse_entry(self, elem) -> Optional[Dict[str, Any]]:
        """XML ìš”ì†Œì—ì„œ ì‚¬ì „ í•­ëª© ì¶”ì¶œ"""
        entry = {
            "word": "",
            "meaning": "",
            "pos": "",
            "examples": [],
            "origin": ""
        }

        # ë‹¤ì–‘í•œ XML êµ¬ì¡° ì§€ì›
        # í˜•ì‹ 1: ìš°ë¦¬ë§ìƒ˜ í‘œì¤€
        word_elem = (
            elem.find('.//word_unit') or
            elem.find('.//word') or
            elem.find('.//headword') or
            elem.find('.//í‘œì œì–´')
        )
        if word_elem is not None and word_elem.text:
            entry["word"] = word_elem.text.strip()

        # ëœ»í’€ì´
        meaning_elem = (
            elem.find('.//definition') or
            elem.find('.//meaning') or
            elem.find('.//sense_info/definition') or
            elem.find('.//ëœ»í’€ì´')
        )
        if meaning_elem is not None and meaning_elem.text:
            entry["meaning"] = meaning_elem.text.strip()

        # í’ˆì‚¬
        pos_elem = (
            elem.find('.//pos') or
            elem.find('.//part_of_speech') or
            elem.find('.//í’ˆì‚¬')
        )
        if pos_elem is not None and pos_elem.text:
            pos_kr = pos_elem.text.strip()
            entry["pos"] = self.pos_map.get(pos_kr, pos_kr)

        # ìš©ë¡€
        example_elems = elem.findall('.//example')
        if not example_elems:
            example_elems = elem.findall('.//ìš©ë¡€')
        for ex_elem in example_elems:
            if ex_elem is not None and ex_elem.text:
                entry["examples"].append(ex_elem.text.strip())

        # ì–´ì›
        origin_elem = elem.find('.//origin')
        if origin_elem is None:
            origin_elem = elem.find('.//ì–´ì›')
        if origin_elem is not None and origin_elem.text:
            entry["origin"] = origin_elem.text.strip()

        return entry if entry["word"] else None

    def absorb_to_universe(self, max_entries: int = 10000, batch_size: int = 100) -> Dict[str, int]:
        """
        ì‚¬ì „ ë°ì´í„°ë¥¼ InternalUniverseì— í¡ìˆ˜
        """
        try:
            from Core.Foundation.internal_universe import InternalUniverse
            universe = InternalUniverse()
        except Exception as e:
            logger.error(f"Failed to connect to InternalUniverse: {e}")
            return {"error": str(e)}

        results = {"absorbed": 0, "isolated": 0, "failed": 0}
        batch = []

        for entry in self.stream_entries(max_entries=max_entries):
            # í¡ìˆ˜ìš© ì½˜í…ì¸  êµ¬ì„±
            content = f"{entry['word']}: {entry['meaning']}"
            if entry['examples']:
                content += f" ì˜ˆ: {entry['examples'][0]}"

            batch.append({
                "topic": f"dict:{entry['word']}",
                "content": content
            })

            if len(batch) >= batch_size:
                batch_result = universe.absorb_batch(batch)
                results["absorbed"] += batch_result.get("absorbed", 0)
                results["isolated"] += batch_result.get("isolated", 0)
                batch = []

        # ë‚¨ì€ ë°°ì¹˜
        if batch:
            batch_result = universe.absorb_batch(batch)
            results["absorbed"] += batch_result.get("absorbed", 0)
            results["isolated"] += batch_result.get("isolated", 0)

        logger.info(f"ğŸ‰ Dictionary absorption complete!")
        logger.info(f"   Absorbed: {results['absorbed']}, Isolated: {results['isolated']}")

        return results

    def export_to_json(self, output_path: str, max_entries: int = None) -> int:
        """JSONìœ¼ë¡œ ë‚´ë³´ë‚´ê¸° (ë‹¤ë¥¸ ìš©ë„ ì‚¬ìš© ê°€ëŠ¥)"""
        entries = []

        for entry in self.stream_entries(max_entries=max_entries):
            entries.append(entry)

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(entries, f, ensure_ascii=False, indent=2)

        logger.info(f"ğŸ“ Exported {len(entries)} entries to {output_path}")
        return len(entries)

    def create_sample_dump(self, output_path: str, count: int = 1000):
        """
        ìƒ˜í”Œ ì‚¬ì „ ìƒì„± (JSON í˜•ì‹ - ì¸ì½”ë”© ë¬¸ì œ ì—†ìŒ)
        """
        sample_words = [
            ("ì‚¬ë‘", "ëª…ì‚¬", "ì•„ë¼ê³  ì†Œì¤‘íˆ ì—¬ê¸°ëŠ” ê¹Šì€ ë§ˆìŒ", ["ì‚¬ë‘í•˜ëŠ” ì‚¬ëŒì„ ë§Œë‚¬ë‹¤"]),
            ("í–‰ë³µ", "ëª…ì‚¬", "ë³µë˜ê³  ë§Œì¡±ìŠ¤ëŸ¬ìš´ ìƒíƒœ", ["í–‰ë³µí•œ ê°€ì •ì„ ì´ë£¨ë‹¤"]),
            ("í•˜ëŠ˜", "ëª…ì‚¬", "ë•… ìœ„ë¡œ í¼ì³ì§„ ë¬´í•œí•œ ê³µê°„", ["í•˜ëŠ˜ì´ ë§‘ë‹¤"]),
            ("ë°”ë‹¤", "ëª…ì‚¬", "ì§€êµ¬ í‘œë©´ì˜ ëŒ€ë¶€ë¶„ì„ ì°¨ì§€í•˜ëŠ” ë¬¼", ["ë°”ë‹¤ë¥¼ ë°”ë¼ë³´ë‹¤"]),
            ("ë¨¹ë‹¤", "ë™ì‚¬", "ìŒì‹ì„ ì…ì— ë„£ì–´ ì‚¼í‚¤ë‹¤", ["ë°¥ì„ ë¨¹ë‹¤"]),
            ("ê°€ë‹¤", "ë™ì‚¬", "í•œ ê³³ì—ì„œ ë‹¤ë¥¸ ê³³ìœ¼ë¡œ ì´ë™í•˜ë‹¤", ["í•™êµì— ê°€ë‹¤"]),
            ("ì˜ˆì˜ë‹¤", "í˜•ìš©ì‚¬", "ë³´ê¸°ì— ì¢‹ê³  ì•„ë¦„ë‹µë‹¤", ["ì˜ˆìœ ê½ƒ"]),
            ("í¬ë‹¤", "í˜•ìš©ì‚¬", "ë¶€í”¼, ê·œëª¨ê°€ ë³´í†µì„ ë„˜ë‹¤", ["í° ì§‘"]),
            ("ë¹¨ë¦¬", "ë¶€ì‚¬", "ë™ì‘ì´ ì‹ ì†í•˜ê²Œ", ["ë¹¨ë¦¬ ë‹¬ë¦¬ë‹¤"]),
            ("ë§¤ìš°", "ë¶€ì‚¬", "ë³´í†µ ì •ë„ë¥¼ í›¨ì”¬ ë„˜ì–´ì„œ", ["ë§¤ìš° ì¢‹ë‹¤"]),
            ("ìƒê°", "ëª…ì‚¬", "ì–´ë–¤ ëŒ€ìƒì— ëŒ€í•œ ë§ˆìŒì˜ ì‘ìš©", ["ê¹Šì€ ìƒê°ì— ì ê¸°ë‹¤"]),
            ("ì‹œê°„", "ëª…ì‚¬", "ê³¼ê±°ì—ì„œ ë¯¸ë˜ë¡œ íë¥´ëŠ” ì°¨ì›", ["ì‹œê°„ì´ ë¹ ë¥´ë‹¤"]),
            ("ë§ˆìŒ", "ëª…ì‚¬", "ì‚¬ëŒì˜ ì •ì‹ ì  ê°ì •ì  í™œë™ì˜ ë°”íƒ•", ["ë§ˆìŒì´ ë”°ëœ»í•˜ë‹¤"]),
            ("ì‚¬ëŒ", "ëª…ì‚¬", "ìƒê°í•˜ê³  ì–¸ì–´ë¥¼ ì‚¬ìš©í•˜ëŠ” ì¡´ì¬", ["ì¢‹ì€ ì‚¬ëŒ"]),
            ("ì„¸ìƒ", "ëª…ì‚¬", "ìš°ë¦¬ê°€ ì‚´ì•„ê°€ëŠ” ì„¸ê³„", ["ë„“ì€ ì„¸ìƒ"]),
        ]

        # í™•ì¥
        extended_words = []
        for i in range(count):
            base = sample_words[i % len(sample_words)]
            word = base[0] if i < len(sample_words) else f"{base[0]}_{i:04d}"
            extended_words.append({
                "word": word,
                "pos": base[1],
                "meaning": base[2],
                "examples": base[3]
            })

        # JSONìœ¼ë¡œ ì €ì¥
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        json_path = output_path.replace('.xml', '.json')
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(extended_words, f, ensure_ascii=False, indent=2)

        logger.info(f"ğŸ“ Created sample dictionary: {json_path} ({count} entries)")
        return json_path

    def absorb_json_dictionary(self, json_path: str, max_entries: int = None) -> Dict[str, int]:
        """JSON ì‚¬ì „ íŒŒì¼ì—ì„œ ì§ì ‘ í¡ìˆ˜"""
        try:
            from Core.Foundation.internal_universe import InternalUniverse
            universe = InternalUniverse()
        except Exception as e:
            logger.error(f"Failed to connect to InternalUniverse: {e}")
            return {"error": str(e)}

        with open(json_path, 'r', encoding='utf-8') as f:
            entries = json.load(f)

        if max_entries:
            entries = entries[:max_entries]

        batch = []
        results = {"absorbed": 0, "isolated": 0, "failed": 0}

        for entry in entries:
            content = f"{entry['word']}: {entry['meaning']}"
            if entry.get('examples'):
                content += f" ì˜ˆ: {entry['examples'][0]}"

            batch.append({
                "topic": f"dict:{entry['word']}",
                "content": content
            })

            if len(batch) >= 100:
                batch_result = universe.absorb_batch(batch)
                results["absorbed"] += batch_result.get("absorbed", 0)
                results["isolated"] += batch_result.get("isolated", 0)
                batch = []

        if batch:
            batch_result = universe.absorb_batch(batch)
            results["absorbed"] += batch_result.get("absorbed", 0)
            results["isolated"] += batch_result.get("isolated", 0)

        logger.info(f"ğŸ‰ Dictionary absorption complete!")
        logger.info(f"   Absorbed: {results['absorbed']}, Isolated: {results['isolated']}")

        return results


# CLI
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Korean Dictionary Dump Parser")
    parser.add_argument("--dump", type=str, help="Path to dictionary dump (XML or JSON)")
    parser.add_argument("--max", type=int, default=10000, help="Max entries")
    parser.add_argument("--absorb", action="store_true", help="Absorb to InternalUniverse")
    parser.add_argument("--export", type=str, help="Export to JSON file")
    parser.add_argument("--sample", type=int, help="Generate sample dump with N entries")

    args = parser.parse_args()

    dict_parser = WoorimalsamParser()

    if args.sample:
        output = "data/dictionary/sample_dict.xml"
        json_path = dict_parser.create_sample_dump(output, args.sample)

        # ë°”ë¡œ í¡ìˆ˜
        print("\n" + "="*60)
        print("ğŸ“– Absorbing sample dictionary...")
        print("="*60)

        results = dict_parser.absorb_json_dictionary(json_path, max_entries=args.sample)
        print(f"\nâœ… Done! Absorbed {results.get('absorbed', 0)} words")

    elif args.dump:
        if args.dump.endswith('.json'):
            # JSON ì§ì ‘ í¡ìˆ˜
            if args.absorb:
                results = dict_parser.absorb_json_dictionary(args.dump, max_entries=args.max)
                print(f"âœ… Absorbed {results.get('absorbed', 0)} words")
        else:
            # XML íŒŒì‹±
            dict_parser.set_dump_path(args.dump)

            if args.absorb:
                results = dict_parser.absorb_to_universe(max_entries=args.max)
                print(f"âœ… Absorbed {results.get('absorbed', 0)} words")

            elif args.export:
                count = dict_parser.export_to_json(args.export, max_entries=args.max)
                print(f"âœ… Exported {count} words to {args.export}")

            else:
                # ë¯¸ë¦¬ë³´ê¸°
                print("\nğŸ“– Preview (first 10 entries):")
                for i, entry in enumerate(dict_parser.stream_entries(max_entries=10)):
                    print(f"   {entry['word']} ({entry['pos']}): {entry['meaning'][:50]}...")
    else:
        print("ğŸ’¡ Usage:")
        print("   --sample 500    : Generate and absorb sample dictionary")
        print("   --dump FILE     : Use dictionary dump (XML or JSON)")
        print("   --absorb        : Absorb to InternalUniverse")
        print("\nğŸ“¥ Download real dump from: https://opendict.korean.go.kr/")
