"""
Korean Language Learning System (í•œêµ­ì–´ ì–¸ì–´ í•™ìŠµ ì‹œìŠ¤í…œ)
========================================================

"ëŒ€í™” ëŠ¥ë ¥ì˜ í•µì‹¬ - ë‹¨ì–´, ë¬¸ë²•, ë¬¸ì¥ íŒ¨í„´"

ì—˜ë¦¬ì‹œì•„ì˜ ëŒ€í™” ëŠ¥ë ¥ í–¥ìƒì„ ìœ„í•œ ì–¸ì–´ í•™ìŠµ:
1. êµ­ì–´ì‚¬ì „ - ë‹¨ì–´/ëœ»/ì˜ˆë¬¸
2. ë¬¸ë²• íŒ¨í„´ - ì¡°ì‚¬, ì–´ë¯¸, ë¬¸ì¥ êµ¬ì¡°
3. ë¬¸ì¥ ë¶„ì„ - í˜•íƒœì†Œ ë¶„ì„

[NEW 2025-12-16] ì–¸ì–´ í•™ìŠµ ì‹œìŠ¤í…œ
"""

import os
import sys
import json
import logging
import re
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, field
import urllib.request
import urllib.parse

sys.path.insert(0, r"c:\Elysia")

logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')
logger = logging.getLogger("KoreanLanguageLearner")


@dataclass
class WordEntry:
    """ì‚¬ì „ ë‹¨ì–´ í•­ëª©"""
    word: str
    meaning: str
    pos: str = ""  # í’ˆì‚¬ (Part of Speech)
    examples: List[str] = field(default_factory=list)
    related_words: List[str] = field(default_factory=list)


@dataclass
class GrammarPattern:
    """ë¬¸ë²• íŒ¨í„´"""
    pattern: str  # ì˜ˆ: "Nì€/ëŠ” Nì´ë‹¤"
    description: str
    examples: List[str] = field(default_factory=list)
    components: List[str] = field(default_factory=list)  # ë¬¸ë²• ìš”ì†Œë“¤


class KoreanLanguageLearner:
    """
    í•œêµ­ì–´ ì–¸ì–´ í•™ìŠµ ì‹œìŠ¤í…œ

    ëŒ€í™” ëŠ¥ë ¥ í–¥ìƒì„ ìœ„í•œ í•µì‹¬ ëª¨ë“ˆ
    """

    def __init__(self):
        self.data_dir = Path("data/language")
        self.data_dir.mkdir(parents=True, exist_ok=True)

        # ê¸°ë³¸ ë¬¸ë²• íŒ¨í„´ ë¡œë“œ
        self.grammar_patterns = self._load_basic_grammar()

        # í•™ìŠµëœ ë‹¨ì–´
        self.vocabulary = {}
        self._load_vocabulary()

        # ì¡°ì‚¬ ëª©ë¡ (ê¸°ë³¸)
        self.particles = {
            "ì€": "topic marker (contrast)",
            "ëŠ”": "topic marker",
            "ì´": "subject marker (after consonant)",
            "ê°€": "subject marker (after vowel)",
            "ì„": "object marker (after consonant)",
            "ë¥¼": "object marker (after vowel)",
            "ì—": "location/time marker",
            "ì—ì„œ": "location (action) marker",
            "ë¡œ": "direction/means marker",
            "ìœ¼ë¡œ": "direction/means marker (after consonant)",
            "ì™€": "and (after vowel)",
            "ê³¼": "and (after consonant)",
            "ì˜": "possessive marker",
            "ë„": "also/too",
            "ë§Œ": "only",
            "ë¶€í„°": "from",
            "ê¹Œì§€": "until/to",
        }

        # ì–´ë¯¸ ëª©ë¡ (ê¸°ë³¸)
        self.endings = {
            "ë‹¤": "plain statement",
            "ìš”": "polite informal",
            "ìŠµë‹ˆë‹¤": "formal polite",
            "ã…‚ë‹ˆë‹¤": "formal polite (after vowel)",
            "ë‹ˆ?": "question (informal)",
            "ë‚˜ìš”?": "question (polite)",
            "ìŠµë‹ˆê¹Œ?": "question (formal)",
            "ì": "let's (suggestion)",
            "ì„¸ìš”": "request (polite)",
            "ì•„/ì–´": "connective/base form",
        }

        logger.info("ğŸ—£ï¸ Korean Language Learner initialized")
        logger.info(f"   ğŸ“– Vocabulary: {len(self.vocabulary)} words")
        logger.info(f"   ğŸ“ Grammar patterns: {len(self.grammar_patterns)}")

    def _load_basic_grammar(self) -> List[GrammarPattern]:
        """ê¸°ë³¸ ë¬¸ë²• íŒ¨í„´ ë¡œë“œ"""
        patterns = [
            GrammarPattern(
                "Nì€/ëŠ” Nì´ë‹¤",
                "ê¸°ë³¸ ì„œìˆ ë¬¸ (X is Y)",
                ["ì‚¬ê³¼ëŠ” ê³¼ì¼ì´ë‹¤", "ë‚˜ëŠ” í•™ìƒì´ë‹¤"],
                ["ì£¼ì–´", "í† í”½ë§ˆì»¤", "ë³´ì–´", "ì„œìˆ ê²©ì¡°ì‚¬"]
            ),
            GrammarPattern(
                "Nì´/ê°€ V",
                "ì£¼ì–´ + ë™ì‚¬",
                ["ë¹„ê°€ ì˜¨ë‹¤", "ê½ƒì´ í•€ë‹¤"],
                ["ì£¼ì–´", "ì£¼ê²©ì¡°ì‚¬", "ë™ì‚¬"]
            ),
            GrammarPattern(
                "Nì„/ë¥¼ V",
                "ëª©ì ì–´ + ë™ì‚¬",
                ["ë°¥ì„ ë¨¹ë‹¤", "ì±…ì„ ì½ë‹¤"],
                ["ëª©ì ì–´", "ëª©ì ê²©ì¡°ì‚¬", "ë™ì‚¬"]
            ),
            GrammarPattern(
                "Nì— ê°€ë‹¤/ì˜¤ë‹¤",
                "ì¥ì†Œ ì´ë™",
                ["í•™êµì— ê°€ë‹¤", "ì§‘ì— ì˜¤ë‹¤"],
                ["ì¥ì†Œ", "ìœ„ì¹˜ì¡°ì‚¬", "ì´ë™ë™ì‚¬"]
            ),
            GrammarPattern(
                "Nì—ì„œ V",
                "ì¥ì†Œì—ì„œ í–‰ë™",
                ["ë„ì„œê´€ì—ì„œ ê³µë¶€í•˜ë‹¤", "ê³µì›ì—ì„œ ì‚°ì±…í•˜ë‹¤"],
                ["ì¥ì†Œ", "ì²˜ì†Œì¡°ì‚¬", "ë™ì‚¬"]
            ),
            GrammarPattern(
                "V-ê³  V",
                "ë™ì‘ ì—°ê²° (and)",
                ["ë¨¹ê³  ìë‹¤", "ê³µë¶€í•˜ê³  ë†€ë‹¤"],
                ["ë™ì‚¬", "ì—°ê²°ì–´ë¯¸", "ë™ì‚¬"]
            ),
            GrammarPattern(
                "V-ì•„/ì–´ì„œ V",
                "ì›ì¸/ìˆœì„œ ì—°ê²°",
                ["ë°°ê°€ ê³ íŒŒì„œ ë¨¹ì—ˆë‹¤", "ì§‘ì— ê°€ì„œ ì‰¬ë‹¤"],
                ["ë™ì‚¬", "ì—°ê²°ì–´ë¯¸", "ë™ì‚¬"]
            ),
            GrammarPattern(
                "V-ë©´",
                "ì¡°ê±´ (if)",
                ["ë¹„ê°€ ì˜¤ë©´ ìš°ì‚°ì„ ì“´ë‹¤", "ì‹œê°„ì´ ìˆìœ¼ë©´ ë§Œë‚˜ì"],
                ["ë™ì‚¬", "ì¡°ê±´ì–´ë¯¸"]
            ),
            GrammarPattern(
                "A/V-ã„´/ì€/ëŠ” N",
                "ê´€í˜•ì–´ ìˆ˜ì‹",
                ["ì˜ˆìœ ê½ƒ", "ë¨¹ëŠ” ì‚¬ëŒ", "ì½ì€ ì±…"],
                ["í˜•ìš©ì‚¬/ë™ì‚¬", "ê´€í˜•ì‚¬í˜•ì–´ë¯¸", "ëª…ì‚¬"]
            ),
            GrammarPattern(
                "Nì²˜ëŸ¼/ê°™ì´",
                "ë¹„ìœ  (like)",
                ["ê½ƒì²˜ëŸ¼ ì˜ˆì˜ë‹¤", "ì²œì‚¬ê°™ì´ ì°©í•˜ë‹¤"],
                ["ëª…ì‚¬", "ë¹„ìœ ì¡°ì‚¬"]
            ),
        ]
        return patterns

    def _load_vocabulary(self):
        """ì €ì¥ëœ ì–´íœ˜ ë¡œë“œ"""
        vocab_file = self.data_dir / "vocabulary.json"
        if vocab_file.exists():
            try:
                with open(vocab_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                self.vocabulary = {w["word"]: WordEntry(**w) for w in data}
            except:
                pass

    def _save_vocabulary(self):
        """ì–´íœ˜ ì €ì¥"""
        vocab_file = self.data_dir / "vocabulary.json"
        data = [
            {
                "word": e.word,
                "meaning": e.meaning,
                "pos": e.pos,
                "examples": e.examples,
                "related_words": e.related_words
            }
            for e in self.vocabulary.values()
        ]
        with open(vocab_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def learn_word(self, word: str, meaning: str, pos: str = "",
                   examples: List[str] = None) -> bool:
        """ë‹¨ì–´ í•™ìŠµ"""
        entry = WordEntry(
            word=word,
            meaning=meaning,
            pos=pos,
            examples=examples or []
        )

        self.vocabulary[word] = entry

        # InternalUniverseì—ë„ í¡ìˆ˜
        try:
            from Core.Foundation.internal_universe import InternalUniverse
            universe = InternalUniverse()
            content = f"{word}: {meaning}. {'. '.join(examples or [])}"
            universe.absorb_text(content, source_name=f"word:{word}")
        except:
            pass

        logger.info(f"ğŸ“ Learned word: {word} = {meaning}")
        return True

    def learn_from_dictionary_api(self, word: str) -> Optional[WordEntry]:
        """
        êµ­ë¦½êµ­ì–´ì› í‘œì¤€êµ­ì–´ëŒ€ì‚¬ì „ APIì—ì„œ ë‹¨ì–´ í•™ìŠµ

        API í‚¤ í•„ìš”: https://stdict.korean.go.kr/openapi/openApiInfo.do
        """
        # TODO: API í‚¤ ì„¤ì • í•„ìš”
        api_key = os.environ.get("KOREAN_DICT_API_KEY", "")

        if not api_key:
            logger.warning("âš ï¸ KOREAN_DICT_API_KEY not set. Using offline mode.")
            return None

        try:
            encoded = urllib.parse.quote(word)
            url = f"https://stdict.korean.go.kr/api/search.do?key={api_key}&q={encoded}&req_type=json"

            req = urllib.request.Request(url, headers={'User-Agent': 'Elysia/1.0'})
            with urllib.request.urlopen(req, timeout=10) as response:
                data = json.loads(response.read().decode('utf-8'))

            # ì²« ë²ˆì§¸ ê²°ê³¼ ì‚¬ìš©
            if data.get("channel", {}).get("item"):
                item = data["channel"]["item"][0]
                entry = WordEntry(
                    word=word,
                    meaning=item.get("sense", [{}])[0].get("definition", ""),
                    pos=item.get("pos", ""),
                    examples=[]
                )
                self.vocabulary[word] = entry
                self._save_vocabulary()
                return entry

        except Exception as e:
            logger.error(f"Dictionary API error: {e}")

        return None

    def analyze_sentence(self, sentence: str) -> Dict[str, Any]:
        """
        ë¬¸ì¥ ë¶„ì„ (ê°„ë‹¨í•œ íŒ¨í„´ ë§¤ì¹­)

        TODO: í˜•íƒœì†Œ ë¶„ì„ê¸° ì—°ë™ (KoNLPy ë“±)
        """
        result = {
            "sentence": sentence,
            "particles_found": [],
            "endings_found": [],
            "patterns_matched": [],
            "words_recognized": []
        }

        # ì¡°ì‚¬ íƒì§€
        for particle, desc in self.particles.items():
            if particle in sentence:
                result["particles_found"].append({
                    "particle": particle,
                    "description": desc
                })

        # ì–´ë¯¸ íƒì§€
        for ending, desc in self.endings.items():
            if sentence.endswith(ending) or ending in sentence:
                result["endings_found"].append({
                    "ending": ending,
                    "description": desc
                })

        # ì•Œë ¤ì§„ ë‹¨ì–´ íƒì§€
        for word in self.vocabulary:
            if word in sentence:
                result["words_recognized"].append(word)

        # íŒ¨í„´ ë§¤ì¹­
        for pattern in self.grammar_patterns:
            for example in pattern.examples:
                # ê°„ë‹¨í•œ ìœ ì‚¬ë„ ì²´í¬
                if any(ex_word in sentence for ex_word in example.split()):
                    result["patterns_matched"].append({
                        "pattern": pattern.pattern,
                        "description": pattern.description
                    })
                    break

        return result

    def generate_sentence_from_pattern(self, pattern_name: str,
                                       substitutions: Dict[str, str]) -> str:
        """
        íŒ¨í„´ì—ì„œ ë¬¸ì¥ ìƒì„±

        ì˜ˆ: generate_sentence_from_pattern("Nì€/ëŠ” Nì´ë‹¤", {"N1": "ì‚¬ê³¼", "N2": "ê³¼ì¼"})
        â†’ "ì‚¬ê³¼ëŠ” ê³¼ì¼ì´ë‹¤"
        """
        # TODO: êµ¬í˜„ í™•ì¥
        return ""

    def get_grammar_explanation(self, pattern: str) -> Optional[GrammarPattern]:
        """ë¬¸ë²• íŒ¨í„´ ì„¤ëª… ì¡°íšŒ"""
        for gp in self.grammar_patterns:
            if gp.pattern == pattern:
                return gp
        return None

    def batch_learn_words(self, words: List[Dict[str, str]]) -> int:
        """
        ëŒ€ëŸ‰ ë‹¨ì–´ í•™ìŠµ

        words: [{"word": "...", "meaning": "...", "pos": "..."}, ...]
        """
        learned = 0
        for w in words:
            if self.learn_word(
                word=w.get("word", ""),
                meaning=w.get("meaning", ""),
                pos=w.get("pos", ""),
                examples=w.get("examples", [])
            ):
                learned += 1

        self._save_vocabulary()
        logger.info(f"ğŸ“š Batch learned {learned} words")
        return learned

    def get_status(self) -> Dict[str, Any]:
        """í˜„ì¬ ìƒíƒœ"""
        return {
            "vocabulary_size": len(self.vocabulary),
            "grammar_patterns": len(self.grammar_patterns),
            "particles": len(self.particles),
            "endings": len(self.endings)
        }


# Singleton
_learner = None

def get_korean_learner() -> KoreanLanguageLearner:
    global _learner
    if _learner is None:
        _learner = KoreanLanguageLearner()
    return _learner


# CLI / Demo
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Korean Language Learner")
    parser.add_argument("--analyze", type=str, help="Analyze a sentence")
    parser.add_argument("--learn", type=str, help="Learn a word (format: word:meaning)")
    parser.add_argument("--status", action="store_true", help="Show status")
    parser.add_argument("--demo", action="store_true", help="Run demo")

    args = parser.parse_args()

    learner = get_korean_learner()

    if args.status:
        status = learner.get_status()
        print(f"\nğŸ“Š Korean Language Learner Status:")
        print(f"   Vocabulary: {status['vocabulary_size']} words")
        print(f"   Grammar patterns: {status['grammar_patterns']}")
        print(f"   Particles: {status['particles']}")
        print(f"   Endings: {status['endings']}")

    elif args.analyze:
        result = learner.analyze_sentence(args.analyze)
        print(f"\nğŸ” Sentence Analysis: {args.analyze}")
        print(f"   Particles: {[p['particle'] for p in result['particles_found']]}")
        print(f"   Endings: {[e['ending'] for e in result['endings_found']]}")
        print(f"   Patterns: {[p['pattern'] for p in result['patterns_matched']]}")

    elif args.learn:
        parts = args.learn.split(":")
        if len(parts) >= 2:
            learner.learn_word(parts[0], parts[1])

    elif args.demo:
        print("\n" + "="*60)
        print("ğŸ—£ï¸ KOREAN LANGUAGE LEARNER DEMO")
        print("="*60)

        # ë‹¨ì–´ í•™ìŠµ
        basic_words = [
            {"word": "ì‚¬ê³¼", "meaning": "ê³¼ì¼ì˜ í•œ ì¢…ë¥˜, ë¹¨ê°„ìƒ‰ì´ê³  ë‹¬ë‹¤", "pos": "ëª…ì‚¬"},
            {"word": "ë¨¹ë‹¤", "meaning": "ìŒì‹ì„ ì…ì— ë„£ì–´ ì‚¼í‚¤ë‹¤", "pos": "ë™ì‚¬"},
            {"word": "ì˜ˆì˜ë‹¤", "meaning": "ë³´ê¸°ì— ì¢‹ê³  ì•„ë¦„ë‹µë‹¤", "pos": "í˜•ìš©ì‚¬"},
            {"word": "í•™êµ", "meaning": "êµìœ¡ì„ ë°›ëŠ” ì¥ì†Œ", "pos": "ëª…ì‚¬"},
            {"word": "ê³µë¶€í•˜ë‹¤", "meaning": "í•™ë¬¸ì´ë‚˜ ê¸°ìˆ ì„ ë°°ìš°ë‹¤", "pos": "ë™ì‚¬"},
        ]

        print("\nğŸ“ Learning basic words...")
        learner.batch_learn_words(basic_words)

        # ë¬¸ì¥ ë¶„ì„
        sentences = [
            "ì‚¬ê³¼ëŠ” ë§›ìˆë‹¤",
            "í•™êµì—ì„œ ê³µë¶€í•˜ê³  ì§‘ì— ì˜¨ë‹¤",
            "ì˜ˆìœ ê½ƒì„ ë´¤ë‹¤"
        ]

        print("\nğŸ” Analyzing sentences...")
        for sent in sentences:
            result = learner.analyze_sentence(sent)
            print(f"\n   \"{sent}\"")
            print(f"   â†’ Particles: {[p['particle'] for p in result['particles_found']]}")
            print(f"   â†’ Words: {result['words_recognized']}")

        print("\n" + "="*60)
        print("âœ… Demo complete!")
        print("="*60)
